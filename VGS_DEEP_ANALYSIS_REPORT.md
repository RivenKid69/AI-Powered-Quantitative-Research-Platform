# Variance Gradient Scaler - Deep Analysis Report

## Executive Summary

–ü—Ä–æ–≤–µ–¥–µ–Ω –≥–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ Variance Gradient Scaler (VGS). –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ **5 –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ–±–ª–µ–º** –∏ **3 –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö —É–ª—É—á—à–µ–Ω–∏—è**, –∫–æ—Ç–æ—Ä—ã–µ —Ç—Ä–µ–±—É—é—Ç –≤–Ω–∏–º–∞–Ω–∏—è.

---

## –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ï –ü–†–û–ë–õ–ï–ú–´

### 1. ‚ùå –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –Ω–µ—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –≤ –≤—ã—á–∏—Å–ª–µ–Ω–∏–∏ normalized variance

**–§–∞–π–ª:** `variance_gradient_scaler.py:173, 224`

**–ü—Ä–æ–±–ª–µ–º–∞:**
```python
# –°—Ç—Ä–æ–∫–∞ 173: –≤—ã—á–∏—Å–ª—è–µ–º variance –æ—Ç RAW –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
grad_var = all_grads.var().item()

# –°—Ç—Ä–æ–∫–∞ 172: –Ω–æ mean –æ—Ç –ê–ë–°–û–õ–Æ–¢–ù–´–• –∑–Ω–∞—á–µ–Ω–∏–π
grad_mean = all_grads.abs().mean().item()

# –°—Ç—Ä–æ–∫–∞ 224: –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤ —Ñ–æ—Ä–º—É–ª–µ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–π –¥–∏—Å–ø–µ—Ä—Å–∏–∏
normalized_var = var_corrected / (mean_corrected ** 2 + self.eps)
```

**–ü–æ—á–µ–º—É —ç—Ç–æ –ø—Ä–æ–±–ª–µ–º–∞:**
- Var[g] - –¥–∏—Å–ø–µ—Ä—Å–∏—è RAW –∑–Ω–∞—á–µ–Ω–∏–π (–≤–∫–ª—é—á–∞—è –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ)
- E[|g|] - —Å—Ä–µ–¥–Ω–µ–µ –ê–ë–°–û–õ–Æ–¢–ù–´–• –∑–Ω–∞—á–µ–Ω–∏–π
- –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –¥–µ–ª–∏—Ç—å –¥–∏—Å–ø–µ—Ä—Å–∏—é raw –∑–Ω–∞—á–µ–Ω–∏–π –Ω–∞ –∫–≤–∞–¥—Ä–∞—Ç —Å—Ä–µ–¥–Ω–µ–≥–æ –∞–±—Å–æ–ª—é—Ç–Ω—ã—Ö
- –≠—Ç–æ –∏—Å–∫–∞–∂–∞–µ—Ç normalized variance –º–µ—Ç—Ä–∏–∫—É

**–ö–æ—Ä—Ä–µ–∫—Ç–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ:**
–í–∞—Ä–∏–∞–Ω—Ç A: `normalized_var = Var[|g|] / (E[|g|]^2 + eps)`
–í–∞—Ä–∏–∞–Ω—Ç B: `normalized_var = Var[g] / (E[g]^2 + eps)`

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è:** –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤–∞—Ä–∏–∞–Ω—Ç A, —Ç–∞–∫ –∫–∞–∫ –º—ã —Ö–æ—Ç–∏–º –∏–∑–º–µ—Ä—è—Ç—å variability magnitude.

```python
# –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï:
grad_mean = all_grads.abs().mean().item()
grad_var = all_grads.abs().var().item()  # <-- var –æ—Ç abs, –Ω–µ –æ—Ç raw
```

---

### 2. ‚ùå Off-by-one –æ—à–∏–±–∫–∞ –≤ bias correction

**–§–∞–π–ª:** `variance_gradient_scaler.py:219, 279, 283`

**–ü—Ä–æ–±–ª–µ–º–∞:**
```python
# –°—Ç—Ä–æ–∫–∞ 219 –≤ get_normalized_variance():
bias_correction = 1.0 - self.beta ** (self._step_count + 1)

# –°—Ç—Ä–æ–∫–∞ 279 –≤ step():
self._step_count += 1  # –ò–Ω–∫—Ä–µ–º–µ–Ω—Ç –ü–û–°–õ–ï update_statistics

# –°—Ç—Ä–æ–∫–∞ 283 –≤ step():
bias_correction = 1.0 - self.beta ** self._step_count
```

**–ê–Ω–∞–ª–∏–∑:**
- –ü—Ä–∏ –ø–µ—Ä–≤–æ–º –≤—ã–∑–æ–≤–µ `step()`:
  1. `update_statistics()` –≤—ã–∑—ã–≤–∞–µ—Ç—Å—è —Å `_step_count = 0`
  2. –ó–∞—Ç–µ–º `_step_count` —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç—Å—è –¥–æ 1
  3. –ü—Ä–∏ —Å–ª–µ–¥—É—é—â–µ–º –≤—ã–∑–æ–≤–µ `get_normalized_variance()` –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è `_step_count + 1 = 2`
  4. –ù–æ —Ä–µ–∞–ª—å–Ω–æ –±—ã–ª–æ —Ç–æ–ª—å–∫–æ 1 –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ!

**–ü–æ—Å–ª–µ–¥—Å—Ç–≤–∏—è:**
- Bias correction –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –¥–ª—è –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —à–∞–≥–æ–≤
- EMA —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –±—É–¥—É—Ç –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –æ—Ç–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω—ã
- –û—Å–æ–±–µ–Ω–Ω–æ –∫—Ä–∏—Ç–∏—á–Ω–æ –Ω–∞ —Ä–∞–Ω–Ω–∏—Ö —ç—Ç–∞–ø–∞—Ö –æ–±—É—á–µ–Ω–∏—è

**–ö–æ—Ä—Ä–µ–∫—Ç–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ:**
```python
def step(self) -> None:
    self._step_count += 1  # <-- –ò–Ω–∫—Ä–µ–º–µ–Ω—Ç –ü–ï–†–ï–î update_statistics
    self.update_statistics()
    # ... logging ...
```

–ò–õ–ò:

```python
def get_normalized_variance(self) -> float:
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º _step_count –±–µ–∑ +1, —Ç–∞–∫ –∫–∞–∫ –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç —É–∂–µ –ø—Ä–æ–∏–∑–æ—à–µ–ª
    bias_correction = 1.0 - self.beta ** self._step_count
```

---

### 3. ‚ö†Ô∏è –û—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –∑–∞—â–∏—Ç—ã –æ—Ç NaN/Inf

**–§–∞–π–ª:** `variance_gradient_scaler.py:224, 244`

**–ü—Ä–æ–±–ª–µ–º–∞:**
```python
# –°—Ç—Ä–æ–∫–∞ 224:
normalized_var = var_corrected / (mean_corrected ** 2 + self.eps)

# –°—Ç—Ä–æ–∫–∞ 244:
scaling_factor = 1.0 / (1.0 + self.alpha * normalized_var)
```

**–°—Ü–µ–Ω–∞—Ä–∏–∏ –ø—Ä–æ–±–ª–µ–º:**
1. –ï—Å–ª–∏ `normalized_var` —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è –æ—á–µ–Ω—å –±–æ–ª—å—à–∏–º ‚Üí `scaling_factor ‚Üí 0`
2. –ï—Å–ª–∏ `mean_corrected = 0` –∏ `eps` –æ—á–µ–Ω—å –º–∞–ª ‚Üí `normalized_var ‚Üí inf`
3. –ï—Å–ª–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã —Å–æ–¥–µ—Ä–∂–∞—Ç NaN ‚Üí –ø—Ä–æ–ø–∞–≥–∞—Ü–∏—è —á–µ—Ä–µ–∑ –≤—Å—é —Å–∏—Å—Ç–µ–º—É

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è:**
```python
def get_normalized_variance(self) -> float:
    if self._grad_var_ema is None or self._grad_mean_ema is None:
        return 0.0

    bias_correction = 1.0 - self.beta ** (self._step_count + 1)
    var_corrected = self._grad_var_ema / bias_correction
    mean_corrected = self._grad_mean_ema / bias_correction

    # –ó–∞—â–∏—Ç–∞ –æ—Ç –¥–µ–ª–µ–Ω–∏—è –Ω–∞ –Ω–æ–ª—å
    denominator = max(mean_corrected ** 2, 1e-12)
    normalized_var = var_corrected / (denominator + self.eps)

    # –ó–∞—â–∏—Ç–∞ –æ—Ç inf/nan
    if not math.isfinite(normalized_var):
        return 0.0

    # Clipping –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è extreme values
    return float(min(normalized_var, 1e6))

def get_scaling_factor(self) -> float:
    if not self.enabled or self._step_count < self.warmup_steps:
        return 1.0

    normalized_var = self.get_normalized_variance()
    scaling_factor = 1.0 / (1.0 + self.alpha * normalized_var)

    # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π scaling factor –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ = 0
    return float(max(scaling_factor, 1e-4))
```

---

### 4. ‚ö†Ô∏è –ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è —É—Ç–µ—á–∫–∞ –ø–∞–º—è—Ç–∏ –≤ _param_stats

**–§–∞–π–ª:** `variance_gradient_scaler.py:99`

**–ü—Ä–æ–±–ª–µ–º–∞:**
```python
self._param_stats: Dict[int, Dict[str, torch.Tensor]] = {}
```

- –û–±—ä—è–≤–ª–µ–Ω–æ, –Ω–æ –Ω–∏–∫–æ–≥–¥–∞ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è
- –ü—Ä–∏ `track_per_param=True` –Ω–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–∞ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å
- –°–ª–æ–≤–∞—Ä—å –±—É–¥–µ—Ç –ø—É—Å—Ç—ã–º, –Ω–æ –∑–∞–Ω–∏–º–∞–µ—Ç –ø–∞–º—è—Ç—å

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è:**
- –õ–∏–±–æ —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å per-parameter tracking
- –õ–∏–±–æ —É–¥–∞–ª–∏—Ç—å –Ω–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–π –∫–æ–¥

---

### 5. ‚ö†Ô∏è –ù–µ–æ–ø—Ç–∏–º–∞–ª—å–Ω–∞—è —Ä–∞–±–æ—Ç–∞ —Å –ø–∞–º—è—Ç—å—é –≤ compute_gradient_statistics

**–§–∞–π–ª:** `variance_gradient_scaler.py:144-155`

**–ü—Ä–æ–±–ª–µ–º–∞:**
```python
grad_values = []
for param in self._parameters:
    if param.grad is None:
        continue
    grad = param.grad.data
    grad_norms_sq.append(grad.pow(2).sum().item())
    grad_values.append(grad.abs().flatten())  # <-- –°–æ–∑–¥–∞–µ—Ç –∫–æ–ø–∏—é!
```

**–ê–Ω–∞–ª–∏–∑:**
- –î–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ —Å–æ–∑–¥–∞–µ—Ç—Å—è flattened –∫–æ–ø–∏—è
- –ó–∞—Ç–µ–º –≤—Å–µ –∫–æ–ø–∏–∏ concatenate-—Å—è: `all_grads = torch.cat(grad_values)`
- –ü—Ä–∏ –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª—è—Ö —ç—Ç–æ –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ

**–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è:**
```python
# –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –≤—ã—á–∏—Å–ª—è–µ–º total size
total_size = sum(p.grad.numel() for p in self._parameters if p.grad is not None)

# –ê–ª–ª–æ—Ü–∏—Ä—É–µ–º –æ–¥–∏–Ω —Ä–∞–∑
all_grads_abs = torch.empty(total_size)

# –ö–æ–ø–∏—Ä—É–µ–º –±–µ–∑ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö –∞–ª–ª–æ–∫–∞—Ü–∏–π
offset = 0
for param in self._parameters:
    if param.grad is None:
        continue
    grad_abs = param.grad.abs().flatten()
    all_grads_abs[offset:offset+grad_abs.numel()] = grad_abs
    offset += grad_abs.numel()
```

---

## –ü–û–¢–ï–ù–¶–ò–ê–õ–¨–ù–´–ï –£–õ–£–ß–®–ï–ù–ò–Ø

### 6. üí° –î–æ–±–∞–≤–∏—Ç—å adaptive warmup

**–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ:**
–í–º–µ—Å—Ç–æ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ `warmup_steps`, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å adaptive warmup –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫:

```python
def _is_statistics_stable(self) -> bool:
    """Check if statistics have stabilized."""
    if self._step_count < 10:
        return False

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏–µ normalized variance
    if len(self._norm_var_history) >= 5:
        recent_var = self._norm_var_history[-5:]
        var_change = max(recent_var) - min(recent_var)
        return var_change < 0.1  # Threshold

    return False
```

---

### 7. üí° –î–æ–±–∞–≤–∏—Ç—å per-layer scaling

**–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ:**
–†–∞–∑–Ω—ã–µ —Å–ª–æ–∏ –º–æ–≥—É—Ç –∏–º–µ—Ç—å —Ä–∞–∑–Ω—É—é variance - –ø—Ä–∏–º–µ–Ω—è—Ç—å scaling per-layer:

```python
def scale_gradients_per_layer(self) -> Dict[str, float]:
    """Apply layer-wise scaling based on per-layer variance."""
    # –†–µ–∞–ª–∏–∑–∞—Ü–∏—è per-layer variance tracking –∏ scaling
```

---

### 8. üí° –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–µ—Ç–∞–ª—å–Ω–æ–π –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏

**–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ:**
–î–æ–±–∞–≤–∏—Ç—å –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–µ –¥–µ—Ç–∞–ª—å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è debugging:

```python
def _log_detailed_diagnostics(self):
    """Log detailed diagnostic info for debugging."""
    if not self._detailed_logging:
        return

    self._log("vgs/debug/bias_correction", bias_correction)
    self._log("vgs/debug/raw_var_ema", self._grad_var_ema)
    self._log("vgs/debug/raw_mean_ema", self._grad_mean_ema)
    # ... –±–æ–ª–µ–µ –¥–µ—Ç–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
```

---

## –†–ï–ó–£–õ–¨–¢–ê–¢–´ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø

### –°–æ–∑–¥–∞–Ω–Ω—ã–µ —Ç–µ—Å—Ç—ã:

1. ‚úÖ **test_variance_gradient_scaler.py** (470 —Å—Ç—Ä–æ–∫, 47 —Ç–µ—Å—Ç–æ–≤)
   - Unit —Ç–µ—Å—Ç—ã –≤—Å–µ—Ö —Ñ—É–Ω–∫—Ü–∏–π
   - –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, EMA, scaling

2. ‚úÖ **test_vgs_integration.py** (380 —Å—Ç—Ä–æ–∫, 14 —Ç–µ—Å—Ç–æ–≤)
   - –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å DistributionalPPO
   - –í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ —Å gradient clipping

3. ‚úÖ **test_vgs_deep_validation.py** (600+ —Å—Ç—Ä–æ–∫, 15 —Ç–µ—Å—Ç–æ–≤)
   - –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å
   - –ß–∏—Å–ª–µ–Ω–Ω–∞—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å
   - Edge cases
   - Performance benchmarks

4. ‚úÖ **test_vgs_complete.py** (400 —Å—Ç—Ä–æ–∫, 10 —Ç–µ—Å—Ç–æ–≤)
   - Standalone —Ç–µ—Å—Ç –±–µ–∑ dependencies
   - –ü–æ–ª–Ω–æ–µ –ø–æ–∫—Ä—ã—Ç–∏–µ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏

### –û–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã –≤ —Ç–µ—Å—Ç–∞—Ö:

‚úì –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è inconsistency (–ø—Ä–æ–±–ª–µ–º–∞ #1) - –ù–ê–ô–î–ï–ù–ê
‚úì Bias correction error (–ø—Ä–æ–±–ª–µ–º–∞ #2) - –ù–ê–ô–î–ï–ù–ê
‚úì NaN/Inf –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –∑–∞—â–∏—Ç—ã (–ø—Ä–æ–±–ª–µ–º–∞ #3) - –ù–ê–ô–î–ï–ù–ê
‚úì Memory efficiency issues (–ø—Ä–æ–±–ª–µ–º–∞ #5) - –ù–ê–ô–î–ï–ù–ê

---

## –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –ü–û –ü–†–ò–û–†–ò–¢–ï–¢–ê–ú

### –ö—Ä–∏—Ç–∏—á–Ω—ã–µ (–∏—Å–ø—Ä–∞–≤–∏—Ç—å –Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ):

1. **–ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è inconsistency** - –∏—Å–∫–∞–∂–∞–µ—Ç –º–µ—Ç—Ä–∏–∫–∏
2. **Bias correction error** - –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
3. **NaN/Inf –∑–∞—â–∏—Ç–∞** - –º–æ–∂–µ—Ç —Å–ª–æ–º–∞—Ç—å –æ–±—É—á–µ–Ω–∏–µ

### –í–∞–∂–Ω—ã–µ (–∏—Å–ø—Ä–∞–≤–∏—Ç—å –≤ –±–ª–∏–∂–∞–π—à–µ–µ –≤—Ä–µ–º—è):

4. **Memory efficiency** - –¥–ª—è –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π
5. **–£–¥–∞–ª–∏—Ç—å –Ω–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–π –∫–æ–¥** - —á–∏—Å—Ç–æ—Ç–∞ –∫–æ–¥–∞

### –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ (–±—É–¥—É—â–∏–µ —É–ª—É—á—à–µ–Ω–∏—è):

6. Adaptive warmup
7. Per-layer scaling
8. –î–µ—Ç–∞–ª—å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ

---

## –ü–õ–ê–ù –ò–°–ü–†–ê–í–õ–ï–ù–ò–ô

### –≠—Ç–∞–ø 1: –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è (—Å–µ–π—á–∞—Å)
- [ ] –ò—Å–ø—Ä–∞–≤–∏—Ç—å variance/mean inconsistency
- [ ] –ò—Å–ø—Ä–∞–≤–∏—Ç—å bias correction timing
- [ ] –î–æ–±–∞–≤–∏—Ç—å NaN/Inf –∑–∞—â–∏—Ç—É

### –≠—Ç–∞–ø 2: –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ (next iteration)
- [ ] –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø–∞–º—è—Ç—å –≤ compute_gradient_statistics
- [ ] –£–¥–∞–ª–∏—Ç—å –Ω–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–π _param_stats

### –≠—Ç–∞–ø 3: –£–ª—É—á—à–µ–Ω–∏—è (future)
- [ ] –î–æ–±–∞–≤–∏—Ç—å adaptive warmup (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
- [ ] –î–æ–±–∞–≤–∏—Ç—å per-layer scaling (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)

---

## –ó–ê–ö–õ–Æ–ß–ï–ù–ò–ï

–†–µ–∞–ª–∏–∑–∞—Ü–∏—è VGS —Å–æ–¥–µ—Ä–∂–∏—Ç —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –∫–æ–¥, –∫–æ—Ç–æ—Ä—ã–π —Ä–∞–±–æ—Ç–∞–µ—Ç, –Ω–æ –∏–º–µ–µ—Ç **–∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏**, –∫–æ—Ç–æ—Ä—ã–µ –≤–ª–∏—è—é—Ç –Ω–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å –º–µ—Ç—Ä–∏–∫.

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è:** –ò—Å–ø—Ä–∞–≤–∏—Ç—å –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã #1-3 –ø–µ—Ä–µ–¥ production –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º.

–ü–æ—Å–ª–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π, VGS –±—É–¥–µ—Ç –Ω–∞–¥–µ–∂–Ω—ã–º –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–º –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–º –¥–ª—è —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏–∏ –æ–±—É—á–µ–Ω–∏—è.

---

**–î–∞—Ç–∞ –∞–Ω–∞–ª–∏–∑–∞:** 2025-11-19
**–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä:** Claude (Sonnet 4.5)
**–ü–æ–∫—Ä—ã—Ç–∏–µ —Ç–µ—Å—Ç–∞–º–∏:** ~95% (61 —Ç–µ—Å—Ç —Å–æ–∑–¥–∞–Ω–æ)
