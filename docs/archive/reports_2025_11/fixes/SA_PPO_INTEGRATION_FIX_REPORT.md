# SA-PPO Integration Fix Report (2025-11-21)

## ‚úÖ –ü–†–û–ë–õ–ï–ú–ê –†–ï–®–ï–ù–ê: Adversarial Training —Ç–µ–ø–µ—Ä—å –ê–ö–¢–ò–í–ï–ù

---

## üî¥ –ò—Å—Ö–æ–¥–Ω–∞—è –ø—Ä–æ–±–ª–µ–º–∞

**–°—Ç–∞—Ç—É—Å –¥–æ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è**: ‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –ù–ï–ò–°–ü–†–ê–í–ù–û–°–¢–¨

**–û–ø–∏—Å–∞–Ω–∏–µ**: State-Adversarial PPO (SA-PPO) –±—ã–ª –ø–æ–ª–Ω–æ—Å—Ç—å—é –Ω–µ–∞–∫—Ç–∏–≤–µ–Ω –∏–∑-–∑–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–µ–π –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏.

### –°–∏–º–ø—Ç–æ–º—ã:
1. ‚ùå `StateAdversarialPPO` wrapper —Å–æ–∑–¥–∞–≤–∞–ª—Å—è, –Ω–æ –Ω–µ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–ª—Å—è –Ω–∞ –º–æ–¥–µ–ª–∏
2. ‚ùå `compute_adversarial_loss()` –ù–ò–ö–û–ì–î–ê –Ω–µ –≤—ã–∑—ã–≤–∞–ª—Å—è –≤ training loop
3. ‚ùå Adversarial perturbations –ù–ï –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–ª–∏—Å—å
4. ‚ùå Robust KL regularization –ù–ï –ø—Ä–∏–º–µ–Ω—è–ª–∞—Å—å
5. ‚ùå –ü–∞—Ä–∞–º–µ—Ç—Ä—ã `adversarial_ratio` –∏ `robust_kl_coef` –ø–æ–ª–Ω–æ—Å—Ç—å—é –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–ª–∏—Å—å

### –í–ª–∏—è–Ω–∏–µ:
- **–ö–†–ò–¢–ò–ß–ï–°–ö–û–ï**: –ü–æ–ª–Ω–∞—è –ø–æ—Ç–µ—Ä—è —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ SA-PPO
- –ê–≥–µ–Ω—Ç –ù–ï –ø–æ–ª—É—á–∞–ª robustness training
- –ú–æ–¥–µ–ª–∏ –æ–±—É—á–∞–ª–∏—Å—å —Ç–æ–ª—å–∫–æ –Ω–∞ clean samples –±–µ–∑ adversarial augmentation

---

## ‚úÖ –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ

### 1. –î–æ–±–∞–≤–ª–µ–Ω—ã –Ω–æ–≤—ã–µ –º–µ—Ç–æ–¥—ã –≤ `adversarial/sa_ppo.py`

**–°–æ–∑–¥–∞–Ω–æ 2 –Ω–æ–≤—ã—Ö –º–µ—Ç–æ–¥–∞ –¥–ª—è –≥–∏–±–∫–æ–π –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏:**

#### `apply_adversarial_augmentation()` (lines 364-449)
- –ü—Ä–∏–º–µ–Ω—è–µ—Ç adversarial perturbations –∫ batch of states
- –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç PGD attack –Ω–∞ –æ—Å–Ω–æ–≤–µ policy loss
- –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç augmented states + sample mask (clean vs adversarial)
- –ù–ï –≤—ã—á–∏—Å–ª—è–µ—Ç loss (–æ—Å—Ç–∞–≤–ª—è–µ—Ç —ç—Ç–æ distributional PPO)

#### `compute_robust_kl_penalty()` (lines 451-493)
- –í—ã—á–∏—Å–ª—è–µ—Ç robust KL regularization –º–µ–∂–¥—É clean –∏ adversarial policies
- –î–æ–±–∞–≤–ª—è–µ—Ç—Å—è –∫ policy loss –∫–∞–∫ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π term
- Penalty = `robust_kl_coef * KL(œÄ_clean || œÄ_adv)`

### 2. –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –≤ `distributional_ppo.py`

**–ú–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏:**

#### –ê. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è (lines 6307-6310)
```python
# SA-PPO (State-Adversarial PPO) wrapper initialization
# This wrapper enables adversarial training for robustness
# Set via set_sa_ppo_wrapper() method after model creation
self._sa_ppo_wrapper: Optional[Any] = None
```

#### –ë. Setter/Getter –º–µ—Ç–æ–¥—ã (lines 6344-6367)
```python
def set_sa_ppo_wrapper(self, wrapper: Optional[Any]) -> None:
    """Set SA-PPO wrapper for adversarial training."""
    self._sa_ppo_wrapper = wrapper
    if wrapper is not None:
        logger.info(f"SA-PPO wrapper attached to model ...")

def get_sa_ppo_wrapper(self) -> Optional[Any]:
    """Get current SA-PPO wrapper instance."""
    return getattr(self, "_sa_ppo_wrapper", None)
```

#### –í. Adversarial augmentation –≤ training loop (lines 8997-9040)
```python
# SA-PPO: Apply adversarial augmentation if wrapper is active
sa_ppo_wrapper = getattr(self, "_sa_ppo_wrapper", None)
sa_ppo_enabled = (
    sa_ppo_wrapper is not None
    and hasattr(sa_ppo_wrapper, "is_adversarial_enabled")
    and sa_ppo_wrapper.is_adversarial_enabled
)

for rollout_data, sample_count, mask_tensor, sample_weight in zip(...):
    # Apply adversarial perturbations to observations
    observations_for_training = rollout_data.observations
    sa_ppo_info = {}
    sa_ppo_sample_mask = None

    if sa_ppo_enabled:
        # Apply adversarial augmentation
        observations_augmented, sa_ppo_sample_mask, sa_ppo_info = \
            sa_ppo_wrapper.apply_adversarial_augmentation(
                states=rollout_data.observations,
                actions=rollout_data.actions,
                advantages=advantages_flat,
                old_log_probs=old_log_probs_flat,
                clip_range=clip_range,
            )
        observations_for_training = observations_augmented

    # Use augmented observations in evaluate_actions
    _values, log_prob, entropy = self.policy.evaluate_actions(
        observations_for_training,  # <-- AUGMENTED!
        ...
    )
```

#### –ì. Robust KL penalty (lines 9264-9287)
```python
# SA-PPO: Add robust KL regularization if enabled
if sa_ppo_enabled and sa_ppo_sample_mask is not None:
    # Extract adversarial samples for robust KL computation
    adv_mask = sa_ppo_sample_mask > 0.5
    if torch.any(adv_mask):
        # Split observations into clean and adversarial
        obs_clean = rollout_data.observations[~adv_mask]
        obs_adv = observations_for_training[adv_mask]
        actions_for_kl = rollout_data.actions[adv_mask]

        # Compute robust KL penalty
        if obs_clean is not None and obs_clean.size(0) > 0:
            robust_kl_value, robust_kl_info = sa_ppo_wrapper.compute_robust_kl_penalty(
                states_clean=obs_clean,
                states_adv=obs_adv,
                actions=actions_for_kl,
            )
            # Add to policy loss
            robust_kl_tensor = policy_loss.new_tensor(robust_kl_value)
            policy_loss = policy_loss + robust_kl_tensor
```

### 3. –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –≤ `training_pbt_adversarial_integration.py`

**–ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï (lines 270-279):**

```python
# CRITICAL FIX: Set wrapper on model to enable adversarial training
# Without this, compute_adversarial_loss is NEVER called!
if hasattr(model, "set_sa_ppo_wrapper"):
    model.set_sa_ppo_wrapper(sa_ppo_wrapper)
    logger.info(f"SA-PPO wrapper attached to model for member {member.member_id}")
else:
    logger.warning(
        f"Model for member {member.member_id} does not support SA-PPO wrapper "
        "(missing set_sa_ppo_wrapper method). Adversarial training DISABLED."
    )
```

**–î–û:** Wrapper —Å–æ–∑–¥–∞–≤–∞–ª—Å—è, –Ω–æ –ù–ï —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–ª—Å—è –Ω–∞ –º–æ–¥–µ–ª–∏
**–ü–û–°–õ–ï:** Wrapper —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç—Å—è —á–µ—Ä–µ–∑ `model.set_sa_ppo_wrapper()` ‚úÖ

---

## üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ

**–°–æ–∑–¥–∞–Ω comprehensive test suite:** [tests/test_sa_ppo_integration_fix.py](tests/test_sa_ppo_integration_fix.py)

### –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ—Å—Ç–æ–≤:

```
============================= 10 passed in 2.87s ==============================
```

**‚úÖ –í—Å–µ —Ç–µ—Å—Ç—ã –ø—Ä–æ—Ö–æ–¥—è—Ç!**

### –ü–æ–∫—Ä—ã—Ç–∏–µ —Ç–µ—Å—Ç–∞–º–∏:

1. ‚úÖ **test_wrapper_can_be_set** - Wrapper —Å–æ–∑–¥–∞—ë—Ç—Å—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ
2. ‚úÖ **test_apply_adversarial_augmentation_disabled** - –û—Ç–∫–ª—é—á–µ–Ω–Ω–∞—è augmentation —Ä–∞–±–æ—Ç–∞–µ—Ç
3. ‚úÖ **test_apply_adversarial_augmentation_enabled** - –í–∫–ª—é—á—ë–Ω–Ω–∞—è augmentation —Ä–∞–±–æ—Ç–∞–µ—Ç
4. ‚úÖ **test_compute_robust_kl_penalty_disabled** - –û—Ç–∫–ª—é—á—ë–Ω–Ω—ã–π KL penalty
5. ‚úÖ **test_compute_robust_kl_penalty_enabled** - –í–∫–ª—é—á—ë–Ω–Ω—ã–π KL penalty
6. ‚úÖ **test_pbt_coordinator_sets_wrapper_on_model** - PBT —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç wrapper
7. ‚úÖ **test_pbt_coordinator_warns_if_model_not_support_wrapper** - Warning –ø—Ä–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏ –ø–æ–¥–¥–µ—Ä–∂–∫–∏
8. ‚úÖ **test_stats_tracking** - –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–µ—Ç—Å—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ
9. ‚úÖ **test_training_works_without_wrapper** - Backward compatibility (wrapper=None)
10. ‚úÖ **test_full_augmentation_pipeline** - –ü–æ–ª–Ω—ã–π pipeline augmentation ‚Üí loss ‚Üí backward

---

## üìä –ß—Ç–æ —Ç–µ–ø–µ—Ä—å —Ä–∞–±–æ—Ç–∞–µ—Ç

### ‚úÖ Adversarial Training Pipeline

1. **Augmentation** (‚úÖ –ê–ö–¢–ò–í–ù–û):
   - PGD attack –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç adversarial perturbations
   - Observations augment–∏—Ä—É—é—Ç—Å—è –¥–ª—è —á–∞—Å—Ç–∏ batch (–ø–æ `adversarial_ratio`)
   - Sample mask –æ—Ç—Å–ª–µ–∂–∏–≤–∞–µ—Ç clean vs adversarial samples

2. **Loss Computation** (‚úÖ –ê–ö–¢–ò–í–ù–û):
   - Augmented observations –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –≤ `evaluate_actions()`
   - Policy –∏ value losses –≤—ã—á–∏—Å–ª—è—é—Ç—Å—è –Ω–∞ mixed batch (clean + adversarial)
   - Robust KL penalty –¥–æ–±–∞–≤–ª—è–µ—Ç—Å—è –∫ policy loss

3. **Robustness** (‚úÖ –ê–ö–¢–ò–í–ù–û):
   - –ú–æ–¥–µ–ª—å –æ–±—É—á–∞–µ—Ç—Å—è –Ω–∞ worst-case perturbations
   - KL regularization –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç –±–æ–ª—å—à–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è policy
   - –ê–≥–µ–Ω—Ç —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è robust –∫ noise –∏ distribution shift

### ‚úÖ –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ (—Ç–µ–ø–µ—Ä—å –∞–∫—Ç–∏–≤–Ω—ã)

–ò–∑ [configs/config_pbt_adversarial.yaml](configs/config_pbt_adversarial.yaml):

```yaml
adversarial:
  enabled: true  # ‚úÖ –¢–µ–ø–µ—Ä—å —Ä–∞–±–æ—Ç–∞–µ—Ç!

  perturbation:
    epsilon: 0.075              # ‚úÖ –ü—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è
    attack_steps: 3             # ‚úÖ –ü—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è
    attack_method: pgd          # ‚úÖ –ü—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è

  adversarial_ratio: 0.5        # ‚úÖ –ü—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è (50% adversarial, 50% clean)
  robust_kl_coef: 0.1           # ‚úÖ –ü—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è
  warmup_updates: 10            # ‚úÖ –ü—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è
  attack_policy: true           # ‚úÖ –ü—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è
  attack_value: true            # ‚úÖ –ü—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è
```

### ‚úÖ –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ (TensorBoard)

–ù–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–æ—Å—Ç—É–ø–Ω—ã –≤ TensorBoard:

- `sa_ppo/enabled` - Adversarial training –∞–∫—Ç–∏–≤–µ–Ω?
- `sa_ppo/update_count` - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ updates
- `sa_ppo/adversarial_samples` - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ adversarial samples
- `sa_ppo/clean_samples` - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ clean samples
- `sa_ppo/adversarial_ratio` - –§–∞–∫—Ç–∏—á–µ—Å–∫–∏–π ratio
- `sa_ppo/robust_kl_penalty` - –ó–Ω–∞—á–µ–Ω–∏–µ robust KL penalty
- `sa_ppo/current_epsilon` - –¢–µ–∫—É—â–∏–π epsilon (–¥–ª—è adaptive schedule)
- `sa_ppo/attack_count` - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ PGD attacks
- `sa_ppo/avg_perturbation_norm` - –°—Ä–µ–¥–Ω—è—è –Ω–æ—Ä–º–∞ perturbations

---

## üéØ –û–∂–∏–¥–∞–µ–º—ã–µ —É–ª—É—á—à–µ–Ω–∏—è

–ü–æ—Å–ª–µ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è —Å –∞–∫—Ç–∏–≤–Ω—ã–º SA-PPO:

1. **üìà –£–ª—É—á—à–µ–Ω–Ω–∞—è robustness**:
   - –ê–≥–µ–Ω—Ç —É—Å—Ç–æ–π—á–∏–≤ –∫ noise –≤ observations
   - –ú–µ–Ω—å—à–µ degradation –ø—Ä–∏ distribution shift
   - –ë–æ–ª–µ–µ —Å—Ç–∞–±–∏–ª—å–Ω–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ –≤ production

2. **üéØ –õ—É—á—à–∞—è generalization**:
   - –ú–æ–¥–µ–ª—å –Ω–µ overfits –∫ training distribution
   - –†–∞–±–æ—Ç–∞–µ—Ç –ª—É—á—à–µ –Ω–∞ unseen data
   - Reduced catastrophic failures

3. **üõ°Ô∏è Defensive capabilities**:
   - –ó–∞—â–∏—Ç–∞ –æ—Ç adversarial attacks
   - –ë–æ–ª–µ–µ robust decision-making
   - –ú–µ–Ω—å—à–µ sensitivity –∫ input perturbations

---

## üîß –î–µ–π—Å—Ç–≤–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è

### –î–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è SA-PPO:

1. **–í–∫–ª—é—á–∏—Ç–µ adversarial training –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏:**
   ```bash
   python train_model_multi_patch.py --config configs/config_pbt_adversarial.yaml
   ```

2. **–ú–æ–Ω–∏—Ç–æ—Ä—å—Ç–µ –Ω–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –≤ TensorBoard:**
   ```bash
   tensorboard --logdir artifacts/pbt_checkpoints
   ```

3. **–ù–∞—Å—Ç—Ä–æ–π—Ç–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã** (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ):
   - `adversarial_ratio` (0.0-1.0) - —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ adversarial/clean samples
   - `robust_kl_coef` (0.0-0.5) - –≤–µ—Å robust KL regularization
   - `epsilon` (0.01-0.15) - –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –Ω–æ—Ä–º–∞ perturbations
   - `attack_steps` (1-10) - –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ PGD iterations

### –î–ª—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –º–æ–¥–µ–ª–µ–π:

**–†–ï–ö–û–ú–ï–ù–î–£–ï–¢–°–Ø** –ø–µ—Ä–µ–æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª–∏, –æ–±—É—á–µ–Ω–Ω—ã–µ –¥–æ 2025-11-21, —á—Ç–æ–±—ã:
- –ü–æ–ª—É—á–∏—Ç—å robustness benefits
- –í–∫–ª—é—á–∏—Ç—å adversarial training
- –£–ª—É—á—à–∏—Ç—å generalization

---

## üìù –ò–∑–º–µ–Ω—ë–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã

1. **[adversarial/sa_ppo.py](adversarial/sa_ppo.py)**
   - –î–æ–±–∞–≤–ª–µ–Ω–æ: `apply_adversarial_augmentation()` (lines 364-449)
   - –î–æ–±–∞–≤–ª–µ–Ω–æ: `compute_robust_kl_penalty()` (lines 451-493)

2. **[distributional_ppo.py](distributional_ppo.py)**
   - –î–æ–±–∞–≤–ª–µ–Ω–æ: `self._sa_ppo_wrapper` initialization (lines 6307-6310)
   - –î–æ–±–∞–≤–ª–µ–Ω–æ: `set_sa_ppo_wrapper()` method (lines 6344-6359)
   - –î–æ–±–∞–≤–ª–µ–Ω–æ: `get_sa_ppo_wrapper()` method (lines 6361-6367)
   - –ú–æ–¥–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–æ: Training loop –¥–ª—è adversarial augmentation (lines 8997-9040)
   - –ú–æ–¥–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–æ: Policy loss –¥–ª—è robust KL penalty (lines 9264-9287)

3. **[training_pbt_adversarial_integration.py](training_pbt_adversarial_integration.py)**
   - –ú–æ–¥–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–æ: `create_member_model()` —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç wrapper (lines 270-279)

4. **[tests/test_sa_ppo_integration_fix.py](tests/test_sa_ppo_integration_fix.py)**
   - –°–æ–∑–¥–∞–Ω–æ: Comprehensive test suite (10 tests, –≤—Å–µ –ø—Ä–æ—Ö–æ–¥—è—Ç ‚úÖ)

---

## üéâ –ó–∞–∫–ª—é—á–µ–Ω–∏–µ

**–ü—Ä–æ–±–ª–µ–º–∞ –ø–æ–ª–Ω–æ—Å—Ç—å—é —Ä–µ—à–µ–Ω–∞!**

‚úÖ Adversarial training —Ç–µ–ø–µ—Ä—å **–ê–ö–¢–ò–í–ï–ù**
‚úÖ SA-PPO wrapper –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω
‚úÖ –í—Å–µ —Ç–µ—Å—Ç—ã –ø—Ä–æ—Ö–æ–¥—è—Ç (10/10)
‚úÖ Backward compatibility —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞
‚úÖ PBT + SA-PPO —Ä–∞–±–æ—Ç–∞—é—Ç –≤–º–µ—Å—Ç–µ

**–°—Ç–∞—Ç—É—Å:** READY FOR PRODUCTION

---

**–î–∞—Ç–∞ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è:** 2025-11-21
**–í–µ—Ä—Å–∏—è:** v2.2
**–ê–≤—Ç–æ—Ä:** Claude Code (Anthropic)
