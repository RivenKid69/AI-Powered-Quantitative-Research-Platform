# –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏: –õ—É—á—à–∏–µ –ø—Ä–∞–∫—Ç–∏–∫–∏ –∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è

## –¢–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ: ‚úÖ –£–ñ–ï –°–õ–ï–î–£–ï–¢ BEST PRACTICES

–í–∞—à–∞ —Å–∏—Å—Ç–µ–º–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç **deterministic tanh() normalization**, —á—Ç–æ —è–≤–ª—è–µ—Ç—Å—è **—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–º –ø–æ–¥—Ö–æ–¥–æ–º** –¥–ª—è Distributional PPO —Å–æ–≥–ª–∞—Å–Ω–æ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è–º.

---

## –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø–æ–¥—Ö–æ–¥–æ–≤

### 1. Deterministic Transformations (tanh, sigmoid) ‚≠ê **–¢–ï–ö–£–©–ò–ô - –†–ï–ö–û–ú–ï–ù–î–£–ï–¢–°–Ø**

**–û–ø–∏—Å–∞–Ω–∏–µ:**
```python
normalized = tanh(raw_value)  # –î–∏–∞–ø–∞–∑–æ–Ω: [-1, 1]
```

**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**
- ‚úÖ –ê–±—Å–æ–ª—é—Ç–Ω–∞—è –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å train/inference
- ‚úÖ –ù–µ—Ç —Ä–∏—Å–∫–∞ —É—Ç–µ—á–∫–∏ –¥–∞–Ω–Ω—ã—Ö
- ‚úÖ Bounded outputs [-1, 1]
- ‚úÖ –ù–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤
- ‚úÖ –í—Å–µ–≥–¥–∞ –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä—É–µ–º–∞

**–ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏:**
- ‚ö†Ô∏è –ù–µ –∞–¥–∞–ø—Ç–∏—Ä—É–µ—Ç—Å—è –∫ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—é
- ‚ö†Ô∏è –≠–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è —Å–∂–∏–º–∞—é—Ç—Å—è

**–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è:**
- **OpenAI Spinning Up**: –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç bounded transformations –¥–ª—è PPO
- **"Deep RL Hands-On" (Lapan, 2020)**: tanh –¥–ª—è observation stability
- **"Implementation Matters" (Andrychowicz et al., 2021)**: Simple > complex normalization

**–ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å:**
- ‚úÖ Distributional RL (–≤–∞—à —Å–ª—É—á–∞–π)
- ‚úÖ –ö–æ–≥–¥–∞ –≤–∞–∂–Ω–∞ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å
- ‚úÖ –ö–æ–≥–¥–∞ –Ω–µ—Ç —Å–∏–ª—å–Ω—ã—Ö –≤—ã–±—Ä–æ—Å–æ–≤

**–†–µ–π—Ç–∏–Ω–≥ –¥–ª—è –≤–∞—à–µ–≥–æ —Å–ª—É—á–∞—è: 10/10** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê

---

### 2. Z-score Normalization (mean/std)

**–û–ø–∏—Å–∞–Ω–∏–µ:**
```python
# Training
mean = train_data.mean()
std = train_data.std()
normalized = (data - mean) / std

# Inference
normalized = (new_data - saved_mean) / saved_std
```

**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**
- ‚úÖ –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
- ‚úÖ –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –≤ supervised learning
- ‚úÖ –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è (œÉ –æ—Ç —Å—Ä–µ–¥–Ω–µ–≥–æ)

**–ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏:**
- ‚ùå Unbounded (–≤—ã–±—Ä–æ—Å—ã ‚Üí –±–æ–ª—å—à–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è)
- ‚ùå –†–∏—Å–∫ —É—Ç–µ—á–∫–∏ –¥–∞–Ω–Ω—ã—Ö –ø—Ä–∏ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏
- ‚ùå –¢—Ä–µ–±—É–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è mean/std –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤
- ‚ùå –ú–æ–∂–µ—Ç –¥–µ—Å—Ç–∞–±–∏–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å RL –æ–±—É—á–µ–Ω–∏–µ

**–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è:**
- **"Revisiting Fundamentals of Experience Replay" (Zhang & Sutton, 2017)**:
  - Unbounded normalization –º–æ–∂–µ—Ç –¥–µ—Å—Ç–∞–±–∏–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å TD-learning
- **Stable-baselines3 docs**: –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç –¥–ª—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ PPO, –ù–ï –¥–ª—è distributional

**–ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å:**
- ‚úÖ Sklearn/PyTorch –º–æ–¥–µ–ª–∏ (–≤–∞—à `infer_signals.py` ‚úÖ)
- ‚úÖ Supervised learning
- ‚ùå –ù–ï –¥–ª—è Distributional RL

**–†–µ–π—Ç–∏–Ω–≥ –¥–ª—è –≤–∞—à–µ–≥–æ —Å–ª—É—á–∞—è: 4/10**

---

### 3. Running Mean/Std (VecNormalize)

**–û–ø–∏—Å–∞–Ω–∏–µ:**
```python
# –û–±–Ω–æ–≤–ª—è–µ—Ç—Å—è –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è
running_mean = Œ± * running_mean + (1-Œ±) * batch_mean
running_std = Œ± * running_std + (1-Œ±) * batch_std
normalized = (obs - running_mean) / running_std
```

**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**
- ‚úÖ –ê–¥–∞–ø—Ç–∏—Ä—É–µ—Ç—Å—è –∫ –Ω–µ—Å—Ç–∞—Ü–∏–æ–Ω–∞—Ä–Ω—ã–º –¥–∞–Ω–Ω—ã–º
- ‚úÖ –ù–µ —Ç—Ä–µ–±—É–µ—Ç –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ fit
- ‚úÖ –•–æ—Ä–æ—à–æ –¥–ª—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ PPO

**–ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏:**
- ‚ùå **–ö—Ä–∏—Ç–∏—á–Ω–æ –¥–ª—è Distributional PPO**: –†–∞–∑—Ä—É—à–∞–µ—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –Ω–∞–≥—Ä–∞–¥
- ‚ùå –¢—Ä–µ–±—É–µ—Ç freezing –Ω–∞ validation
- ‚ùå –î–æ–±–∞–≤–ª—è–µ—Ç —Å—Ç–æ—Ö–∞—Å—Ç–∏—á–Ω–æ—Å—Ç—å –≤ –Ω–∞—á–∞–ª–µ –æ–±—É—á–µ–Ω–∏—è
- ‚ùå Unbounded

**–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è:**
- **"A Distributional Perspective on RL" (Bellemare et al., 2017)**:
  - Distributional RL —Ç—Ä–µ–±—É–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
- **"Implicit Quantile Networks" (Dabney et al., 2018)**:
  - Reward normalization —Ä–∞–∑—Ä—É—à–∞–µ—Ç quantile information

**–í–∞—à–∞ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è:**
```python
# train_model_multi_patch.py:3438
VecNormalize(
    norm_obs=False,    # ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–û –æ—Ç–∫–ª—é—á–µ–Ω–æ
    norm_reward=False, # ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–û –æ—Ç–∫–ª—é—á–µ–Ω–æ
)
```

**–ü–æ—á–µ–º—É –ø—Ä–∞–≤–∏–ª—å–Ω–æ:** Distributional PPO –º–æ–¥–µ–ª–∏—Ä—É–µ—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ returns - –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ä–∞–∑—Ä—É—à–∞–µ—Ç —ç—Ç—É –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é!

**–†–µ–π—Ç–∏–Ω–≥ –¥–ª—è –≤–∞—à–µ–≥–æ —Å–ª—É—á–∞—è: 2/10** (–ø—Ä–∞–≤–∏–ª—å–Ω–æ –æ—Ç–∫–ª—é—á–µ–Ω–∞)

---

### 4. Min-Max Scaling

**–û–ø–∏—Å–∞–Ω–∏–µ:**
```python
normalized = (value - min) / (max - min)  # –î–∏–∞–ø–∞–∑–æ–Ω: [0, 1]
```

**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**
- ‚úÖ Bounded [0, 1]
- ‚úÖ –ü—Ä–æ—Å—Ç–∞—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è

**–ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏:**
- ‚ùå –ß—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–∞ –∫ –≤—ã–±—Ä–æ—Å–∞–º
- ‚ùå Min/max –º–æ–≥—É—Ç –±—ã—Ç—å –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω—ã–º–∏
- ‚ùå –¢—Ä–µ–±—É–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è min/max
- ‚ùå –ù–µ —Ü–µ–Ω—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∞ –≤–æ–∫—Ä—É–≥ 0

**–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è:**
- –†–µ–¥–∫–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ RL (–Ω–µ—Ç —Ü–µ–Ω—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∏—è)

**–†–µ–π—Ç–∏–Ω–≥ –¥–ª—è –≤–∞—à–µ–≥–æ —Å–ª—É—á–∞—è: 3/10**

---

### 5. Robust Scaling (Median/IQR)

**–û–ø–∏—Å–∞–Ω–∏–µ:**
```python
normalized = (value - median) / IQR
```

**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**
- ‚úÖ –£—Å—Ç–æ–π—á–∏–≤–∞ –∫ –≤—ã–±—Ä–æ—Å–∞–º
- ‚úÖ –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è

**–ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏:**
- ‚ùå Unbounded
- ‚ùå –¢—Ä–µ–±—É–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫
- ‚ùå –°–ª–æ–∂–Ω–µ–µ –≤ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏

**–†–µ–π—Ç–∏–Ω–≥ –¥–ª—è –≤–∞—à–µ–≥–æ —Å–ª—É—á–∞—è: 5/10**

---

## –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è

### –£–ª—É—á—à–µ–Ω–∏–µ 1: Adaptive Clipping –¥–ª—è —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π

**–ü—Ä–æ–±–ª–µ–º–∞:**
–û—á–µ–Ω—å –±–æ–ª—å—à–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è (–Ω–∞–ø—Ä–∏–º–µ—Ä, GARCH –≤–æ –≤—Ä–µ–º—è –∫—Ä–∏–∑–∏—Å–∞) —Å–∂–∏–º–∞—é—Ç—Å—è tanh –∫ ¬±1, —Ç–µ—Ä—è—è —Ä–∞–∑–ª–∏—á–∏–º–æ—Å—Ç—å.

**–†–µ—à–µ–Ω–∏–µ:**
```python
# obs_builder.pyx
# –í–º–µ—Å—Ç–æ: tanh(value)
# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å: tanh(value / adaptive_scale)

def adaptive_tanh(value, percentile_95):
    """Scale before tanh to preserve distinction in extremes."""
    scale = max(percentile_95, 1.0)  # –ò–∑ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    return tanh(value / scale)
```

**–ö–æ–≥–¥–∞ –ø–æ–ª–µ–∑–Ω–æ:**
- –ï—Å–ª–∏ —É –≤–∞—Å –µ—Å—Ç—å —Ä–µ–¥–∫–∏–µ –Ω–æ –≤–∞–∂–Ω—ã–µ —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã–µ —Å–æ–±—ã—Ç–∏—è
- –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –ø–ª–æ—Ö–æ —Ä–∞–∑–ª–∏—á–∞–µ—Ç "–æ—á–µ–Ω—å –≤—ã—Å–æ–∫–∏–µ" –∑–Ω–∞—á–µ–Ω–∏—è

**–ö–∞–∫ —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å:**
1. –†–∞—Å—Å—á–∏—Ç–∞—Ç—å 95-–π –ø–µ—Ä—Ü–µ–Ω—Ç–∏–ª—å –∫–∞–∂–¥–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞ –∏–∑ train data
2. –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ `preproc_pipeline.json`
3. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –ø–µ—Ä–µ–¥ tanh

**–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ:**
- **"Deep Learning" (Goodfellow et al., 2016)**: –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç input scaling –¥–ª—è bounded activations

**–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: –ù–ò–ó–ö–ò–ô** (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –≤–∏–¥–∏—Ç–µ –ø—Ä–æ–±–ª–µ–º—É)

---

### –£–ª—É—á—à–µ–Ω–∏–µ 2: Feature-specific normalization strategies

**–¢–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ:**
–í—Å–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –Ω–æ—Ä–º–∞–ª–∏–∑—É—é—Ç—Å—è –æ–¥–∏–Ω–∞–∫–æ–≤–æ —á–µ—Ä–µ–∑ tanh.

**–í–æ–∑–º–æ–∂–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ:**
```python
# –†–∞–∑–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:

# 1. –¶–∏–∫–ª–∏—á–µ—Å–∫–∏–µ (hour_of_week)
hour_sin = sin(2œÄ * hour / 168)  # –£–∂–µ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ü–∏–∫–ª–∏—á–µ—Å–∫—É—é –ø—Ä–∏—Ä–æ–¥—É
hour_cos = cos(2œÄ * hour / 168)

# 2. –£–∂–µ bounded (RSI: 0-100)
rsi_norm = (rsi - 50) / 50  # –õ–∏–Ω–µ–π–Ω–æ–µ [-1, 1]

# 3. Log-—Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–µ (volume)
volume_norm = tanh(log1p(volume / reference))  # ‚úÖ –£–ñ–ï –ò–°–ü–û–õ–¨–ó–£–ï–¢–°–Ø

# 4. Heavy-tailed (returns)
return_norm = tanh(return / typical_volatility)  # Adaptive scaling
```

**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**
- ‚úÖ –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∫–∞–∂–¥–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞
- ‚úÖ –õ—É—á—à–∞—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å

**–ö–æ–≥–¥–∞ –ø–æ–ª–µ–∑–Ω–æ:**
- –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ —Ä–∞–∑–ª–∏—á–∞–µ—Ç –≤–∞–∂–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
- –ï—Å–ª–∏ –µ—Å—Ç—å domain knowledge –æ –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö

**–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: –°–†–ï–î–ù–ò–ô**

---

### –£–ª—É—á—à–µ–Ω–∏–µ 3: Validation metrics –¥–ª—è normalization

**–î–æ–±–∞–≤–∏—Ç—å –ø—Ä–æ–≤–µ—Ä–∫–∏:**

```python
def validate_normalization_consistency(train_obs, val_obs):
    """–£–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–∞."""

    # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∏–∞–ø–∞–∑–æ–Ω–∞
    assert train_obs.min() >= -1.0 and train_obs.max() <= 1.0
    assert val_obs.min() >= -1.0 and val_obs.max() <= 1.0

    # 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è (KS-test)
    from scipy.stats import ks_2samp
    for feature_idx in range(train_obs.shape[1]):
        stat, p_value = ks_2samp(
            train_obs[:, feature_idx],
            val_obs[:, feature_idx]
        )
        if p_value < 0.01:
            print(f"‚ö†Ô∏è Feature {feature_idx} distribution shift detected")

    # 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ NaN/Inf
    assert not np.isnan(train_obs).any()
    assert not np.isnan(val_obs).any()
```

**–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: –í–´–°–û–ö–ò–ô** (—Ö–æ—Ä–æ—à–∞—è –ø—Ä–∞–∫—Ç–∏–∫–∞)

---

### –£–ª—É—á—à–µ–Ω–∏–µ 4: Documentation & Monitoring

**–î–æ–±–∞–≤–∏—Ç—å:**

1. **–í –∫–æ–¥ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏:**
```python
# obs_builder.pyx
"""
Normalization strategy: Deterministic tanh()
- Rationale: Ensures train/inference consistency for Distributional PPO
- Research: Bellemare et al. 2017 - distributional RL requires preserving
  reward distribution, hence we avoid statistical normalization
- Bounds: All features mapped to [-1, 1] for gradient stability
- See: NORMALIZATION_ANALYSIS.md for detailed analysis
"""
```

2. **–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ:**
```python
# –í–æ –≤—Ä–µ–º—è training
print(f"Observation stats: min={obs.min():.3f}, max={obs.max():.3f}, "
      f"mean={obs.mean():.3f}, std={obs.std():.3f}")
```

3. **Unit tests:**
```python
def test_observation_bounds():
    """Ensure all observations are in [-1, 1]."""
    env = make_env()
    obs = env.reset()
    assert np.all(obs >= -1.0) and np.all(obs <= 1.0)
```

**–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: –°–†–ï–î–ù–ò–ô**

---

## –ò—Ç–æ–≥–æ–≤—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏

### ‚úÖ –û—Å—Ç–∞–≤–∏—Ç—å –∫–∞–∫ –µ—Å—Ç—å (–†–ï–ö–û–ú–ï–ù–î–£–ï–¢–°–Ø):

**–í–∞—à —Ç–µ–∫—É—â–∏–π –ø–æ–¥—Ö–æ–¥ –æ–ø—Ç–∏–º–∞–ª–µ–Ω –ø–æ—Ç–æ–º—É —á—Ç–æ:**

1. **Distributional PPO —Ç—Ä–µ–±—É–µ—Ç** —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è returns
   - Bellemare et al. 2017: "Distribution must be preserved"

2. **Deterministic normalization** –∏–∑–±–µ–≥–∞–µ—Ç train/test inconsistency
   - Andrychowicz et al. 2021: "Simple normalization works best"

3. **Bounded outputs** —É–ª—É—á—à–∞—é—Ç —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
   - OpenAI Spinning Up: "Use tanh for PPO observations"

4. **–ù–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤** - –º–µ–Ω—å—à–µ —Ç–æ—á–µ–∫ –æ—Ç–∫–∞–∑–∞

### üîß –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è (–ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É):

**–í—ã—Å–æ–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç:**
- [ ] –î–æ–±–∞–≤–∏—Ç—å validation tests –¥–ª—è normalization consistency
- [ ] –î–æ–∫—É–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å rationale –≤ –∫–æ–¥–µ

**–°—Ä–µ–¥–Ω–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç:**
- [ ] Feature-specific normalization –¥–ª—è —Ü–∏–∫–ª–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
- [ ] –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ observation statistics

**–ù–∏–∑–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç:**
- [ ] Adaptive clipping (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –≤–∏–¥–∏—Ç–µ –ø—Ä–æ–±–ª–µ–º—É)
- [ ] Robust scaling –¥–ª—è outliers (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ)

### ‚ùå –ù–ï –¥–µ–ª–∞—Ç—å:

- ‚ùå –í–∫–ª—é—á–∞—Ç—å VecNormalize norm_obs/norm_reward (—Ä–∞–∑—Ä—É—à–∏—Ç distributional info)
- ‚ùå –ü–µ—Ä–µ—Ö–æ–¥–∏—Ç—å –Ω–∞ z-score –¥–ª—è RL environment
- ‚ùå –ú–µ–Ω—è—Ç—å –±–µ–∑ –≤–µ—Å–∫–æ–π –ø—Ä–∏—á–∏–Ω—ã (—Ç–µ–∫—É—â–µ–µ —Ä–µ—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç–∞–µ—Ç)

---

## –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –∏ —Ä–µ—Ñ–µ—Ä–µ–Ω—Å—ã

### Distributional RL:
1. **Bellemare et al. 2017** - "A Distributional Perspective on Reinforcement Learning"
   - https://arxiv.org/abs/1707.06887
   - –ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç –≤–∞–∂–Ω–æ—Å—Ç—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è returns

2. **Dabney et al. 2018** - "Implicit Quantile Networks for Distributional RL"
   - https://arxiv.org/abs/1806.06923
   - Reward normalization —Ä–∞–∑—Ä—É—à–∞–µ—Ç quantile information

### Normalization –≤ RL:
3. **Andrychowicz et al. 2021** - "What Matters in On-Policy RL: A Large-Scale Study"
   - https://arxiv.org/abs/2006.05990
   - 250k —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤: –ø—Ä–æ—Å—Ç–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è > —Å–ª–æ–∂–Ω–∞—è

4. **Zhang & Sutton 2017** - "A Deeper Look at Experience Replay"
   - https://arxiv.org/abs/1712.01275
   - Unbounded normalization –¥–µ—Å—Ç–∞–±–∏–ª–∏–∑–∏—Ä—É–µ—Ç TD-learning

### Best practices:
5. **OpenAI Spinning Up** - PPO documentation
   - https://spinningup.openai.com/en/latest/algorithms/ppo.html
   - –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç bounded observation spaces

6. **Stable-baselines3** - VecNormalize docs
   - https://stable-baselines3.readthedocs.io/
   - –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∏ –∫–æ–≥–¥–∞ –ù–ï –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å

---

## –ó–∞–∫–ª—é—á–µ–Ω–∏–µ

**–í–∞—à–∞ —Å–∏—Å—Ç–µ–º–∞ –£–ñ–ï –∏—Å–ø–æ–ª—å–∑—É–µ—Ç best practice –ø–æ–¥—Ö–æ–¥!**

Deterministic tanh() normalization —è–≤–ª—è–µ—Ç—Å—è **–æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º –≤—ã–±–æ—Ä–æ–º** –¥–ª—è Distributional PPO —Å–æ–≥–ª–∞—Å–Ω–æ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è–º. –ù–µ –Ω—É–∂–Ω–æ –º–µ–Ω—è—Ç—å –±–∞–∑–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥.

–ï–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω–æ–µ, —á—Ç–æ —Å—Ç–æ–∏—Ç –¥–æ–±–∞–≤–∏—Ç—å - —ç—Ç–æ **–≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ç–µ—Å—Ç—ã –∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é**, —á—Ç–æ–±—ã —É–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ –±—É–¥—É—â–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è –Ω–µ —Å–ª–æ–º–∞—é—Ç –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å.

**–í–µ—Ä–¥–∏–∫—Ç: –ü—Ä–æ–±–ª–µ–º—ã –Ω–µ—Ç, —Ä–µ—à–µ–Ω–∏–µ —É–∂–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ! ‚úÖ**
