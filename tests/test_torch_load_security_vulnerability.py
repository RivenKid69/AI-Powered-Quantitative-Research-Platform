"""
Security Test: torch.load() Arbitrary Code Execution Vulnerability

This test demonstrates the security vulnerability in using torch.load() without
weights_only=True parameter. The test creates a malicious pickle payload that
executes arbitrary code during unpickling.

Reference: https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models
"""
import os
import pickle
import tempfile
from pathlib import Path
import torch
import pytest


# Global flag to track if malicious code was executed
MALICIOUS_CODE_EXECUTED = False
MALICIOUS_PAYLOAD_DATA = None


class MaliciousPayload:
    """
    A malicious class that executes code during unpickling.

    This demonstrates the vulnerability: when pickle.load() or torch.load()
    deserializes this object, __reduce__ is called, which can execute
    arbitrary code.
    """
    def __reduce__(self):
        # This code will be executed during unpickling!
        # In a real attack, this could:
        # - Steal credentials/API keys
        # - Install backdoors
        # - Delete files
        # - Exfiltrate data
        global MALICIOUS_CODE_EXECUTED, MALICIOUS_PAYLOAD_DATA

        # Set flag to prove code execution
        MALICIOUS_CODE_EXECUTED = True
        MALICIOUS_PAYLOAD_DATA = "PWNED: Arbitrary code executed during unpickling!"

        # Return a callable and its arguments
        # Using print as a "safe" demonstration instead of os.system
        return (print, ("WARNING: SECURITY BREACH: Malicious code executed during torch.load()!",))


def create_malicious_checkpoint(path: Path):
    """
    Create a malicious PyTorch checkpoint that executes code when loaded.

    This simulates an attacker creating a backdoored model checkpoint.
    """
    # Create a seemingly legitimate state dict
    state_dict = {
        'model': {
            'layer1.weight': torch.randn(10, 5),
            'layer1.bias': torch.randn(10),
        },
        'optimizer': {
            'state': {},
            'param_groups': [],
        },
        # Hidden malicious payload
        'metadata': MaliciousPayload(),
    }

    # Save using standard torch.save (which uses pickle)
    torch.save(state_dict, path)


def test_vulnerability_demonstration():
    """
    Test 1: Demonstrate that torch.load() WITHOUT weights_only=True
    executes arbitrary code from malicious checkpoints.

    Expected: FAILS (vulnerability exists)
    """
    global MALICIOUS_CODE_EXECUTED, MALICIOUS_PAYLOAD_DATA

    # Reset global flags
    MALICIOUS_CODE_EXECUTED = False
    MALICIOUS_PAYLOAD_DATA = None

    with tempfile.TemporaryDirectory() as tmpdir:
        checkpoint_path = Path(tmpdir) / "malicious_checkpoint.pt"

        # Create malicious checkpoint
        create_malicious_checkpoint(checkpoint_path)

        # Simulate current vulnerable code (NO weights_only parameter)
        print("\n[VULN] Testing VULNERABLE code (without weights_only)...")
        loaded_data = torch.load(checkpoint_path, map_location="cpu")

        # Check if malicious code was executed
        assert MALICIOUS_CODE_EXECUTED, (
            "Malicious code was NOT executed. This is unexpected - "
            "the vulnerability demonstration failed."
        )
        assert MALICIOUS_PAYLOAD_DATA is not None

        print(f"[PASS] Vulnerability confirmed: {MALICIOUS_PAYLOAD_DATA}")
        print("       This proves that torch.load() without weights_only=True")
        print("       can execute arbitrary code from malicious checkpoints.")


def test_safe_loading_with_weights_only():
    """
    Test 2: Demonstrate that torch.load() WITH weights_only=True
    prevents arbitrary code execution.

    Expected: PASSES (vulnerability mitigated)
    """
    global MALICIOUS_CODE_EXECUTED, MALICIOUS_PAYLOAD_DATA

    # Reset global flags
    MALICIOUS_CODE_EXECUTED = False
    MALICIOUS_PAYLOAD_DATA = None

    with tempfile.TemporaryDirectory() as tmpdir:
        checkpoint_path = Path(tmpdir) / "malicious_checkpoint.pt"

        # Create malicious checkpoint
        create_malicious_checkpoint(checkpoint_path)

        # Try to load with weights_only=True (SAFE)
        print("\n[SAFE] Testing SAFE code (with weights_only=True)...")

        # This should raise an exception instead of executing code
        with pytest.raises((pickle.UnpicklingError, RuntimeError, AttributeError)) as exc_info:
            loaded_data = torch.load(checkpoint_path, map_location="cpu", weights_only=True)

        # Verify malicious code was NOT executed
        assert not MALICIOUS_CODE_EXECUTED, (
            f"CRITICAL: Malicious code was executed even with weights_only=True! "
            f"Data: {MALICIOUS_PAYLOAD_DATA}"
        )

        print(f"[PASS] Safe loading confirmed: Malicious checkpoint rejected")
        print(f"       Exception raised: {type(exc_info.value).__name__}")
        print("       This proves that weights_only=True prevents code execution.")


def test_check_production_code_for_vulnerability():
    """
    Test 3: Check if production code files contain the vulnerability.

    This test scans actual production files to confirm they use
    unsafe torch.load() calls.
    """
    import re

    files_to_check = [
        "adversarial/pbt_scheduler.py",
        "infer_signals.py",
    ]

    vulnerable_files = []

    for file_path in files_to_check:
        full_path = Path(__file__).parent / file_path

        if not full_path.exists():
            continue

        content = full_path.read_text(encoding='utf-8')

        # Look for torch.load() calls
        torch_load_pattern = r'torch\.load\s*\([^)]*\)'
        matches = re.findall(torch_load_pattern, content)

        for match in matches:
            # Check if weights_only=True is present
            if 'weights_only=True' not in match and 'weights_only' not in match:
                vulnerable_files.append({
                    'file': file_path,
                    'call': match.strip(),
                })

    if vulnerable_files:
        print("\n[VULN] VULNERABILITY FOUND in production code:")
        for vuln in vulnerable_files:
            print(f"        File: {vuln['file']}")
            print(f"        Code: {vuln['call']}")

        # This test should initially fail to demonstrate the vulnerability exists
        pytest.fail(
            f"Found {len(vulnerable_files)} vulnerable torch.load() calls. "
            "These calls should use weights_only=True parameter."
        )
    else:
        print("\n[PASS] No vulnerable torch.load() calls found in production code.")


def test_pbt_scheduler_vulnerability():
    """
    Test 4: Specific test for PBT scheduler vulnerability

    Simulates the exploit scenario in PBT training where a malicious
    checkpoint could compromise the entire training run.
    """
    global MALICIOUS_CODE_EXECUTED, MALICIOUS_PAYLOAD_DATA

    # Reset
    MALICIOUS_CODE_EXECUTED = False
    MALICIOUS_PAYLOAD_DATA = None

    with tempfile.TemporaryDirectory() as tmpdir:
        # Simulate PBT checkpoint directory
        checkpoint_path = Path(tmpdir) / "member_123_checkpoint.pt"
        create_malicious_checkpoint(checkpoint_path)

        # Simulate PBT code from adversarial/pbt_scheduler.py:274
        print("\n[VULN] Simulating PBT exploit scenario...")
        new_state_dict = torch.load(checkpoint_path)  # VULNERABLE!

        assert MALICIOUS_CODE_EXECUTED, (
            "Expected malicious code to execute in PBT scenario"
        )

        print("[PASS] PBT vulnerability confirmed: Malicious checkpoint executed code")
        print("       Impact: Entire PBT training run could be compromised")


def test_infer_signals_vulnerability():
    """
    Test 5: Specific test for infer_signals.py vulnerability

    Simulates the scenario where a user loads a "shared model" that
    contains malicious code.
    """
    global MALICIOUS_CODE_EXECUTED, MALICIOUS_PAYLOAD_DATA

    # Reset
    MALICIOUS_CODE_EXECUTED = False
    MALICIOUS_PAYLOAD_DATA = None

    with tempfile.TemporaryDirectory() as tmpdir:
        # Simulate model file
        model_path = Path(tmpdir) / "best_model.pt"
        create_malicious_checkpoint(model_path)

        # Simulate infer_signals.py:35
        print("\n[VULN] Simulating model loading scenario...")
        model = torch.load(model_path, map_location="cpu")  # VULNERABLE!

        assert MALICIOUS_CODE_EXECUTED, (
            "Expected malicious code to execute in inference scenario"
        )

        print("[PASS] Inference vulnerability confirmed: Malicious model executed code")
        print("       Impact: Production inference system could be compromised")
        print("       Risk: API keys, credentials, data could be stolen")


if __name__ == "__main__":
    print("=" * 80)
    print("TORCH.LOAD() SECURITY VULNERABILITY TEST SUITE")
    print("=" * 80)
    print()
    print("This test suite demonstrates the security vulnerability in using")
    print("torch.load() without the weights_only=True parameter.")
    print()
    print("PyTorch Security Advisory:")
    print("https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models")
    print()
    print("=" * 80)

    # Run tests manually for demonstration
    try:
        test_vulnerability_demonstration()
    except AssertionError as e:
        print(f"[FAIL] Test failed: {e}")

    try:
        test_safe_loading_with_weights_only()
    except AssertionError as e:
        print(f"[FAIL] Test failed: {e}")

    try:
        test_check_production_code_for_vulnerability()
    except Exception as e:
        print(f"[FAIL] Test failed: {e}")

    try:
        test_pbt_scheduler_vulnerability()
    except AssertionError as e:
        print(f"[FAIL] Test failed: {e}")

    try:
        test_infer_signals_vulnerability()
    except AssertionError as e:
        print(f"[FAIL] Test failed: {e}")

    print("\n" + "=" * 80)
    print("RECOMMENDATION:")
    print("  Add weights_only=True to ALL torch.load() calls:")
    print("  torch.load(path, map_location='cpu', weights_only=True)")
    print("=" * 80)
