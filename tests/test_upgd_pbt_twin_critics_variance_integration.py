"""
Comprehensive Integration Tests for UPGD + PBT + Twin Critics + Variance Scaling

This test suite validates the integration of all advanced optimization technologies:
1. UPGD Optimizer (Utility-based Perturbed Gradient Descent)
2. Population-Based Training (PBT)
3. Twin Critics (adversarial training)
4. Variance Gradient Scaling (VGS)

Tests check for:
- Component compatibility
- Numerical stability
- Training convergence
- State persistence
- Edge cases and failure modes
"""

import pytest
import torch
import torch.nn as nn
import gymnasium as gym
import numpy as np
from pathlib import Path
from unittest.mock import Mock, patch
from stable_baselines3.common.vec_env import DummyVecEnv
from distributional_ppo import DistributionalPPO
from optimizers import UPGD, AdaptiveUPGD, UPGDW
from variance_gradient_scaler import VarianceGradientScaler
from adversarial.pbt_scheduler import PBTScheduler, PBTConfig, HyperparamConfig


def make_simple_env():
    """Create a simple test environment."""
    return DummyVecEnv([lambda: gym.make("CartPole-v1")])


class TestUPGDWithVarianceScaling:
    """Test UPGD optimizer integration with Variance Gradient Scaler."""

    def test_upgd_vgs_basic_integration(self):
        """Test basic integration of UPGD with VGS."""
        # Create simple model
        model = nn.Sequential(
            nn.Linear(4, 64),
            nn.ReLU(),
            nn.Linear(64, 2)
        )

        # Setup optimizer and VGS
        optimizer = AdaptiveUPGD(model.parameters(), lr=1e-3)
        vgs = VarianceGradientScaler(
            model.parameters(),
            enabled=True,
            alpha=0.1,
            warmup_steps=10
        )

        # Simulate training step
        x = torch.randn(32, 4)
        target = torch.randint(0, 2, (32,))

        optimizer.zero_grad()
        output = model(x)
        loss = nn.functional.cross_entropy(output, target)
        loss.backward()

        # Apply VGS before optimizer step
        scaling_factor = vgs.scale_gradients()
        optimizer.step()
        vgs.step()

        # Check that scaling was applied
        assert isinstance(scaling_factor, float)
        assert 0.0 < scaling_factor <= 1.0

        # Check VGS state updated
        assert vgs._step_count == 1
        assert vgs._grad_mean_ema is not None

    def test_upgd_vgs_numerical_stability(self):
        """Test numerical stability with UPGD + VGS over many steps."""
        model = nn.Sequential(
            nn.Linear(4, 32),
            nn.ReLU(),
            nn.Linear(32, 2)
        )

        optimizer = AdaptiveUPGD(model.parameters(), lr=3e-4, sigma=0.01)
        vgs = VarianceGradientScaler(
            model.parameters(),
            enabled=True,
            beta=0.99,
            alpha=0.1,
            warmup_steps=50
        )

        # Train for many steps
        for step in range(200):
            x = torch.randn(16, 4)
            target = torch.randint(0, 2, (16,))

            optimizer.zero_grad()
            output = model(x)
            loss = nn.functional.cross_entropy(output, target)
            loss.backward()

            vgs.scale_gradients()
            optimizer.step()
            vgs.step()

            # Check no NaN/Inf in parameters
            for param in model.parameters():
                assert torch.all(torch.isfinite(param)), f"NaN/Inf at step {step}"

            # Check VGS statistics are finite
            if vgs._grad_mean_ema is not None:
                assert np.isfinite(vgs._grad_mean_ema), f"VGS mean NaN at step {step}"
                assert np.isfinite(vgs._grad_var_ema), f"VGS var NaN at step {step}"
                assert np.isfinite(vgs.get_normalized_variance()), f"VGS normalized_var NaN at step {step}"

    def test_vgs_warmup_behavior(self):
        """Test that VGS warmup correctly transitions to active scaling."""
        model = nn.Linear(4, 2)

        optimizer = UPGD(model.parameters(), lr=1e-3)
        vgs = VarianceGradientScaler(
            model.parameters(),
            enabled=True,
            warmup_steps=10
        )

        scaling_factors = []

        for step in range(20):
            x = torch.randn(8, 4)
            target = torch.randint(0, 2, (8,))

            optimizer.zero_grad()
            output = model(x)
            loss = nn.functional.cross_entropy(output, target)
            loss.backward()

            scaling_factor = vgs.scale_gradients()
            scaling_factors.append(scaling_factor)

            optimizer.step()
            vgs.step()

        # During warmup (steps 0-9), scaling should be 1.0
        for i in range(10):
            assert scaling_factors[i] == 1.0, f"Step {i} should have scaling=1.0"

        # After warmup (steps 10+), scaling might be < 1.0
        # (depending on gradient variance, but at least it should be computed)
        assert vgs._step_count >= 10

    def test_vgs_disabled_mode(self):
        """Test that VGS in disabled mode doesn't affect gradients."""
        model = nn.Linear(4, 2)

        optimizer = UPGD(model.parameters(), lr=1e-3)
        vgs = VarianceGradientScaler(
            model.parameters(),
            enabled=False  # Disabled
        )

        x = torch.randn(8, 4)
        target = torch.randint(0, 2, (8,))

        optimizer.zero_grad()
        output = model(x)
        loss = nn.functional.cross_entropy(output, target)
        loss.backward()

        # Store original gradients
        original_grads = [p.grad.clone() for p in model.parameters() if p.grad is not None]

        # Apply VGS (should do nothing when disabled)
        scaling_factor = vgs.scale_gradients()
        assert scaling_factor == 1.0

        # Check gradients unchanged
        for orig_grad, param in zip(original_grads, model.parameters()):
            if param.grad is not None:
                assert torch.allclose(param.grad, orig_grad)

    def test_vgs_state_persistence(self):
        """Test VGS state dict save/load."""
        model = nn.Linear(4, 2)
        vgs = VarianceGradientScaler(
            model.parameters(),
            enabled=True,
            beta=0.95,
            alpha=0.2,
            warmup_steps=100
        )

        # Train for a few steps to accumulate state
        optimizer = UPGD(model.parameters(), lr=1e-3)
        for _ in range(50):
            x = torch.randn(8, 4)
            target = torch.randint(0, 2, (8,))

            optimizer.zero_grad()
            output = model(x)
            loss = nn.functional.cross_entropy(output, target)
            loss.backward()

            vgs.scale_gradients()
            optimizer.step()
            vgs.step()

        # Save state
        state_dict = vgs.state_dict()

        # Create new VGS and load state
        vgs_new = VarianceGradientScaler(model.parameters())
        vgs_new.load_state_dict(state_dict)

        # Check state matches
        assert vgs_new.enabled == vgs.enabled
        assert vgs_new.beta == vgs.beta
        assert vgs_new.alpha == vgs.alpha
        assert vgs_new.warmup_steps == vgs.warmup_steps
        assert vgs_new._step_count == vgs._step_count
        assert vgs_new._grad_mean_ema == vgs._grad_mean_ema
        assert vgs_new._grad_var_ema == vgs._grad_var_ema


class TestUPGDWithTwinCritics:
    """Test UPGD optimizer with Twin Critics adversarial training."""

    def test_upgd_twin_critics_basic(self):
        """Test UPGD works with Twin Critics enabled."""
        env = make_simple_env()

        model = DistributionalPPO(
            "MlpPolicy",
            env,
            optimizer_class="adaptive_upgd",
            use_twin_critics=True,  # Enable Twin Critics
            adversarial_training=True,
            n_steps=64,
            n_epochs=2,
            verbose=0,
        )

        # Check Twin Critics is active
        assert hasattr(model.policy, 'critics')
        assert len(model.policy.critics) == 2  # Twin critics

        # Train
        model.learn(total_timesteps=256)

        # Check optimizer state exists for both critics
        optimizer = model.policy.optimizer
        assert isinstance(optimizer, AdaptiveUPGD)
        assert len(optimizer.state) > 0

        env.close()

    def test_upgd_twin_critics_gradient_flow(self):
        """Test that gradients flow correctly through Twin Critics with UPGD."""
        env = make_simple_env()

        model = DistributionalPPO(
            "MlpPolicy",
            env,
            optimizer_class="adaptive_upgd",
            use_twin_critics=True,
            adversarial_training=True,
            n_steps=64,
            n_epochs=2,
            verbose=0,
        )

        # Perform training step
        model.learn(total_timesteps=128)

        # Check that critic parameters have gradients computed
        # (at least during some training steps)
        optimizer = model.policy.optimizer

        # Check optimizer state was created for critic parameters
        critic_params_with_state = 0
        for param_group in optimizer.param_groups:
            for p in param_group['params']:
                if p in optimizer.state:
                    state = optimizer.state[p]
                    # Should have UPGD state
                    if 'avg_utility' in state:
                        critic_params_with_state += 1

        assert critic_params_with_state > 0, "Twin Critics params should have UPGD state"

        env.close()

    def test_twin_critics_numerical_stability_with_upgd(self):
        """Test numerical stability of Twin Critics + UPGD over extended training."""
        env = make_simple_env()

        model = DistributionalPPO(
            "MlpPolicy",
            env,
            optimizer_class="adaptive_upgd",
            optimizer_kwargs={"lr": 3e-4, "sigma": 0.01},
            use_twin_critics=True,
            adversarial_training=True,
            n_steps=64,
            n_epochs=4,
            verbose=0,
        )

        # Train for longer period
        model.learn(total_timesteps=1024)

        # Check all parameters are finite
        for param in model.policy.parameters():
            assert torch.all(torch.isfinite(param)), "Parameters should be finite"

        # Check optimizer state is finite
        optimizer = model.policy.optimizer
        for p in optimizer.state:
            state = optimizer.state[p]
            for key in ['avg_utility', 'first_moment', 'sec_moment']:
                if key in state:
                    assert torch.all(torch.isfinite(state[key])), f"{key} should be finite"

        env.close()


class TestUPGDWithPBT:
    """Test UPGD optimizer with Population-Based Training."""

    def test_pbt_hyperparam_exploration(self):
        """Test that PBT can explore UPGD hyperparameters."""
        config = PBTConfig(
            population_size=3,
            perturbation_interval=5,
            hyperparams=[
                HyperparamConfig(
                    name="lr",
                    min_value=1e-5,
                    max_value=1e-3,
                    is_log_scale=True,
                    perturbation_factor=1.2,
                ),
                HyperparamConfig(
                    name="sigma",
                    min_value=0.0001,
                    max_value=0.1,
                    is_log_scale=True,
                    perturbation_factor=1.5,
                ),
            ],
            exploit_method="truncation",
            explore_method="both",
            truncation_ratio=0.33,
            checkpoint_dir="/tmp/pbt_upgd_test",
        )

        scheduler = PBTScheduler(config, seed=42)
        population = scheduler.initialize_population()

        # Check population initialized
        assert len(population) == 3
        for member in population:
            assert "lr" in member.hyperparams
            assert "sigma" in member.hyperparams
            assert config.hyperparams[0].min_value <= member.hyperparams["lr"] <= config.hyperparams[0].max_value
            assert config.hyperparams[1].min_value <= member.hyperparams["sigma"] <= config.hyperparams[1].max_value

    def test_pbt_exploit_and_explore_with_upgd(self):
        """Test PBT exploit and explore operations with UPGD hyperparameters."""
        config = PBTConfig(
            population_size=4,
            perturbation_interval=10,
            hyperparams=[
                HyperparamConfig(name="lr", min_value=1e-5, max_value=1e-3),
                HyperparamConfig(name="sigma", min_value=0.001, max_value=0.1),
                HyperparamConfig(name="beta_utility", min_value=0.99, max_value=0.999),
            ],
            metric_mode="max",
        )

        scheduler = PBTScheduler(config, seed=123)
        population = scheduler.initialize_population()

        # Simulate performance updates
        for i, member in enumerate(population):
            performance = 10.0 + i * 2.0  # Different performances
            scheduler.update_performance(
                member,
                performance=performance,
                step=5,
                model_state_dict={"dummy": torch.randn(2, 2)},
            )

        # Trigger exploit and explore for worst performer
        worst_member = population[0]  # Lowest performance
        worst_member.step = 10  # Trigger at perturbation interval

        new_state_dict, new_hyperparams = scheduler.exploit_and_explore(
            worst_member,
            model_state_dict={"dummy": torch.randn(2, 2)},
        )

        # Check that hyperparameters were perturbed
        assert "lr" in new_hyperparams
        assert "sigma" in new_hyperparams
        assert "beta_utility" in new_hyperparams

        # May have exploited from better performer
        # (new_state_dict might be None if using binary tournament and lost)

    def test_pbt_population_divergence_prevention(self):
        """Test that PBT prevents population collapse with UPGD."""
        config = PBTConfig(
            population_size=5,
            perturbation_interval=5,
            hyperparams=[
                HyperparamConfig(
                    name="lr",
                    min_value=1e-5,
                    max_value=1e-3,
                    perturbation_factor=1.2,
                ),
            ],
            exploit_method="truncation",
            explore_method="both",
            truncation_ratio=0.2,
        )

        scheduler = PBTScheduler(config, seed=456)
        population = scheduler.initialize_population()

        # Simulate multiple rounds of PBT
        for round_idx in range(10):
            # Update performances
            for i, member in enumerate(population):
                # Random performance with some variance
                performance = np.random.randn() + i * 0.1
                member.step = round_idx * 5
                scheduler.update_performance(
                    member,
                    performance=performance,
                    step=member.step,
                    model_state_dict={"dummy": torch.randn(2, 2)},
                )

            # Exploit and explore for each member
            for member in population:
                if scheduler.should_exploit_and_explore(member):
                    _, new_hyperparams = scheduler.exploit_and_explore(
                        member,
                        model_state_dict={"dummy": torch.randn(2, 2)},
                    )
                    member.hyperparams = new_hyperparams

        # Check population diversity (learning rates should vary)
        learning_rates = [m.hyperparams["lr"] for m in population]
        lr_std = np.std(learning_rates)

        # Should maintain some diversity
        assert lr_std > 1e-8, "Population should maintain diversity"


class TestFullIntegration:
    """Test full integration of UPGD + PBT + Twin Critics + VGS."""

    def test_all_components_together_basic(self):
        """Test all components work together in basic scenario."""
        env = make_simple_env()

        # Create model with all features
        model = DistributionalPPO(
            "MlpPolicy",
            env,
            # UPGD Optimizer
            optimizer_class="adaptive_upgd",
            optimizer_kwargs={"lr": 3e-4, "sigma": 0.01},
            # Twin Critics
            use_twin_critics=True,
            adversarial_training=True,
            # Variance Scaling (if supported via config)
            vgs_enabled=True,
            vgs_alpha=0.1,
            vgs_warmup_steps=50,
            # Training params
            n_steps=64,
            n_epochs=2,
            batch_size=64,
            verbose=0,
        )

        # Should initialize without errors
        assert model.policy.optimizer is not None

        # Train
        model.learn(total_timesteps=256)

        # Check components are working
        assert isinstance(model.policy.optimizer, AdaptiveUPGD)

        env.close()

    def test_full_integration_numerical_stability(self):
        """Test numerical stability with all components active."""
        env = make_simple_env()

        model = DistributionalPPO(
            "MlpPolicy",
            env,
            optimizer_class="adaptive_upgd",
            optimizer_kwargs={"lr": 3e-4, "sigma": 0.01, "beta_utility": 0.999},
            use_twin_critics=True,
            adversarial_training=True,
            vgs_enabled=True,
            vgs_alpha=0.15,
            vgs_warmup_steps=30,
            n_steps=128,
            n_epochs=4,
            batch_size=64,
            max_grad_norm=0.5,
            verbose=0,
        )

        # Train for extended period
        model.learn(total_timesteps=2048)

        # Check all parameters are finite
        for param in model.policy.parameters():
            assert torch.all(torch.isfinite(param)), "Parameters contain NaN/Inf"

        # Check optimizer state
        optimizer = model.policy.optimizer
        for p in optimizer.state:
            state = optimizer.state[p]
            for key, value in state.items():
                if isinstance(value, torch.Tensor):
                    assert torch.all(torch.isfinite(value)), f"Optimizer state {key} contains NaN/Inf"

        env.close()

    def test_save_load_with_all_components(self, tmp_path):
        """Test save/load preserves all component states."""
        env = make_simple_env()

        model = DistributionalPPO(
            "MlpPolicy",
            env,
            optimizer_class="adaptive_upgd",
            optimizer_kwargs={"lr": 1e-4, "sigma": 0.005},
            use_twin_critics=True,
            adversarial_training=True,
            vgs_enabled=True,
            n_steps=64,
            n_epochs=2,
            verbose=0,
        )

        # Train
        model.learn(total_timesteps=256)

        # Get some state before save
        param_before = next(model.policy.parameters()).clone()

        # Save
        save_path = tmp_path / "full_integration_model.zip"
        model.save(save_path)

        # Load
        loaded_model = DistributionalPPO.load(save_path, env=env)

        # Check parameters match
        param_after = next(loaded_model.policy.parameters())
        assert torch.allclose(param_before, param_after, atol=1e-6)

        # Continue training
        loaded_model.learn(total_timesteps=128)

        env.close()

    def test_gradient_flow_all_components(self):
        """Test that gradients flow correctly through all components."""
        env = make_simple_env()

        model = DistributionalPPO(
            "MlpPolicy",
            env,
            optimizer_class="adaptive_upgd",
            use_twin_critics=True,
            adversarial_training=True,
            vgs_enabled=True,
            n_steps=64,
            n_epochs=2,
            verbose=0,
        )

        # Get initial parameters
        initial_params = [p.clone() for p in model.policy.parameters()]

        # Train
        model.learn(total_timesteps=128)

        # Check parameters changed (gradients were applied)
        changed_params = 0
        for initial, current in zip(initial_params, model.policy.parameters()):
            if not torch.allclose(initial, current, atol=1e-6):
                changed_params += 1

        assert changed_params > 0, "Some parameters should have changed during training"

        env.close()


class TestEdgeCasesAndFailureModes:
    """Test edge cases and potential failure modes."""

    def test_zero_gradients_handling(self):
        """Test handling of zero gradients with all components."""
        model = nn.Linear(4, 2)
        optimizer = AdaptiveUPGD(model.parameters(), lr=1e-3)
        vgs = VarianceGradientScaler(model.parameters(), enabled=True, warmup_steps=0)

        # Create scenario with zero gradients
        optimizer.zero_grad()
        # Don't call backward - gradients stay None/zero

        # Should handle gracefully
        scaling_factor = vgs.scale_gradients()
        assert scaling_factor == 1.0  # No gradients to scale

        # Step should not crash
        optimizer.step()
        vgs.step()

    def test_extremely_large_gradients(self):
        """Test stability with extremely large gradients."""
        model = nn.Linear(4, 2)
        optimizer = AdaptiveUPGD(model.parameters(), lr=1e-3, sigma=0.01)
        vgs = VarianceGradientScaler(model.parameters(), enabled=True, alpha=0.5)

        # Create large gradients manually
        x = torch.randn(8, 4)
        target = torch.randint(0, 2, (8,))

        optimizer.zero_grad()
        output = model(x)
        loss = nn.functional.cross_entropy(output, target)
        loss.backward()

        # Scale gradients to be very large
        for param in model.parameters():
            if param.grad is not None:
                param.grad.data.mul_(1000.0)

        # VGS should scale them down
        scaling_factor = vgs.scale_gradients()

        # Apply optimizer step
        optimizer.step()
        vgs.step()

        # Check parameters are still finite
        for param in model.parameters():
            assert torch.all(torch.isfinite(param))

    def test_mixed_precision_compatibility(self):
        """Test compatibility with mixed precision training."""
        env = make_simple_env()

        # Note: This is a basic check, full AMP testing would require more setup
        model = DistributionalPPO(
            "MlpPolicy",
            env,
            optimizer_class="adaptive_upgd",
            use_twin_critics=True,
            vgs_enabled=True,
            n_steps=64,
            verbose=0,
        )

        # Should work with default precision
        model.learn(total_timesteps=128)

        # Parameters should be finite
        for param in model.policy.parameters():
            assert torch.all(torch.isfinite(param))

        env.close()

    def test_batch_size_one_handling(self):
        """Test handling of batch size 1 (edge case for statistics)."""
        model = nn.Linear(4, 2)
        optimizer = UPGD(model.parameters(), lr=1e-3)
        vgs = VarianceGradientScaler(model.parameters(), enabled=True, warmup_steps=0)

        # Single sample batch
        x = torch.randn(1, 4)
        target = torch.randint(0, 2, (1,))

        optimizer.zero_grad()
        output = model(x)
        loss = nn.functional.cross_entropy(output, target)
        loss.backward()

        # Should handle batch size 1
        vgs.scale_gradients()
        optimizer.step()
        vgs.step()

        # Check statistics are computed (or handled gracefully)
        assert vgs._grad_mean_ema is not None or vgs._step_count == 1

    def test_parameter_groups_with_different_lrs(self):
        """Test UPGD with different learning rates for different parameter groups."""
        model = nn.Sequential(
            nn.Linear(4, 32),
            nn.ReLU(),
            nn.Linear(32, 2)
        )

        # Create parameter groups with different LRs
        param_groups = [
            {"params": model[0].parameters(), "lr": 1e-3},
            {"params": model[2].parameters(), "lr": 1e-4},
        ]

        optimizer = AdaptiveUPGD(param_groups)
        vgs = VarianceGradientScaler(model.parameters(), enabled=True)

        # Train
        for _ in range(10):
            x = torch.randn(8, 4)
            target = torch.randint(0, 2, (8,))

            optimizer.zero_grad()
            output = model(x)
            loss = nn.functional.cross_entropy(output, target)
            loss.backward()

            vgs.scale_gradients()
            optimizer.step()
            vgs.step()

        # Check both groups have state
        for group in optimizer.param_groups:
            for p in group['params']:
                if p.requires_grad:
                    assert p in optimizer.state or optimizer.state[p]['step'] >= 0


class TestPerformanceAndConvergence:
    """Test performance characteristics and convergence properties."""

    def test_upgd_convergence_speed(self):
        """Test UPGD convergence on simple task."""
        env = make_simple_env()

        model = DistributionalPPO(
            "MlpPolicy",
            env,
            optimizer_class="adaptive_upgd",
            optimizer_kwargs={"lr": 3e-4},
            n_steps=256,
            n_epochs=10,
            batch_size=256,
            verbose=0,
        )

        # Train for reasonable duration
        model.learn(total_timesteps=5000)

        # Should have learned something (average reward should improve)
        # This is a basic check - full evaluation would need more metrics
        assert len(model.policy.optimizer.state) > 0

        env.close()

    def test_memory_usage_stability(self):
        """Test that memory usage doesn't grow unbounded."""
        env = make_simple_env()

        model = DistributionalPPO(
            "MlpPolicy",
            env,
            optimizer_class="adaptive_upgd",
            use_twin_critics=True,
            vgs_enabled=True,
            n_steps=64,
            n_epochs=2,
            verbose=0,
        )

        # Train multiple times
        for _ in range(5):
            model.learn(total_timesteps=256)

        # Optimizer state should be bounded (one state per parameter)
        num_params = sum(1 for _ in model.policy.parameters())
        num_states = len(model.policy.optimizer.state)

        assert num_states <= num_params, "Optimizer state shouldn't grow beyond number of parameters"

        env.close()


class TestCrossComponentInteractions:
    """Test interactions between different components."""

    def test_vgs_scaling_with_upgd_perturbation(self):
        """Test VGS scaling interacts correctly with UPGD noise perturbation."""
        model = nn.Linear(4, 2)
        optimizer = AdaptiveUPGD(model.parameters(), lr=1e-3, sigma=0.1)  # Large sigma
        vgs = VarianceGradientScaler(model.parameters(), enabled=True, alpha=0.5, warmup_steps=0)

        # Track gradient norms before and after VGS
        grad_norms = []

        for _ in range(20):
            x = torch.randn(16, 4)
            target = torch.randint(0, 2, (16,))

            optimizer.zero_grad()
            output = model(x)
            loss = nn.functional.cross_entropy(output, target)
            loss.backward()

            # Get grad norm before VGS
            grad_norm_before = torch.norm(torch.cat([p.grad.flatten() for p in model.parameters()]))

            # Apply VGS
            scaling_factor = vgs.scale_gradients()

            # Get grad norm after VGS
            grad_norm_after = torch.norm(torch.cat([p.grad.flatten() for p in model.parameters()]))

            grad_norms.append((grad_norm_before.item(), grad_norm_after.item(), scaling_factor))

            # UPGD adds noise, which VGS already scaled
            optimizer.step()
            vgs.step()

        # VGS should have reduced gradient norms when variance is high
        # At least some scaling should have occurred
        some_scaling_occurred = any(sf < 1.0 for _, _, sf in grad_norms)
        # This might not always be true in early steps, but checking structure works

    def test_twin_critics_with_pbt_hyperparams(self):
        """Test Twin Critics works with PBT-perturbed hyperparameters."""
        env = make_simple_env()

        # Initial hyperparameters
        initial_lr = 1e-4
        initial_sigma = 0.005

        model = DistributionalPPO(
            "MlpPolicy",
            env,
            optimizer_class="adaptive_upgd",
            optimizer_kwargs={"lr": initial_lr, "sigma": initial_sigma},
            use_twin_critics=True,
            adversarial_training=True,
            n_steps=64,
            n_epochs=2,
            verbose=0,
        )

        # Train initially
        model.learn(total_timesteps=128)

        # Simulate PBT hyperparameter update
        new_lr = initial_lr * 1.2  # PBT perturbation
        new_sigma = initial_sigma * 0.8

        # Update optimizer hyperparameters
        for param_group in model.policy.optimizer.param_groups:
            param_group['lr'] = new_lr
            param_group['sigma'] = new_sigma

        # Continue training with new hyperparameters
        model.learn(total_timesteps=128)

        # Check hyperparameters were updated
        assert model.policy.optimizer.param_groups[0]['lr'] == new_lr
        assert model.policy.optimizer.param_groups[0]['sigma'] == new_sigma

        env.close()


if __name__ == "__main__":
    pytest.main([__file__, "-v", "--tb=short", "-x"])
