"""
Test suite for PPO critic target clipping fix.

This test verifies that:
1. Targets remain UNCLIPPED in critic loss computation (training section)
2. Targets remain UNCLIPPED in explained variance computation (eval section)
3. Only predictions are clipped, not targets (PPO VF clipping formula)
4. Both quantile and distributional (C51) paths use unclipped targets

Theoretical background:
According to the PPO paper, the value function clipping formula is:
    L^CLIP_VF = max((V(s) - V_targ)^2, (clip(V(s), V_old±ε) - V_targ)^2)

The target V_targ MUST remain unchanged in both terms of the max.
Only the prediction V(s) is clipped in the second term.

Bug description:
Previously, the code was clipping targets during normalization (e.g., clamping
to ±ret_clip or ±value_clip_limit), and then using these clipped targets in the
loss computation. This violated the PPO formula and introduced bias in gradients.

Fix:
Use target_returns_norm_raw (unclipped) instead of target_returns_norm (clipped)
in all loss and explained variance computations.
"""

import pytest
import torch
import numpy as np
from unittest.mock import Mock, patch, MagicMock
from typing import Optional


class TestPPOTargetUnclipped:
    """Test that PPO uses unclipped targets in critic loss."""

    @pytest.fixture
    def mock_ppo_instance(self):
        """Create a mock PPO instance with necessary attributes."""
        ppo = Mock()
        ppo.device = "cpu"
        ppo.normalize_returns = True
        ppo._value_norm_clip_min = -5.0
        ppo._value_norm_clip_max = 5.0
        ppo._value_clip_limit_scaled = 10.0
        ppo._value_clip_limit_unscaled = 100.0
        ppo._use_quantile_value = True
        ppo.logger = Mock()

        # Mock policy
        ppo.policy = Mock()
        ppo.policy.v_min = -10.0
        ppo.policy.v_max = 10.0
        ppo.policy.num_atoms = 51
        ppo.policy.atoms = torch.linspace(-10.0, 10.0, 51)

        return ppo

    def test_training_targets_unclipped_quantile(self):
        """
        Test that training section uses unclipped targets for quantile value head.

        This simulates the code path in distributional_ppo.py around lines 8050-8400.
        """
        # Simulate GAE returns that would be clipped
        raw_returns = torch.tensor([100.0, -100.0, 50.0, -50.0])
        ret_mu = torch.tensor(0.0)
        ret_std = torch.tensor(10.0)

        # Normalization (creates both clipped and unclipped versions)
        target_returns_norm_raw = (raw_returns - ret_mu) / ret_std
        # These would be very large: [10.0, -10.0, 5.0, -5.0]

        value_norm_clip_min = -5.0
        value_norm_clip_max = 5.0

        target_returns_norm = target_returns_norm_raw.clamp(
            value_norm_clip_min, value_norm_clip_max
        )
        # Clipped to: [5.0, -5.0, 5.0, -5.0]

        # CRITICAL: Loss should use UNCLIPPED targets
        # Simulating line 8368: targets_norm_for_loss = target_returns_norm_raw_selected

        # Verify that clipped and unclipped are different
        assert not torch.allclose(target_returns_norm, target_returns_norm_raw)

        # Verify that the unclipped version has the extreme values
        assert target_returns_norm_raw[0].item() == pytest.approx(10.0)
        assert target_returns_norm_raw[1].item() == pytest.approx(-10.0)

        # Verify that the clipped version is clamped
        assert target_returns_norm[0].item() == pytest.approx(5.0)
        assert target_returns_norm[1].item() == pytest.approx(-5.0)

        # The fix ensures we use target_returns_norm_raw, not target_returns_norm
        targets_for_loss = target_returns_norm_raw.reshape(-1, 1)

        # Verify we're using the correct (unclipped) values
        assert targets_for_loss[0, 0].item() == pytest.approx(10.0)
        assert targets_for_loss[1, 0].item() == pytest.approx(-10.0)

    def test_training_targets_unclipped_distributional(self):
        """
        Test that distributional (C51) path uses unclipped targets.

        This simulates the code path around line 8196-8199.
        """
        # Simulate normalized returns
        raw_returns = torch.tensor([100.0, -100.0, 50.0, -50.0])
        ret_mu = torch.tensor(0.0)
        ret_std = torch.tensor(10.0)

        target_returns_norm_raw = (raw_returns - ret_mu) / ret_std
        # [10.0, -10.0, 5.0, -5.0]

        value_norm_clip_min = -5.0
        value_norm_clip_max = 5.0
        target_returns_norm = target_returns_norm_raw.clamp(
            value_norm_clip_min, value_norm_clip_max
        )
        # [5.0, -5.0, 5.0, -5.0]

        # C51 support bounds
        v_min = -10.0
        v_max = 10.0

        # CRITICAL: Should use UNCLIPPED for distributional projection
        # Line 8198: clamped_targets = target_returns_norm_raw.clamp(v_min, v_max)
        clamped_targets_correct = target_returns_norm_raw.clamp(v_min, v_max)

        # WRONG way (old bug): use already-clipped targets
        clamped_targets_wrong = target_returns_norm.clamp(v_min, v_max)

        # Verify difference
        assert not torch.allclose(clamped_targets_correct, clamped_targets_wrong)

        # The correct version should have the full range
        assert clamped_targets_correct[0].item() == pytest.approx(10.0)
        assert clamped_targets_correct[1].item() == pytest.approx(-10.0)

        # The wrong version would be double-clipped
        assert clamped_targets_wrong[0].item() == pytest.approx(5.0)
        assert clamped_targets_wrong[1].item() == pytest.approx(-5.0)

    def test_eval_targets_unclipped(self):
        """
        Test that evaluation section uses unclipped targets.

        This simulates the code path around line 7155-7158.
        """
        # Simulate GAE returns
        raw_returns = torch.tensor([100.0, -100.0, 50.0, -50.0])
        ret_mu = torch.tensor(0.0)
        ret_std = torch.tensor(10.0)

        # Normalization
        target_returns_norm_unclipped = (raw_returns - ret_mu) / ret_std
        value_norm_clip_min = -5.0
        value_norm_clip_max = 5.0
        target_returns_norm = target_returns_norm_unclipped.clamp(
            value_norm_clip_min, value_norm_clip_max
        )

        # CRITICAL: Eval should use UNCLIPPED targets
        # Line 7158: target_norm_col = target_returns_norm_unclipped.reshape(-1, 1)
        target_norm_col = target_returns_norm_unclipped.reshape(-1, 1)

        # Verify we're using unclipped values
        assert target_norm_col[0, 0].item() == pytest.approx(10.0)
        assert target_norm_col[1, 0].item() == pytest.approx(-10.0)

        # Not the clipped ones
        assert target_norm_col[0, 0].item() != pytest.approx(5.0)

    def test_explained_variance_batches_unclipped(self):
        """
        Test that value_target_batches_norm stores unclipped targets.

        This simulates the code path around line 8257-8260.
        """
        # Simulate normalized returns
        raw_returns = torch.tensor([100.0, -100.0, 50.0, -50.0])
        ret_mu = torch.tensor(0.0)
        ret_std = torch.tensor(10.0)

        target_returns_norm_raw = (raw_returns - ret_mu) / ret_std
        value_norm_clip_min = -5.0
        value_norm_clip_max = 5.0
        target_returns_norm = target_returns_norm_raw.clamp(
            value_norm_clip_min, value_norm_clip_max
        )

        # Simulate selection
        valid_indices = torch.tensor([0, 1, 2, 3])
        target_returns_norm_raw_flat = target_returns_norm_raw.reshape(-1)
        target_returns_norm_raw_selected = target_returns_norm_raw_flat[valid_indices]

        # CRITICAL: Should store UNCLIPPED targets
        # Line 8258: target_returns_norm_raw_selected.reshape(-1, 1)
        value_target_batch = target_returns_norm_raw_selected.reshape(-1, 1)

        # Verify unclipped values
        assert value_target_batch[0, 0].item() == pytest.approx(10.0)
        assert value_target_batch[1, 0].item() == pytest.approx(-10.0)

    def test_vf_clipping_formula_correctness(self):
        """
        Test that VF clipping implements the correct PPO formula.

        Formula: L^CLIP_VF = max((V - V_targ)^2, (clip(V, V_old±ε) - V_targ)^2)

        Key: V_targ is the SAME (unclipped) in both terms.
        """
        # Simulated values
        V_pred = torch.tensor([8.0])  # Current prediction
        V_old = torch.tensor([5.0])   # Old prediction
        V_targ_unclipped = torch.tensor([10.0])  # True target (unclipped)
        V_targ_clipped = torch.tensor([5.0])     # Wrongly clipped target
        epsilon = 2.0

        # Correct implementation
        loss_unclipped_correct = (V_pred - V_targ_unclipped) ** 2
        V_pred_clipped = torch.clamp(V_pred, V_old - epsilon, V_old + epsilon)
        loss_clipped_correct = (V_pred_clipped - V_targ_unclipped) ** 2
        loss_correct = torch.max(loss_unclipped_correct, loss_clipped_correct)

        # Wrong implementation (using clipped target)
        loss_unclipped_wrong = (V_pred - V_targ_clipped) ** 2
        loss_clipped_wrong = (V_pred_clipped - V_targ_clipped) ** 2
        loss_wrong = torch.max(loss_unclipped_wrong, loss_clipped_wrong)

        # Verify they're different (bug has impact)
        assert not torch.allclose(loss_correct, loss_wrong)

        # Correct loss should be larger (target is further from prediction)
        assert loss_correct.item() > loss_wrong.item()

        # Calculate expected values
        # V_pred = 8.0, V_targ_unclipped = 10.0
        # loss_unclipped = (8-10)^2 = 4
        # V_pred_clipped = clamp(8, 3, 7) = 7
        # loss_clipped = (7-10)^2 = 9
        # loss = max(4, 9) = 9
        assert loss_correct.item() == pytest.approx(9.0)

        # Wrong calculation:
        # loss_unclipped = (8-5)^2 = 9
        # loss_clipped = (7-5)^2 = 4
        # loss = max(9, 4) = 9
        # In this case they're the same, but generally they differ
        assert loss_wrong.item() == pytest.approx(9.0)

    def test_gradient_impact(self):
        """
        Test that using clipped vs unclipped targets produces different gradients.

        This demonstrates the practical impact of the bug.
        """
        # Create tensors with gradients
        V_pred = torch.tensor([8.0], requires_grad=True)
        V_targ_unclipped = torch.tensor([10.0])
        V_targ_clipped = torch.tensor([5.0])

        # Loss with unclipped target (correct)
        loss_correct = (V_pred - V_targ_unclipped) ** 2
        loss_correct.backward()
        grad_correct = V_pred.grad.clone()

        # Reset gradient
        V_pred.grad = None

        # Loss with clipped target (wrong)
        loss_wrong = (V_pred - V_targ_clipped) ** 2
        loss_wrong.backward()
        grad_wrong = V_pred.grad.clone()

        # Verify gradients are different
        assert not torch.allclose(grad_correct, grad_wrong)

        # Correct gradient: d/dV[(V-10)^2] at V=8 = 2*(8-10) = -4
        assert grad_correct.item() == pytest.approx(-4.0)

        # Wrong gradient: d/dV[(V-5)^2] at V=8 = 2*(8-5) = 6
        assert grad_wrong.item() == pytest.approx(6.0)

        # The bug causes the gradient to point in the WRONG DIRECTION!
        assert grad_correct.item() * grad_wrong.item() < 0

    def test_no_clipping_when_limit_is_none(self):
        """
        Test that when clip limits are None, targets remain truly unclipped.
        """
        raw_returns = torch.tensor([100.0, -100.0, 50.0, -50.0])
        ret_mu = torch.tensor(0.0)
        ret_std = torch.tensor(10.0)

        target_returns_norm_unclipped = (raw_returns - ret_mu) / ret_std

        # When _value_clip_limit_scaled is None
        value_clip_limit_scaled = None

        if value_clip_limit_scaled is not None:
            target_returns_norm = torch.clamp(
                target_returns_norm_unclipped,
                min=-value_clip_limit_scaled,
                max=value_clip_limit_scaled,
            )
        else:
            target_returns_norm = target_returns_norm_unclipped

        # Should be identical
        assert torch.allclose(target_returns_norm, target_returns_norm_unclipped)

    def test_clipping_only_affects_statistics_not_loss(self):
        """
        Test that clipping is only used for statistics/logging, not for loss.
        """
        # Simulate normalized returns
        raw_returns = torch.tensor([100.0, -100.0, 50.0, -50.0])
        ret_mu = torch.tensor(0.0)
        ret_std = torch.tensor(10.0)

        target_returns_norm_raw = (raw_returns - ret_mu) / ret_std
        target_returns_norm = target_returns_norm_raw.clamp(-5.0, 5.0)

        # For loss: use unclipped
        targets_for_loss = target_returns_norm_raw.reshape(-1, 1)

        # For statistics: can use clipped (line 8233)
        target_norm_for_stats = target_returns_norm

        # Verify they're different
        assert not torch.allclose(targets_for_loss.squeeze(), target_norm_for_stats)

        # But loss uses the unclipped version
        assert torch.allclose(targets_for_loss.squeeze(), target_returns_norm_raw)


class TestPPOTargetUnclippedIntegration:
    """Integration tests to verify the fix in realistic scenarios."""

    def test_extreme_returns_not_double_clipped(self):
        """
        Test that extreme returns are not double-clipped.

        Scenario: GAE returns produce extreme values (e.g., ±100).
        After normalization: ±10 (if std=10).
        Normalization clip: ±5.
        C51 support: [-10, 10].

        Bug: First clip to ±5, then clip to [-10, 10] -> stays at ±5 (WRONG!)
        Fix: Don't clip to ±5 for loss, only clip to [-10, 10] for C51 -> ±10 (CORRECT!)
        """
        raw_returns = torch.tensor([100.0, -100.0])
        ret_mu = torch.tensor(0.0)
        ret_std = torch.tensor(10.0)

        # Normalization
        target_returns_norm_raw = (raw_returns - ret_mu) / ret_std  # [10.0, -10.0]
        target_returns_norm = target_returns_norm_raw.clamp(-5.0, 5.0)  # [5.0, -5.0]

        # C51 projection (CORRECT: use unclipped)
        v_min, v_max = -10.0, 10.0
        clamped_for_c51 = target_returns_norm_raw.clamp(v_min, v_max)  # [10.0, -10.0]

        # Verify no double clipping
        assert clamped_for_c51[0].item() == pytest.approx(10.0)
        assert clamped_for_c51[1].item() == pytest.approx(-10.0)

        # NOT double-clipped to ±5
        assert clamped_for_c51[0].item() != pytest.approx(5.0)

    def test_bias_reduction_in_value_estimates(self):
        """
        Test that using unclipped targets reduces bias in value estimates.

        Clipping targets towards zero introduces a systematic bias that makes
        the critic underestimate the magnitude of returns.
        """
        # Simulate a batch of returns with high values
        raw_returns = torch.tensor([80.0, 90.0, 95.0, 100.0, 85.0])
        ret_mu = torch.tensor(0.0)
        ret_std = torch.tensor(10.0)

        target_returns_norm_raw = (raw_returns - ret_mu) / ret_std
        # [8.0, 9.0, 9.5, 10.0, 8.5]

        target_returns_norm_clipped = target_returns_norm_raw.clamp(-5.0, 5.0)
        # [5.0, 5.0, 5.0, 5.0, 5.0] - all clipped to 5.0!

        # Using clipped targets would train the critic to predict ~5.0
        # But the true values are ~9.0 on average

        mean_unclipped = target_returns_norm_raw.mean()
        mean_clipped = target_returns_norm_clipped.mean()

        # Huge bias introduced by clipping!
        bias = (mean_clipped - mean_unclipped).abs()
        assert bias.item() > 3.0  # More than 3.0 units of bias

        # Correct approach uses unclipped targets
        assert mean_unclipped.item() == pytest.approx(9.0)
        assert mean_clipped.item() == pytest.approx(5.0)


if __name__ == "__main__":
    pytest.main([__file__, "-v", "--tb=short"])
