"""
Comprehensive tests for bug fixes (2025-11-22)

This module contains tests for three bug fixes:
1. BUG #1: SA-PPO Epsilon Schedule (ALREADY FIXED - verification test)
2. BUG #2: PBT Ready Percentage Deadlock (NEW FIX)
3. BUG #3: Quantile Monotonicity Not Enforced (NEW FIX)

All tests follow research-backed approaches and best practices.
"""

import pytest
import torch
import numpy as np
from pathlib import Path
import tempfile
import shutil

from adversarial.sa_ppo import StateAdversarialPPO, SAPPOConfig
from adversarial.pbt_scheduler import PBTScheduler, PBTConfig, HyperparamConfig, PopulationMember
from adversarial.state_perturbation import PerturbationConfig
from custom_policy_patch1 import QuantileValueHead


# ============================================================================
# BUG #1: SA-PPO Epsilon Schedule - Verification Tests
# ============================================================================


class TestSAPPOEpsilonSchedule:
    """Test that SA-PPO epsilon schedule is computed correctly (BUG #1 - already fixed)."""

    def test_epsilon_schedule_uses_hardcoded_max_updates(self):
        """Verify epsilon schedule uses hardcoded max_updates (currently 1000)."""
        # NOTE: This test verifies the current implementation where max_updates
        # is hardcoded to 1000 in _get_current_epsilon(). This was reported as
        # Bug #1 but is actually a FALSE POSITIVE - the code works correctly.

        config = SAPPOConfig(
            enabled=True,
            adaptive_epsilon=True,
            epsilon_schedule="linear",
            perturbation=PerturbationConfig(epsilon=0.1),
            epsilon_final=0.05,
        )

        # Mock model with minimal interface
        class MockModel:
            class Policy:
                def get_distribution(self, obs):
                    pass
                def predict_values(self, obs):
                    return torch.zeros(obs.size(0))
            policy = Policy()

        sa_ppo = StateAdversarialPPO(
            config=config,
            model=MockModel(),
        )

        # Verify epsilon progresses correctly with hardcoded max_updates=1000
        test_cases = [
            (0, 0.10),      # 0% progress: epsilon_init
            (500, 0.075),   # 50% progress: (0.1 + 0.05) / 2
            (1000, 0.05),   # 100% progress: epsilon_final
        ]

        for update_count, expected_epsilon in test_cases:
            sa_ppo._update_count = update_count
            actual_epsilon = sa_ppo._get_current_epsilon()
            assert abs(actual_epsilon - expected_epsilon) < 1e-6, (
                f"At update {update_count}, expected epsilon={expected_epsilon}, "
                f"got {actual_epsilon}"
            )

    def test_epsilon_schedule_linear_progression(self):
        """Verify linear epsilon schedule progresses correctly."""
        # NOTE: This test verifies linear interpolation with hardcoded max_updates=1000
        config = SAPPOConfig(
            enabled=True,
            adaptive_epsilon=True,
            epsilon_schedule="linear",
            perturbation=PerturbationConfig(epsilon=0.1),
            epsilon_final=0.05,
        )

        class MockModel:
            class Policy:
                def get_distribution(self, obs):
                    pass
                def predict_values(self, obs):
                    return torch.zeros(obs.size(0))
            policy = Policy()

        sa_ppo = StateAdversarialPPO(config=config, model=MockModel())

        # Test epsilon at different progress points (hardcoded max_updates=1000)
        test_cases = [
            (0, 0.10),      # Start: epsilon_init
            (250, 0.0875),  # 25%: 0.1 + (0.05 - 0.1) * 0.25
            (500, 0.075),   # 50%: 0.1 + (0.05 - 0.1) * 0.5
            (750, 0.0625),  # 75%: 0.1 + (0.05 - 0.1) * 0.75
            (1000, 0.05),   # End: epsilon_final
            (1500, 0.05),   # After max_updates: should clamp to epsilon_final
        ]

        for update_count, expected_epsilon in test_cases:
            sa_ppo._update_count = update_count
            actual_epsilon = sa_ppo._get_current_epsilon()
            assert abs(actual_epsilon - expected_epsilon) < 1e-6, (
                f"At update {update_count}: expected epsilon={expected_epsilon:.4f}, "
                f"got {actual_epsilon:.4f}"
            )

    def test_epsilon_constant_when_adaptive_disabled(self):
        """Verify epsilon remains constant when adaptive_epsilon=False."""
        # NOTE: This test verifies that epsilon doesn't change when adaptation is disabled
        config = SAPPOConfig(
            enabled=True,
            adaptive_epsilon=False,  # Disable adaptation
            epsilon_schedule="linear",  # Should be ignored
            perturbation=PerturbationConfig(epsilon=0.1),
            epsilon_final=0.05,  # Should be ignored
        )

        class MockModel:
            class Policy:
                def get_distribution(self, obs):
                    pass
                def predict_values(self, obs):
                    return torch.zeros(obs.size(0))
            policy = Policy()

        sa_ppo = StateAdversarialPPO(config=config, model=MockModel())

        # Epsilon should remain constant at initial value (0.1)
        expected_epsilon = 0.1
        for update_count in [0, 100, 500, 1000, 5000]:
            sa_ppo._update_count = update_count
            actual_epsilon = sa_ppo._get_current_epsilon()
            assert abs(actual_epsilon - expected_epsilon) < 1e-6, (
                f"At update {update_count}: expected constant epsilon={expected_epsilon}, "
                f"got {actual_epsilon}"
            )


# ============================================================================
# BUG #2: PBT Ready Percentage Deadlock - NEW TESTS
# ============================================================================


class TestPBTDeadlockPrevention:
    """Test PBT deadlock prevention mechanism (BUG #2 - new fix)."""

    @pytest.fixture
    def temp_checkpoint_dir(self):
        """Create temporary directory for PBT checkpoints."""
        tmpdir = tempfile.mkdtemp()
        yield tmpdir
        shutil.rmtree(tmpdir)

    def test_pbt_fallback_activates_after_max_wait(self, temp_checkpoint_dir):
        """Verify PBT fallback mechanism activates after max consecutive failures."""
        config = PBTConfig(
            population_size=10,
            perturbation_interval=5,
            hyperparams=[
                HyperparamConfig(name="lr", min_value=1e-5, max_value=1e-3, is_log_scale=True)
            ],
            ready_percentage=0.8,  # Requires 8/10 members ready
            min_ready_members=2,   # Fallback: allow PBT with >= 2 members
            ready_check_max_wait=10,  # Fallback after 10 consecutive failures
            checkpoint_dir=temp_checkpoint_dir,
        )

        scheduler = PBTScheduler(config, seed=42)
        population = scheduler.initialize_population()

        # Simulate scenario: only 3 members have performance (< 8 required)
        # This would normally block PBT indefinitely
        for i in range(3):
            population[i].performance = float(i)
            population[i].step = 5

        # First 9 calls should skip PBT (not enough ready members)
        for attempt in range(9):
            result = scheduler.exploit_and_explore(population[0])
            new_params, new_hyperparams, checkpoint_format = result

            assert new_params is None, f"Attempt {attempt + 1}: Should skip PBT (not enough ready members)"
            assert scheduler._failed_ready_checks == attempt + 1

        # 10th call should trigger fallback (3 members >= min_ready_members=2)
        result = scheduler.exploit_and_explore(population[0])
        new_params, new_hyperparams, checkpoint_format = result

        # Fallback activated: should proceed with exploration (even without exploitation)
        assert new_hyperparams is not None, "Fallback should allow exploration to proceed"
        assert scheduler._failed_ready_checks == 0, "Failed checks counter should reset after fallback"

    def test_pbt_no_fallback_if_insufficient_min_members(self, temp_checkpoint_dir):
        """Verify fallback does NOT activate if ready_count < min_ready_members."""
        config = PBTConfig(
            population_size=10,
            perturbation_interval=5,
            hyperparams=[
                HyperparamConfig(name="lr", min_value=1e-5, max_value=1e-3)
            ],
            ready_percentage=0.8,  # Requires 8/10
            min_ready_members=3,   # Minimum 3 members
            ready_check_max_wait=5,
            checkpoint_dir=temp_checkpoint_dir,
        )

        scheduler = PBTScheduler(config, seed=42)
        population = scheduler.initialize_population()

        # Only 2 members ready (< min_ready_members=3)
        for i in range(2):
            population[i].performance = float(i)
            population[i].step = 5

        # Even after max_wait, should NOT trigger fallback
        for attempt in range(10):
            result = scheduler.exploit_and_explore(population[0])
            new_params, new_hyperparams, checkpoint_format = result

            assert new_params is None, f"Attempt {attempt + 1}: Should skip PBT (insufficient min_ready_members)"

    def test_pbt_failed_checks_reset_when_sufficient_members_ready(self, temp_checkpoint_dir):
        """Verify failed_ready_checks counter resets when sufficient members become ready."""
        config = PBTConfig(
            population_size=10,
            perturbation_interval=5,
            hyperparams=[
                HyperparamConfig(name="lr", min_value=1e-5, max_value=1e-3)
            ],
            ready_percentage=0.8,
            min_ready_members=2,
            ready_check_max_wait=10,
            checkpoint_dir=temp_checkpoint_dir,
        )

        scheduler = PBTScheduler(config, seed=42)
        population = scheduler.initialize_population()

        # Initially 3 members ready (< 8 required)
        for i in range(3):
            population[i].performance = float(i)
            population[i].step = 5
            population[i].checkpoint_path = Path(temp_checkpoint_dir) / f"member_{i}.pt"
            # Save dummy checkpoint
            torch.save({"format_version": "v2_full_parameters", "data": {}}, population[i].checkpoint_path)

        # Accumulate 5 failed checks
        for _ in range(5):
            scheduler.exploit_and_explore(population[0])
        assert scheduler._failed_ready_checks == 5

        # Now make 8 members ready (sufficient)
        for i in range(3, 8):
            population[i].performance = float(i)
            population[i].step = 5
            population[i].checkpoint_path = Path(temp_checkpoint_dir) / f"member_{i}.pt"
            torch.save({"format_version": "v2_full_parameters", "data": {}}, population[i].checkpoint_path)

        # Next call should reset counter
        scheduler.exploit_and_explore(population[0])
        assert scheduler._failed_ready_checks == 0, "Counter should reset when sufficient members ready"

    def test_pbt_stats_include_failed_ready_checks(self, temp_checkpoint_dir):
        """Verify get_stats() includes failed_ready_checks metric."""
        config = PBTConfig(
            population_size=5,
            checkpoint_dir=temp_checkpoint_dir,
        )

        scheduler = PBTScheduler(config, seed=42)
        scheduler.initialize_population()

        stats = scheduler.get_stats()
        assert "pbt/failed_ready_checks" in stats
        assert stats["pbt/failed_ready_checks"] == 0

        # Trigger some failed checks
        scheduler._failed_ready_checks = 7
        stats = scheduler.get_stats()
        assert stats["pbt/failed_ready_checks"] == 7


# ============================================================================
# BUG #3: Quantile Monotonicity - NEW TESTS
# ============================================================================


class TestQuantileMonotonicity:
    """Test optional quantile monotonicity enforcement (BUG #3 - new fix)."""

    def test_quantile_head_without_monotonicity(self):
        """Verify QuantileValueHead without monotonicity allows non-monotonic outputs."""
        head = QuantileValueHead(
            input_dim=64,
            num_quantiles=5,
            huber_kappa=1.0,
            enforce_monotonicity=False,  # Default behavior
        )

        # Create input that might produce non-monotonic quantiles
        # Use specific weights to force non-monotonic output
        with torch.no_grad():
            # Set weights to produce: [5, 2, 8, 1, 9] (non-monotonic)
            head.linear.weight.zero_()
            head.linear.bias.copy_(torch.tensor([5.0, 2.0, 8.0, 1.0, 9.0]))

        latent = torch.ones(1, 64)
        quantiles = head.forward(latent)

        # Should NOT be sorted (monotonicity not enforced)
        expected = torch.tensor([[5.0, 2.0, 8.0, 1.0, 9.0]])
        assert torch.allclose(quantiles, expected), (
            f"Without monotonicity: expected {expected}, got {quantiles}"
        )

    def test_quantile_head_with_monotonicity(self):
        """Verify QuantileValueHead with monotonicity enforces sorted outputs."""
        head = QuantileValueHead(
            input_dim=64,
            num_quantiles=5,
            huber_kappa=1.0,
            enforce_monotonicity=True,  # Enable sorting
        )

        # Create input that produces non-monotonic quantiles
        with torch.no_grad():
            head.linear.weight.zero_()
            head.linear.bias.copy_(torch.tensor([5.0, 2.0, 8.0, 1.0, 9.0]))

        latent = torch.ones(1, 64)
        quantiles = head.forward(latent)

        # Should be sorted: [1, 2, 5, 8, 9]
        expected_sorted = torch.tensor([[1.0, 2.0, 5.0, 8.0, 9.0]])
        assert torch.allclose(quantiles, expected_sorted), (
            f"With monotonicity: expected sorted {expected_sorted}, got {quantiles}"
        )

    def test_quantile_monotonicity_preserves_gradients(self):
        """Verify monotonicity enforcement is differentiable (gradients flow)."""
        head = QuantileValueHead(
            input_dim=64,
            num_quantiles=5,
            huber_kappa=1.0,
            enforce_monotonicity=True,
        )

        latent = torch.randn(4, 64, requires_grad=True)
        quantiles = head.forward(latent)

        # Verify monotonicity
        for i in range(quantiles.size(0)):
            sorted_quantiles, _ = torch.sort(quantiles[i])
            assert torch.allclose(quantiles[i], sorted_quantiles), (
                f"Sample {i}: quantiles not sorted"
            )

        # Verify gradients flow
        loss = quantiles.sum()
        loss.backward()

        assert latent.grad is not None, "Gradients should flow through sorting"
        assert not torch.isnan(latent.grad).any(), "Gradients should not be NaN"
        assert not torch.isinf(latent.grad).any(), "Gradients should not be inf"

    def test_quantile_monotonicity_batch_consistency(self):
        """Verify monotonicity is enforced independently for each batch element."""
        head = QuantileValueHead(
            input_dim=64,
            num_quantiles=10,
            huber_kappa=1.0,
            enforce_monotonicity=True,
        )

        batch_size = 32
        latent = torch.randn(batch_size, 64)
        quantiles = head.forward(latent)

        # Check each sample is monotonic
        for i in range(batch_size):
            for j in range(quantiles.size(1) - 1):
                assert quantiles[i, j] <= quantiles[i, j + 1], (
                    f"Sample {i}: Q[{j}]={quantiles[i, j]:.4f} > Q[{j + 1}]={quantiles[i, j + 1]:.4f}"
                )

    def test_quantile_monotonicity_preserves_tau_alignment(self):
        """Verify monotonicity doesn't break tau (quantile level) alignment."""
        num_quantiles = 21
        head = QuantileValueHead(
            input_dim=64,
            num_quantiles=num_quantiles,
            huber_kappa=1.0,
            enforce_monotonicity=True,
        )

        # Verify tau values are still correct (midpoint formula)
        expected_taus = torch.tensor([(i + 0.5) / num_quantiles for i in range(num_quantiles)])
        assert torch.allclose(head.taus, expected_taus, atol=1e-6), (
            "Monotonicity enforcement should not change tau values"
        )

    def test_quantile_monotonicity_default_is_false(self):
        """Verify default behavior is enforce_monotonicity=False (backward compatibility)."""
        head = QuantileValueHead(
            input_dim=64,
            num_quantiles=5,
            huber_kappa=1.0,
            # enforce_monotonicity not specified - should default to False
        )

        assert head.enforce_monotonicity is False, (
            "Default enforce_monotonicity should be False for backward compatibility"
        )


# ============================================================================
# Integration Tests
# ============================================================================


class TestBugFixesIntegration:
    """Integration tests combining multiple bug fixes."""

    @pytest.fixture
    def temp_checkpoint_dir(self):
        tmpdir = tempfile.mkdtemp()
        yield tmpdir
        shutil.rmtree(tmpdir)

    def test_pbt_with_quantile_monotonicity(self, temp_checkpoint_dir):
        """Integration test: PBT with quantile heads using monotonicity enforcement."""
        # This tests that the fixes work together in a realistic scenario

        # Create quantile heads with monotonicity
        critic_head = QuantileValueHead(
            input_dim=128,
            num_quantiles=21,
            huber_kappa=1.0,
            enforce_monotonicity=True,
        )

        # Create PBT scheduler with deadlock prevention
        pbt_config = PBTConfig(
            population_size=4,
            perturbation_interval=10,
            hyperparams=[
                HyperparamConfig(name="learning_rate", min_value=1e-5, max_value=1e-3, is_log_scale=True),
            ],
            ready_percentage=0.75,  # 3/4 required
            min_ready_members=2,    # Fallback to 2
            ready_check_max_wait=5,
            checkpoint_dir=temp_checkpoint_dir,
        )

        scheduler = PBTScheduler(pbt_config, seed=42)
        population = scheduler.initialize_population()

        # Simulate training: some members produce non-monotonic quantiles
        latent = torch.randn(8, 128)
        quantiles = critic_head(latent)

        # Verify monotonicity enforced
        for i in range(quantiles.size(0)):
            for j in range(quantiles.size(1) - 1):
                assert quantiles[i, j] <= quantiles[i, j + 1], (
                    f"Integration test: quantiles not monotonic at sample {i}, position {j}"
                )

        # Verify PBT can handle partial population (deadlock prevention)
        # Only 2 members ready (< 75% = 3, but >= min_ready_members = 2)
        for i in range(2):
            population[i].performance = float(i)
            population[i].step = 10
            checkpoint_path = Path(temp_checkpoint_dir) / f"member_{i}.pt"
            population[i].checkpoint_path = str(checkpoint_path)
            # Save checkpoint with quantile head state
            torch.save({
                "format_version": "v2_full_parameters",
                "data": {"quantile_head": critic_head.state_dict()}
            }, checkpoint_path)

        # Trigger fallback after max_wait
        for _ in range(4):
            scheduler.exploit_and_explore(population[0])

        # 5th call should activate fallback (2 >= min_ready_members)
        assert scheduler._failed_ready_checks == 4, "Should have 4 failed checks before fallback"

        result = scheduler.exploit_and_explore(population[0])
        new_params, new_hyperparams, checkpoint_format = result

        assert new_hyperparams is not None, "PBT should proceed with fallback"
        # After fallback, counter is reset to 0, but next call will increment it again
        # because ready_count is still < required_count (this is expected behavior)
        assert scheduler._failed_ready_checks == 0, "Fallback should reset counter to 0"

        # Next call will increment counter again (ready_count still insufficient)
        scheduler.exploit_and_explore(population[0])
        assert scheduler._failed_ready_checks == 1, "Counter increments again after fallback"


if __name__ == "__main__":
    pytest.main([__file__, "-v", "--tb=short"])
