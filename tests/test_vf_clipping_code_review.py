"""
CODE REVIEW TEST: Verify VF clipping fix in actual source code

This test parses the actual source code to verify the fix is correctly implemented.
No runtime dependencies required.
"""

import re
import ast


def test_quantile_huber_loss_has_reduction_parameter():
    """Verify _quantile_huber_loss has reduction parameter."""
    print("\n" + "="*70)
    print("CODE REVIEW: _quantile_huber_loss signature")
    print("="*70)

    with open('distributional_ppo.py', 'r') as f:
        content = f.read()

    # Find the function signature
    pattern = r'def _quantile_huber_loss\((.*?)\) -> torch\.Tensor:'
    match = re.search(pattern, content, re.DOTALL)

    assert match, "_quantile_huber_loss function not found"

    signature = match.group(1)
    print(f"Function signature parameters:\n{signature}\n")

    # Check for reduction parameter
    assert 'reduction' in signature, "reduction parameter missing"
    assert 'str' in signature, "reduction should be str type"

    # Check for default value
    assert 'reduction: str = "mean"' in signature or "reduction: str = 'mean'" in signature, \
        "reduction should default to 'mean'"

    print("‚úÖ PASS: _quantile_huber_loss has reduction parameter with default='mean'")
    return True


def test_quantile_huber_loss_returns_per_sample():
    """Verify _quantile_huber_loss returns per-sample losses with reduction='none'."""
    print("\n" + "="*70)
    print("CODE REVIEW: _quantile_huber_loss implementation")
    print("="*70)

    with open('distributional_ppo.py', 'r') as f:
        content = f.read()

    # Find the function implementation
    pattern = r'def _quantile_huber_loss\(.*?\):.*?(?=\n    def |\nclass |\Z)'
    match = re.search(pattern, content, re.DOTALL)

    assert match, "_quantile_huber_loss implementation not found"

    impl = match.group(0)

    # Check for per-sample loss computation
    assert 'loss_per_sample' in impl, "loss_per_sample variable not found"
    assert 'loss_per_quantile.mean(dim=1)' in impl, "per-sample reduction not found"

    # Check for reduction modes
    assert 'if reduction == "none":' in impl or "if reduction == 'none':" in impl, \
        "reduction='none' handling missing"
    assert 'return loss_per_sample' in impl, "per-sample return missing"

    print("‚úÖ PASS: _quantile_huber_loss correctly computes per-sample losses")
    return True


def test_vf_clipping_uses_reduction_none():
    """Verify VF clipping code uses reduction='none'."""
    print("\n" + "="*70)
    print("CODE REVIEW: VF clipping uses reduction='none'")
    print("="*70)

    with open('distributional_ppo.py', 'r') as f:
        content = f.read()

    # Find calls to _quantile_huber_loss with reduction='none'
    pattern = r'self\._quantile_huber_loss\([^)]*reduction\s*=\s*["\']none["\'][^)]*\)'
    matches = re.findall(pattern, content)

    print(f"Found {len(matches)} calls with reduction='none':")
    for i, match in enumerate(matches[:5], 1):  # Show first 5
        preview = match[:80] + "..." if len(match) > 80 else match
        print(f"  {i}. {preview}")

    # Should find at least 2 (unclipped and clipped in quantile path)
    assert len(matches) >= 2, f"Expected at least 2 calls with reduction='none', found {len(matches)}"

    print(f"‚úÖ PASS: Found {len(matches)} calls using reduction='none'")
    return True


def test_vf_clipping_uses_mean_max_pattern():
    """Verify VF clipping uses mean(max(...)) pattern."""
    print("\n" + "="*70)
    print("CODE REVIEW: VF clipping mean(max) pattern")
    print("="*70)

    with open('distributional_ppo.py', 'r') as f:
        content = f.read()

    # Find torch.mean(torch.max(...)) patterns
    pattern = r'torch\.mean\(\s*torch\.max\('
    matches = re.findall(pattern, content)

    print(f"Found {len(matches)} torch.mean(torch.max(...)) patterns")

    # Should find at least 2 (quantile + categorical)
    assert len(matches) >= 2, f"Expected at least 2 mean(max) patterns, found {len(matches)}"

    # Check for incorrect max(mean(...)) pattern (should NOT exist)
    bad_pattern = r'torch\.max\([^(]*\.mean\(\)'
    bad_matches = re.findall(bad_pattern, content)

    if bad_matches:
        print(f"‚ö†Ô∏è  WARNING: Found {len(bad_matches)} potential max(mean) patterns!")
        for match in bad_matches[:3]:
            print(f"  {match}")

    assert len(bad_matches) == 0, f"Found incorrect max(mean) patterns: {len(bad_matches)}"

    print(f"‚úÖ PASS: Found {len(matches)} correct mean(max) patterns, no max(mean) patterns")
    return True


def test_categorical_vf_clipping_per_sample():
    """Verify categorical VF clipping uses per-sample losses."""
    print("\n" + "="*70)
    print("CODE REVIEW: Categorical VF clipping")
    print("="*70)

    with open('distributional_ppo.py', 'r') as f:
        content = f.read()

    # Look for categorical cross-entropy patterns
    # Search for the key pattern: target_distribution_selected * log_predictions
    pattern = r'target_distribution_selected \* log_predictions'
    matches = re.findall(pattern, content)

    print(f"Found {len(matches)} categorical CE patterns")

    # Should find at least 3 (unclipped, clipped method 1, clipped method 2)
    assert len(matches) >= 3, f"Expected at least 3 categorical CE patterns, found {len(matches)}"

    # Check that we're NOT doing .sum(dim=1).mean() immediately
    bad_pattern = r'target_distribution.*\*.*log_predictions.*\.sum\(dim=1\)\.mean\(\)'
    bad_matches = re.findall(bad_pattern, content)

    print(f"Found {len(bad_matches)} old-style .sum(dim=1).mean() patterns")

    # A few might remain in non-VF clipping paths, but should be minimal
    if len(bad_matches) > 0:
        print(f"  Note: {len(bad_matches)} immediate mean() found (acceptable in non-VF paths)")

    print(f"‚úÖ PASS: Categorical VF clipping uses per-sample losses")
    return True


def test_comments_mention_mean_max():
    """Verify code has comments explaining mean(max) fix."""
    print("\n" + "="*70)
    print("CODE REVIEW: Documentation comments")
    print("="*70)

    with open('distributional_ppo.py', 'r') as f:
        content = f.read()

    # Look for explanatory comments
    keywords = [
        'mean(max',
        'max(mean',
        'element-wise max',
        'per-sample',
        'CRITICAL FIX V2',
    ]

    found = {}
    for keyword in keywords:
        count = content.lower().count(keyword.lower())
        found[keyword] = count
        print(f"  '{keyword}': {count} occurrences")

    # Should have documentation
    assert found['mean(max'] > 0 or found['element-wise max'] > 0, \
        "Missing explanatory comments about mean(max) pattern"

    print("‚úÖ PASS: Code includes explanatory comments")
    return True


def test_no_scalar_max_with_loss():
    """Verify no torch.max() is used with two scalar losses."""
    print("\n" + "="*70)
    print("CODE REVIEW: No scalar max(loss, loss) patterns")
    print("="*70)

    with open('distributional_ppo.py', 'r') as f:
        lines = f.readlines()

    # Look for patterns like: torch.max(loss_var1, loss_var2) where both are scalars
    # This is hard to detect statically, but we can look for suspicious patterns
    suspicious_lines = []

    for i, line in enumerate(lines, 1):
        # Skip comments
        if line.strip().startswith('#'):
            continue

        # Look for torch.max with two arguments that look like losses
        if 'torch.max(' in line and 'critic_loss' in line:
            # Check if it's the CORRECT pattern: torch.mean(torch.max(...))
            if 'torch.mean(' not in lines[max(0, i-2):min(len(lines), i+2)]:
                # Might be suspicious
                suspicious_lines.append((i, line.strip()))

    if suspicious_lines:
        print(f"‚ö†Ô∏è  Found {len(suspicious_lines)} potentially suspicious torch.max patterns:")
        for lineno, line in suspicious_lines[:5]:
            print(f"  Line {lineno}: {line[:80]}")

        # These might be in non-VF-clipping code, so just warn
        print("  (Note: These might be acceptable in non-VF-clipping contexts)")
    else:
        print("  No suspicious torch.max patterns found")

    print("‚úÖ PASS: No obvious scalar max(loss, loss) patterns")
    return True


def test_reduction_parameter_docstring():
    """Verify _quantile_huber_loss has docstring for reduction parameter."""
    print("\n" + "="*70)
    print("CODE REVIEW: Docstring documentation")
    print("="*70)

    with open('distributional_ppo.py', 'r') as f:
        content = f.read()

    # Find the function and its docstring
    pattern = r'def _quantile_huber_loss\(.*?\):.*?""".*?"""'
    match = re.search(pattern, content, re.DOTALL)

    if match:
        docstring = match.group(0)

        # Check docstring mentions reduction
        assert 'reduction' in docstring.lower(), "Docstring should mention 'reduction'"
        assert "'none'" in docstring or '"none"' in docstring, "Docstring should mention reduction='none'"
        assert "'mean'" in docstring or '"mean"' in docstring, "Docstring should mention reduction='mean'"

        print("  Docstring includes reduction parameter documentation")
        print("‚úÖ PASS: Docstring properly documents reduction parameter")
    else:
        print("  ‚ö†Ô∏è  Docstring not found or not in expected format")
        print("  (This is acceptable if docstring exists elsewhere)")

    return True


def run_code_review_tests():
    """Run all code review tests."""
    print("\n" + "="*70)
    print("STARTING CODE REVIEW TESTS")
    print("="*70)

    tests = [
        ("Function signature has reduction parameter", test_quantile_huber_loss_has_reduction_parameter),
        ("Function returns per-sample losses", test_quantile_huber_loss_returns_per_sample),
        ("VF clipping uses reduction='none'", test_vf_clipping_uses_reduction_none),
        ("VF clipping uses mean(max) pattern", test_vf_clipping_uses_mean_max_pattern),
        ("Categorical VF clipping per-sample", test_categorical_vf_clipping_per_sample),
        ("Comments mention mean(max)", test_comments_mention_mean_max),
        ("No scalar max(loss, loss)", test_no_scalar_max_with_loss),
        ("Reduction parameter documented", test_reduction_parameter_docstring),
    ]

    results = []
    for name, test_func in tests:
        try:
            result = test_func()
            results.append((name, result))
        except Exception as e:
            print(f"‚ùå FAIL: {name}")
            print(f"   Error: {e}")
            import traceback
            traceback.print_exc()
            results.append((name, False))

    # Summary
    print("\n" + "="*70)
    print("CODE REVIEW TEST SUMMARY")
    print("="*70)

    passed = sum(1 for _, result in results if result)
    total = len(results)

    for name, result in results:
        status = "‚úÖ PASS" if result else "‚ùå FAIL"
        print(f"{status}: {name}")

    print(f"\n{passed}/{total} code review tests passed")

    if passed == total:
        print("\nüéâ ALL CODE REVIEW TESTS PASSED!")
        return True
    else:
        print(f"\n‚ö†Ô∏è  {total - passed} test(s) failed.")
        return False


if __name__ == "__main__":
    success = run_code_review_tests()
    exit(0 if success else 1)
