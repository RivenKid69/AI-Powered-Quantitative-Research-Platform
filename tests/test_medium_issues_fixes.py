#!/usr/bin/env python3
"""
Comprehensive test suite for MEDIUM priority issues fixes.

Tests all 6 code fixes applied to resolve MEDIUM issues #1, #3, #4, #5, #9.
Each test is clearly labeled with the issue number it addresses.
"""

import pytest
import numpy as np
import pandas as pd
import math
from pathlib import Path
import sys

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))


# ==============================================================================
# MEDIUM #1: Return Fallback NaN (reward.pyx)
# ==============================================================================

class TestMedium1_ReturnFallbackNaN:
    """Test that log_return returns NaN for invalid inputs instead of 0.0."""

    def test_log_return_invalid_prev_net_worth(self):
        """Test that log_return returns NaN when prev_net_worth is invalid."""
        # Import the Cython function (if available)
        try:
            from reward import log_return
        except ImportError:
            pytest.skip("reward module not compiled, skipping Cython test")

        # Test with prev_net_worth = 0.0
        result = log_return(100.0, 0.0)
        assert np.isnan(result), "log_return should return NaN when prev_net_worth is 0.0"

        # Test with prev_net_worth < 0.0
        result = log_return(100.0, -10.0)
        assert np.isnan(result), "log_return should return NaN when prev_net_worth is negative"

    def test_log_return_invalid_net_worth(self):
        """Test that log_return returns NaN when net_worth is invalid."""
        try:
            from reward import log_return
        except ImportError:
            pytest.skip("reward module not compiled, skipping Cython test")

        # Test with net_worth = 0.0
        result = log_return(0.0, 100.0)
        assert np.isnan(result), "log_return should return NaN when net_worth is 0.0"

        # Test with net_worth < 0.0
        result = log_return(-10.0, 100.0)
        assert np.isnan(result), "log_return should return NaN when net_worth is negative"

    def test_log_return_valid_inputs(self):
        """Test that log_return returns correct value for valid inputs."""
        try:
            from reward import log_return
        except ImportError:
            pytest.skip("reward module not compiled, skipping Cython test")

        # Test normal case: 5% gain
        result = log_return(105.0, 100.0)
        expected = math.log(105.0 / 100.0)
        assert abs(result - expected) < 1e-6, f"Expected {expected}, got {result}"
        assert not np.isnan(result), "Valid inputs should not return NaN"

    def test_log_return_zero_change(self):
        """Test that log_return returns 0.0 for no change (not NaN)."""
        try:
            from reward import log_return
        except ImportError:
            pytest.skip("reward module not compiled, skipping Cython test")

        # Test zero return (no change)
        result = log_return(100.0, 100.0)
        assert abs(result - 0.0) < 1e-6, "Zero change should return 0.0"
        assert not np.isnan(result), "Zero change should return 0.0, not NaN"

    def test_semantic_clarity(self):
        """Test semantic clarity: 0.0 = no change, NaN = missing data."""
        try:
            from reward import log_return
        except ImportError:
            pytest.skip("reward module not compiled, skipping Cython test")

        # Case 1: No change (genuine zero return)
        no_change = log_return(100.0, 100.0)
        assert no_change == 0.0, "Genuine no change should be 0.0"
        assert not np.isnan(no_change), "Zero return should not be NaN"

        # Case 2: Missing data (invalid inputs)
        missing_data = log_return(100.0, 0.0)
        assert np.isnan(missing_data), "Invalid inputs should be NaN"

        # Verify they are distinguishable
        assert no_change != missing_data or (no_change == 0.0 and np.isnan(missing_data)), \
            "Model must be able to distinguish 'no change' from 'missing data'"


# ==============================================================================
# MEDIUM #3: Outlier Detection (features_pipeline.py)
# ==============================================================================

class TestMedium3_OutlierDetection:
    """Test winsorization for outlier detection."""

    def test_winsorize_array_basic(self):
        """Test basic winsorization functionality."""
        from features_pipeline import winsorize_array

        # Create data with outliers
        data = np.array([1.0, 2.0, 3.0, 100.0, 4.0, 5.0, -50.0])

        # Winsorize at 1st/99th percentile
        result = winsorize_array(data, lower_percentile=1.0, upper_percentile=99.0)

        # Check that extremes are clipped
        assert np.max(result) < 100.0, "Max outlier should be clipped"
        assert np.min(result) > -50.0, "Min outlier should be clipped"

        # Check that bulk is preserved (middle 5 values unchanged)
        middle_indices = [0, 1, 2, 4, 5]
        for i in middle_indices:
            assert result[i] == data[i], f"Non-outlier at index {i} should be unchanged"

    def test_winsorize_array_flash_crash(self):
        """Test winsorization handles flash crash scenario."""
        from features_pipeline import winsorize_array

        # Simulate returns with flash crash
        normal_returns = [0.01, 0.02, 0.03, 0.02, 0.01, -0.01, -0.02]
        flash_crash = -0.50  # -50% crash
        returns = np.array(normal_returns + [flash_crash])

        # Winsorize
        clean = winsorize_array(returns, lower_percentile=1.0, upper_percentile=99.0)

        # Flash crash should be clipped to 1st percentile (â‰ˆ -0.02)
        assert clean[-1] > flash_crash, "Flash crash should be clipped"
        assert clean[-1] >= np.percentile(returns, 1.0), "Should be clipped to 1st percentile"

    def test_winsorize_preserves_bulk(self):
        """Test that winsorization preserves 98% of data unchanged."""
        from features_pipeline import winsorize_array

        # Create 1000 normal returns
        np.random.seed(42)
        returns = np.random.randn(1000) * 0.01  # 1% daily vol

        # Add extreme outliers
        returns[0] = -0.30  # Extreme negative
        returns[-1] = 0.40   # Extreme positive

        # Winsorize
        clean = winsorize_array(returns, lower_percentile=1.0, upper_percentile=99.0)

        # Count unchanged values (should be ~98%)
        unchanged = np.sum(clean == returns)
        unchanged_pct = unchanged / len(returns) * 100

        assert unchanged_pct >= 95.0, f"At least 95% should be unchanged, got {unchanged_pct:.1f}%"

    def test_winsorize_with_nan(self):
        """Test that winsorization handles NaN values correctly."""
        from features_pipeline import winsorize_array

        # Data with NaN
        data = np.array([1.0, 2.0, np.nan, 100.0, 4.0, 5.0])

        # Winsorize (should ignore NaN)
        result = winsorize_array(data, lower_percentile=1.0, upper_percentile=99.0)

        # NaN should remain NaN
        assert np.isnan(result[2]), "NaN should remain NaN after winsorization"

        # Other values should be processed
        assert result[-3] < 100.0, "Outlier should be clipped even with NaN present"

    def test_feature_pipeline_uses_winsorization(self):
        """Test that FeaturePipeline applies winsorization during fit()."""
        from features_pipeline import FeaturePipeline

        # Create data with outlier in a feature that gets normalized (volume)
        # Note: 'close' may be excluded from normalization in some configurations
        df = pd.DataFrame({
            'timestamp': range(100),
            'symbol': ['BTC'] * 100,
            'close': [100.0] * 100,
            'volume': [1.0] * 99 + [1000.0],  # One extreme outlier in volume
        })

        # Fit pipeline WITH winsorization
        pipe_with = FeaturePipeline(enable_winsorization=True, winsorize_percentiles=(1.0, 99.0))
        pipe_with.fit({'BTC': df})

        # Fit pipeline WITHOUT winsorization
        pipe_without = FeaturePipeline(enable_winsorization=False)
        pipe_without.fit({'BTC': df})

        # Stats should be different for volume (which gets normalized)
        vol_stats_with = pipe_with.stats.get('volume', {})
        vol_stats_without = pipe_without.stats.get('volume', {})

        # Mean should be more robust with winsorization
        mean_with = vol_stats_with.get('mean', 0)
        mean_without = vol_stats_without.get('mean', 0)

        # Without winsorization, mean should be pulled up by outlier
        # With winsorization, outlier is clipped so mean stays low
        assert mean_with < mean_without, \
            f"Winsorization should reduce mean: with={mean_with:.2f}, without={mean_without:.2f}"


# ==============================================================================
# MEDIUM #4: Zero Std Fallback (features_pipeline.py)
# ==============================================================================

class TestMedium4_ZeroStdFallback:
    """Test explicit zero handling for constant features."""

    def test_constant_feature_normalized_to_zero(self):
        """Test that constant features are normalized to zeros."""
        from features_pipeline import FeaturePipeline

        # Create data with constant feature
        df = pd.DataFrame({
            'timestamp': range(100),
            'symbol': ['BTC'] * 100,
            'close': [100.0] * 100,  # Constant (zero variance)
            'volume': np.random.randn(100),  # Variable
        })

        # Fit and transform
        pipe = FeaturePipeline(enable_winsorization=False)
        pipe.fit({'BTC': df})
        result = pipe.transform_df(df)

        # Constant feature should be normalized to zeros
        assert 'close_z' in result.columns, "close_z column should exist"
        assert np.allclose(result['close_z'].dropna(), 0.0, atol=1e-6), \
            "Constant feature should be normalized to zeros"

    def test_is_constant_flag_stored(self):
        """Test that is_constant flag is stored in stats."""
        from features_pipeline import FeaturePipeline

        df = pd.DataFrame({
            'timestamp': range(100),
            'symbol': ['BTC'] * 100,
            'close': [100.0] * 100,  # Constant
            'volume': [1.0, 2.0, 3.0] * 33 + [1.0],  # Variable
        })

        pipe = FeaturePipeline(enable_winsorization=False)
        pipe.fit({'BTC': df})

        # Check stats
        close_stats = pipe.stats.get('close', {})
        volume_stats = pipe.stats.get('volume', {})

        assert close_stats.get('is_constant') == True, "close should be marked as constant"
        assert volume_stats.get('is_constant') == False, "volume should not be marked as constant"

    def test_constant_with_nan_handled_correctly(self):
        """Test edge case: constant feature with NaN."""
        from features_pipeline import FeaturePipeline

        df = pd.DataFrame({
            'timestamp': range(100),
            'symbol': ['BTC'] * 100,
            'close': [100.0] * 98 + [np.nan, 100.0],  # Constant with one NaN
            'volume': [1.0] * 100,
        })

        pipe = FeaturePipeline(enable_winsorization=False)
        pipe.fit({'BTC': df})
        result = pipe.transform_df(df)

        # All non-NaN values should be zero
        close_z_not_nan = result['close_z'].dropna()
        assert np.allclose(close_z_not_nan, 0.0, atol=1e-6), \
            "Constant feature should normalize to zeros even with NaN present"


# ==============================================================================
# MEDIUM #5: Lookahead Bias (features_pipeline.py)
# ==============================================================================

class TestMedium5_LookaheadBias:
    """Test that close shifting happens only once, not twice."""

    def test_no_double_shifting_in_fit_transform(self):
        """Test that close is shifted only once when using fit() then transform_df()."""
        from features_pipeline import FeaturePipeline

        # Create simple data
        df = pd.DataFrame({
            'timestamp': range(10),
            'symbol': ['BTC'] * 10,
            'close': [100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0, 107.0, 108.0, 109.0],
            'volume': [1.0] * 10,
        })

        # Original close values
        original_close = df['close'].copy()

        # Fit pipeline
        pipe = FeaturePipeline(enable_winsorization=False)
        pipe.fit({'BTC': df})

        # Check if shift was performed in fit (flag should be set)
        shift_in_fit = pipe._close_shifted_in_fit

        # Transform same data
        result = pipe.transform_df(df)

        # If shift was done in fit, transform_df should NOT shift again
        # Verify by checking the flag prevents re-shifting

        # Test the core fix: flag prevents double-shifting
        # After fit() that shifts: flag = True
        # After transform_df(): flag should still be True (no second shift)

        assert shift_in_fit == True, "close should be marked as shifted after fit()"

        # Verify that calling transform_df multiple times doesn't cause additional shifts
        result1 = pipe.transform_df(df)
        result2 = pipe.transform_df(df)

        # Both transforms should produce identical results (no additional shifting)
        if 'close' in result1.columns and 'close' in result2.columns:
            # Compare close columns - should be identical
            close1 = result1['close'].values
            close2 = result2['close'].values

            assert np.array_equal(close1, close2, equal_nan=True), \
                "Multiple transform_df() calls should produce identical results (no re-shifting)"

    def test_shift_tracking_flag_prevents_double_shift(self):
        """Test that _close_shifted_in_fit flag prevents double shifting."""
        from features_pipeline import FeaturePipeline

        df = pd.DataFrame({
            'timestamp': range(5),
            'symbol': ['BTC'] * 5,
            'close': [100.0, 101.0, 102.0, 103.0, 104.0],
            'volume': [1.0] * 5,
        })

        pipe = FeaturePipeline(enable_winsorization=False)

        # Initially, flag should be False
        assert pipe._close_shifted_in_fit == False, "Flag should start as False"

        # After fit(), flag should be True
        pipe.fit({'BTC': df})
        assert pipe._close_shifted_in_fit == True, "Flag should be True after fit()"

        # Multiple transform_df() calls should not shift again
        result1 = pipe.transform_df(df)
        result2 = pipe.transform_df(df)

        # Results should be identical (no additional shifting)
        if 'close' in result1.columns and 'close' in result2.columns:
            pd.testing.assert_series_equal(result1['close'], result2['close'],
                                          check_names=False)

    def test_reset_clears_shift_flag(self):
        """Test that reset() clears the shift tracking flag."""
        from features_pipeline import FeaturePipeline

        df = pd.DataFrame({
            'timestamp': range(5),
            'symbol': ['BTC'] * 5,
            'close': [100.0, 101.0, 102.0, 103.0, 104.0],
            'volume': [1.0] * 5,
        })

        pipe = FeaturePipeline(enable_winsorization=False)
        pipe.fit({'BTC': df})

        # Flag should be True after fit
        assert pipe._close_shifted_in_fit == True

        # Reset
        pipe.reset()

        # Flag should be False again
        assert pipe._close_shifted_in_fit == False, "reset() should clear shift flag"


# ==============================================================================
# MEDIUM #9: Hard-coded Reward Clip (reward.pyx)
# ==============================================================================

class TestMedium9_RewardCapParameter:
    """Test that reward_cap parameter is used instead of hard-coded value."""

    def test_reward_cap_parameter_exists(self):
        """Test that compute_reward_view accepts reward_cap parameter."""
        try:
            import inspect
            from reward import compute_reward_view

            # Check function signature
            sig = inspect.signature(compute_reward_view)
            params = list(sig.parameters.keys())

            # Should have reward_cap parameter
            assert 'reward_cap' in params, "compute_reward_view should have reward_cap parameter"

        except ImportError:
            pytest.skip("reward module not compiled, skipping Cython test")

    def test_reward_cap_default_value(self):
        """Test that reward_cap has default value of 10.0 for backward compatibility."""
        try:
            import inspect
            from reward import compute_reward_view

            sig = inspect.signature(compute_reward_view)
            reward_cap_param = sig.parameters.get('reward_cap')

            if reward_cap_param:
                default = reward_cap_param.default
                assert default == 10.0, f"Default reward_cap should be 10.0, got {default}"

        except ImportError:
            pytest.skip("reward module not compiled, skipping Cython test")

    def test_reward_clipping_respects_custom_cap(self):
        """Test that custom reward_cap is actually used for clipping."""
        try:
            from reward import compute_reward_view
            from risk_enums import ClosedReason

            # This test would require calling compute_reward_view with custom reward_cap
            # and verifying that rewards are clipped to that value
            # Skipping detailed implementation test as it requires full state setup

            pytest.skip("Detailed reward_cap test requires full EnvState setup")

        except ImportError:
            pytest.skip("reward module not compiled, skipping Cython test")


# ==============================================================================
# Integration Tests
# ==============================================================================

class TestIntegration:
    """Integration tests for multiple fixes working together."""

    def test_full_pipeline_with_all_fixes(self):
        """Test that all fixes work together in a realistic pipeline."""
        from features_pipeline import FeaturePipeline

        # Create realistic data with challenges:
        # - Outliers (flash crash)
        # - Constant features
        # - NaN values
        np.random.seed(42)
        n = 200

        close_prices = 100 + np.cumsum(np.random.randn(n) * 0.01)
        close_prices[50] = 50.0  # Flash crash
        close_prices[100:110] = 105.0  # Constant period

        df = pd.DataFrame({
            'timestamp': range(n),
            'symbol': ['BTC'] * n,
            'close': close_prices,
            'volume': np.random.randn(n) * 100,
            'constant_feature': [1.0] * n,  # Constant
        })

        # Add some NaN
        df.loc[25, 'volume'] = np.nan

        # Process with all fixes enabled
        pipe = FeaturePipeline(
            enable_winsorization=True,
            winsorize_percentiles=(1.0, 99.0)
        )

        pipe.fit({'BTC': df})
        result = pipe.transform_df(df)

        # Verify all fixes applied:
        # 1. Winsorization handled flash crash
        volume_stats = pipe.stats.get('volume', {})
        assert volume_stats is not None, "volume stats should exist"

        # 2. Constant feature normalized to zeros
        if 'constant_feature_z' in result.columns:
            assert np.allclose(result['constant_feature_z'].dropna(), 0.0, atol=1e-6), \
                "Constant feature should be zeros"

        # 3. Close shifted only once
        assert pipe._close_shifted_in_fit == True, "Close should be marked as shifted"

        # 4. Pipeline completed without errors
        assert result is not None, "Pipeline should complete successfully"


if __name__ == '__main__':
    pytest.main([__file__, '-v', '--tb=short'])
