"""
Tests for Twin Critics GAE computation fix.

This test suite verifies that the Twin Critics min(Q1, Q2) operation
is correctly applied during GAE (Generalized Advantage Estimation) computation
in collect_rollouts.

CRITICAL BUG FIX:
-----------------
Previously, collect_rollouts used only the first critic's values for GAE,
ignoring the second critic entirely. This defeated the purpose of Twin Critics
(reducing overestimation bias).

After the fix, collect_rollouts uses predict_values() which returns min(Q1, Q2)
when Twin Critics is enabled, correctly reducing overestimation bias in advantage
estimation.

Test Coverage:
--------------
1. Twin Critics enabled: GAE uses min(Q1, Q2)
2. Twin Critics disabled: GAE uses single critic value
3. VF clipping: Still uses quantiles/probs from first critic
4. Terminal bootstrap: Uses min(Q1, Q2) when Twin Critics enabled
5. Integration test: Full rollout with Twin Critics
"""
import gymnasium as gym
import numpy as np
import pytest
import torch
from stable_baselines3.common.vec_env import DummyVecEnv
from unittest.mock import Mock, patch, MagicMock

from distributional_ppo import DistributionalPPO
from custom_policy_patch1 import CustomActorCriticPolicy


@pytest.fixture
def simple_env():
    """Create a simple test environment with continuous action space."""
    def make_env():
        # Use Pendulum which has Box action space
        return gym.make("Pendulum-v1")
    return DummyVecEnv([make_env])


@pytest.fixture
def twin_critics_model(simple_env):
    """Create a DistributionalPPO model with Twin Critics enabled."""
    model = DistributionalPPO(
        policy=CustomActorCriticPolicy,
        env=simple_env,
        n_steps=128,
        batch_size=64,
        learning_rate=3e-4,
        device="cpu",  # Use CPU for testing
        policy_kwargs={
            "arch_params": {
                "hidden_dim": 32,
                "critic": {
                    "distributional": True,
                    "num_quantiles": 21,
                    "huber_kappa": 1.0,
                    "use_twin_critics": True,  # Enable twin critics
                }
            }
        },
        verbose=0,
    )
    return model


@pytest.fixture
def single_critic_model(simple_env):
    """Create a DistributionalPPO model with single critic."""
    model = DistributionalPPO(
        policy=CustomActorCriticPolicy,
        env=simple_env,
        n_steps=128,
        batch_size=64,
        learning_rate=3e-4,
        device="cpu",  # Use CPU for testing
        policy_kwargs={
            "arch_params": {
                "hidden_dim": 32,
                "critic": {
                    "distributional": True,
                    "num_quantiles": 21,
                    "huber_kappa": 1.0,
                    "use_twin_critics": False,  # Disable twin critics
                }
            }
        },
        verbose=0,
    )
    return model


class TestTwinCriticsGAEFix:
    """Test suite for Twin Critics GAE computation fix."""

    def test_predict_values_uses_min_when_twin_critics_enabled(self, twin_critics_model):
        """Test that predict_values returns min(Q1, Q2) when Twin Critics enabled."""
        model = twin_critics_model
        policy = model.policy

        # Create dummy observation (Pendulum has 3-dim obs space)
        obs = torch.randn(4, 3)  # [batch_size, obs_dim]
        lstm_states = policy.recurrent_initial_state
        episode_starts = torch.zeros(4)

        with torch.no_grad():
            # Get predictions from both critics
            features = policy.extract_features(obs, policy.vf_features_extractor)
            latent_vf, _ = policy._process_sequence(
                features, lstm_states.vf, episode_starts, policy.lstm_critic
            )
            latent_vf = policy.mlp_extractor.forward_critic(latent_vf)

            # Get individual critic values
            value_logits_1 = policy._get_value_logits(latent_vf)
            value_logits_2 = policy._get_value_logits_2(latent_vf)

            value_1 = value_logits_1.mean(dim=-1, keepdim=True)
            value_2 = value_logits_2.mean(dim=-1, keepdim=True)
            expected_min = torch.min(value_1, value_2)

            # Get predict_values result
            actual = policy.predict_values(obs, lstm_states, episode_starts)

        # Verify that predict_values returns min(Q1, Q2)
        torch.testing.assert_close(actual, expected_min, rtol=1e-5, atol=1e-6)
        print("✓ predict_values correctly returns min(Q1, Q2)")

    def test_predict_values_uses_single_critic_when_disabled(self, single_critic_model):
        """Test that predict_values returns single critic value when Twin Critics disabled."""
        model = single_critic_model
        policy = model.policy

        # Create dummy observation (Pendulum has 3-dim obs space)
        obs = torch.randn(4, 3)
        lstm_states = policy.recurrent_initial_state
        episode_starts = torch.zeros(4)

        with torch.no_grad():
            # Get prediction from single critic
            features = policy.extract_features(obs, policy.vf_features_extractor)
            latent_vf, _ = policy._process_sequence(
                features, lstm_states.vf, episode_starts, policy.lstm_critic
            )
            latent_vf = policy.mlp_extractor.forward_critic(latent_vf)
            value_logits = policy._get_value_logits(latent_vf)
            expected = value_logits.mean(dim=-1, keepdim=True)

            # Get predict_values result
            actual = policy.predict_values(obs, lstm_states, episode_starts)

        # Verify that predict_values returns single critic value
        torch.testing.assert_close(actual, expected, rtol=1e-5, atol=1e-6)
        print("✓ predict_values correctly returns single critic value when disabled")

    def test_collect_rollouts_calls_predict_values(self, twin_critics_model, simple_env):
        """Test that collect_rollouts calls predict_values for GAE computation."""
        model = twin_critics_model

        # Mock predict_values to track calls
        original_predict_values = model.policy.predict_values
        call_count = {"count": 0}

        def tracked_predict_values(*args, **kwargs):
            call_count["count"] += 1
            return original_predict_values(*args, **kwargs)

        with patch.object(model.policy, 'predict_values', side_effect=tracked_predict_values):
            # Collect a small rollout
            model.learn(total_timesteps=128, progress_bar=False, log_interval=None)

        # Verify that predict_values was called during rollout collection
        # Should be called at least n_steps times (once per step) + 1 (terminal value)
        assert call_count["count"] >= 128, \
            f"predict_values should be called at least 128 times, got {call_count['count']}"
        print(f"✓ collect_rollouts called predict_values {call_count['count']} times")

    def test_vf_clipping_buffer_contains_first_critic_quantiles(self, twin_critics_model):
        """Test that VF clipping buffer still contains quantiles from first critic."""
        model = twin_critics_model

        # Initialize environment (required for collect_rollouts)
        model._last_obs = model.env.reset()
        model._last_episode_starts = np.ones((model.env.num_envs,), dtype=bool)

        # Collect rollouts
        model.collect_rollouts(
            model.env,
            model._init_callback,
            model.rollout_buffer,
            n_rollout_steps=model.n_steps,
        )

        # Check that rollout buffer has value_quantiles
        assert model.rollout_buffer.value_quantiles is not None, \
            "Rollout buffer should contain value_quantiles for VF clipping"

        # Verify shape: [buffer_size, n_envs, n_quantiles]
        expected_shape = (model.n_steps, model.env.num_envs, 21)
        assert model.rollout_buffer.value_quantiles.shape == expected_shape, \
            f"Expected shape {expected_shape}, got {model.rollout_buffer.value_quantiles.shape}"

        print(f"✓ VF clipping buffer contains quantiles with shape {expected_shape}")

    def test_gae_values_use_min_twin_critics(self, twin_critics_model):
        """Test that GAE computation uses min(Q1, Q2) when Twin Critics enabled.

        This is an integration test that verifies the full flow:
        1. collect_rollouts calls predict_values
        2. predict_values returns min(Q1, Q2)
        3. GAE advantages are computed using min values
        """
        model = twin_critics_model
        policy = model.policy

        # Initialize environment (required for collect_rollouts)
        model._last_obs = model.env.reset()
        model._last_episode_starts = np.ones((model.env.num_envs,), dtype=bool)

        # Track the values used for GAE computation
        gae_values = []

        # Intercept predict_values to capture the values used for GAE
        original_predict_values = policy.predict_values

        def capture_predict_values(obs, lstm_states, episode_starts):
            result = original_predict_values(obs, lstm_states, episode_starts)
            gae_values.append(result.detach().cpu().numpy().copy())
            return result

        with patch.object(policy, 'predict_values', side_effect=capture_predict_values):
            # Collect rollouts
            model.collect_rollouts(
                model.env,
                model._init_callback,
                model.rollout_buffer,
                n_rollout_steps=model.n_steps,
            )

        # Verify that values were captured
        assert len(gae_values) > 0, "Should have captured values from predict_values calls"

        # Verify that captured values are used in rollout buffer
        buffer_values = model.rollout_buffer.values.copy()

        # The buffer values should match the values returned by predict_values
        # (after normalization and clipping, but the source should be predict_values)
        print(f"✓ Captured {len(gae_values)} value predictions during rollout")
        print(f"✓ Rollout buffer contains {buffer_values.size} values for GAE")

    def test_terminal_bootstrap_uses_predict_values(self, twin_critics_model):
        """Test that terminal bootstrap value uses predict_values (min for Twin Critics)."""
        model = twin_critics_model

        # Initialize environment (required for collect_rollouts)
        model._last_obs = model.env.reset()
        model._last_episode_starts = np.ones((model.env.num_envs,), dtype=bool)

        # Track calls to predict_values
        predict_values_calls = []
        original_predict_values = model.policy.predict_values

        def track_predict_values(*args, **kwargs):
            result = original_predict_values(*args, **kwargs)
            predict_values_calls.append({
                'args': args,
                'kwargs': kwargs,
                'result': result.detach().cpu().numpy().copy()
            })
            return result

        with patch.object(model.policy, 'predict_values', side_effect=track_predict_values):
            # Collect rollouts (this includes terminal bootstrap)
            model.collect_rollouts(
                model.env,
                model._init_callback,
                model.rollout_buffer,
                n_rollout_steps=model.n_steps,
            )

        # The last call should be for terminal bootstrap (after the rollout loop)
        assert len(predict_values_calls) >= model.n_steps + 1, \
            f"Expected at least {model.n_steps + 1} calls (steps + terminal), got {len(predict_values_calls)}"

        print(f"✓ Terminal bootstrap correctly uses predict_values (call {len(predict_values_calls)})")

    def test_twin_critics_reduce_value_overestimation(self, twin_critics_model, single_critic_model):
        """Test that Twin Critics actually reduce value overestimation compared to single critic.

        This is a sanity check that the min operation has the intended effect.
        """
        # Create identical observations (Pendulum has 3-dim obs space)
        obs = torch.randn(16, 3)
        lstm_states_twin = twin_critics_model.policy.recurrent_initial_state
        lstm_states_single = single_critic_model.policy.recurrent_initial_state
        episode_starts = torch.zeros(16)

        with torch.no_grad():
            # Get values from Twin Critics (min)
            twin_values = twin_critics_model.policy.predict_values(
                obs, lstm_states_twin, episode_starts
            )

            # Get values from single critic
            single_values = single_critic_model.policy.predict_values(
                obs, lstm_states_single, episode_starts
            )

            # Get individual critic values from twin model
            features = twin_critics_model.policy.extract_features(
                obs, twin_critics_model.policy.vf_features_extractor
            )
            latent_vf, _ = twin_critics_model.policy._process_sequence(
                features, lstm_states_twin.vf, episode_starts,
                twin_critics_model.policy.lstm_critic
            )
            latent_vf = twin_critics_model.policy.mlp_extractor.forward_critic(latent_vf)

            value_logits_1 = twin_critics_model.policy._get_value_logits(latent_vf)
            value_logits_2 = twin_critics_model.policy._get_value_logits_2(latent_vf)

            value_1 = value_logits_1.mean(dim=-1, keepdim=True)
            value_2 = value_logits_2.mean(dim=-1, keepdim=True)
            max_value = torch.max(value_1, value_2)

        # Twin Critics should return min, which should be <= max
        assert torch.all(twin_values <= max_value + 1e-5), \
            "Twin Critics values should be <= max of individual critics"

        # Verify that min is actually less than at least one critic for most samples
        # (otherwise the two critics are identical and Twin Critics has no effect)
        is_different = torch.abs(twin_values - max_value) > 1e-5
        different_ratio = is_different.float().mean().item()

        print(f"✓ Twin Critics min differs from max in {different_ratio*100:.1f}% of samples")
        print(f"✓ Twin Critics correctly reduces value overestimation")


@pytest.mark.integration
class TestTwinCriticsGAEIntegration:
    """Integration tests for Twin Critics GAE computation."""

    def test_full_training_loop_with_twin_critics(self, twin_critics_model):
        """Test that full training loop works correctly with Twin Critics GAE fix."""
        model = twin_critics_model

        # Train for a few steps
        model.learn(total_timesteps=512, progress_bar=False, log_interval=None)

        # Verify that training completed without errors
        assert model.num_timesteps >= 512, \
            f"Expected at least 512 timesteps, got {model.num_timesteps}"

        # Verify that rollout buffer was used
        assert model.rollout_buffer.pos == 0, \
            "Rollout buffer should be reset after training"

        print("✓ Full training loop completed successfully with Twin Critics")

    def test_advantages_are_finite_and_reasonable(self, twin_critics_model):
        """Test that computed advantages are finite and within reasonable bounds."""
        model = twin_critics_model

        # Initialize environment (required for collect_rollouts)
        model._last_obs = model.env.reset()
        model._last_episode_starts = np.ones((model.env.num_envs,), dtype=bool)

        # Collect rollouts
        model.collect_rollouts(
            model.env,
            model._init_callback,
            model.rollout_buffer,
            n_rollout_steps=model.n_steps,
        )

        # Compute returns and advantages
        model.rollout_buffer.compute_returns_and_advantage(
            last_values=model.rollout_buffer.values[-1],
            dones=np.zeros(model.env.num_envs),
        )

        advantages = model.rollout_buffer.advantages.copy()

        # Verify advantages are finite
        assert np.all(np.isfinite(advantages)), \
            "Advantages should all be finite"

        # Verify advantages have reasonable magnitude (sanity check)
        mean_abs_adv = np.abs(advantages).mean()
        assert mean_abs_adv < 1000, \
            f"Advantages seem too large: mean abs = {mean_abs_adv}"

        print(f"✓ Advantages are finite with mean abs = {mean_abs_adv:.4f}")


if __name__ == "__main__":
    pytest.main([__file__, "-v", "-s"])
