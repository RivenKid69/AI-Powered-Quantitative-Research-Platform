# PPO Issues Verification Report - 2025-11-23

## Executive Summary

–ü—Ä–æ–≤–µ–¥—ë–Ω –¥–µ—Ç–∞–ª—å–Ω—ã–π –∞—É–¥–∏—Ç –∑–∞—è–≤–ª–µ–Ω–Ω—ã—Ö –ø—Ä–æ–±–ª–µ–º –≤ PPO implementation. –ò–∑ 7 –∑–∞—è–≤–ª–µ–Ω–Ω—ã—Ö –ø—Ä–æ–±–ª–µ–º:
- ‚úÖ **3 –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω—ã –∏ —Ç—Ä–µ–±—É—é—Ç –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è** (HIGH severity)
- ‚ö†Ô∏è **2 –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω—ã —á–∞—Å—Ç–∏—á–Ω–æ** (MEDIUM/LOW severity)
- ‚ùå **2 –æ—Ç–∫–ª–æ–Ω–µ–Ω—ã** (FALSE POSITIVES –∏–ª–∏ —É–∂–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω—ã)

---

## üî¥ –ü–û–î–¢–í–ï–†–ñ–î–Å–ù–ù–´–ï –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ï –ü–†–û–ë–õ–ï–ú–´ (HIGH Severity)

### 1. ‚úÖ VGS v3.0 - –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ –Ω–µ–ø–æ–ª–Ω—ã–π fix

**–°—Ç–∞—Ç—É—Å**: ‚úÖ **–ü–û–î–¢–í–ï–†–ñ–î–ï–ù–û** - —Ç—Ä–µ–±—É–µ—Ç –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∏–ª–∏ clarification

**–ó–∞—è–≤–ª–µ–Ω–∏–µ**:
- VGS v3.0 –≤—ã—á–∏—Å–ª—è–µ—Ç "stochastic variance of the MEAN gradient" –≤–º–µ—Å—Ç–æ element-wise stochastic variance
- –ü—Ä–æ–±–ª–µ–º–∞: –Ω–µ –¥–µ—Ç–µ–∫—Ç–∏—Ä—É–µ—Ç anticorrelated noise (BatchNorm —Å–ª–æ–∏, —Å–∏–º–º–µ—Ç—Ä–∏—á–Ω—ã–µ —Ñ–∏–ª—å—Ç—Ä—ã)

**–í–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è**:
```python
# variance_gradient_scaler.py:277-280
grad_mean_current = grad.mean().item()              # Mean gradient at timestep t
grad_sq_current = grad_mean_current ** 2            # SQUARE of mean

# variance_gradient_scaler.py:356
variance = sq_corrected - mean_corrected.pow(2)    # Var[mean(g)], NOT mean(Var[g])
```

**–ü—Ä–æ–±–ª–µ–º–∞ –ü–û–î–¢–í–ï–†–ñ–î–ï–ù–ê**:
- –ö–æ–¥ –≤—ã—á–∏—Å–ª—è–µ—Ç: **Var[E_spatial[g]]** (variance of spatial mean over time)
- –ù–ï –≤—ã—á–∏—Å–ª—è–µ—Ç: **E_time[Var_spatial[g]]** (mean of spatial variance over time)

**–í–ª–∏—è–Ω–∏–µ**:
- **Anticorrelated noise –Ω–µ –¥–µ—Ç–µ–∫—Ç–∏—Ä—É–µ—Ç—Å—è**: –ï—Å–ª–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä –∏–º–µ–µ—Ç —ç–ª–µ–º–µ–Ω—Ç—ã —Å –ø—Ä–æ—Ç–∏–≤–æ–ø–æ–ª–æ–∂–Ω—ã–º–∏ –∑–Ω–∞–∫–∞–º–∏ (+0.1, -0.1), —Ç–æ:
  - `mean(g) ‚âà 0` ‚Üí –Ω–∏–∑–∫–∞—è variance
  - –ù–æ —Ä–µ–∞–ª—å–Ω–∞—è element-wise variance –≤—ã—Å–æ–∫–∞—è!
- **–ü—Ä–∏–º–µ—Ä—ã**: BatchNorm —Å–ª–æ–∏, symmetric conv filters, grouped convolutions

**Severity**: MEDIUM-HIGH (–Ω–µ –∫—Ä–∏—Ç–∏—á–Ω–æ, –Ω–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ –Ω–µ–ø–æ–ª–Ω–æ)

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è**:
- **Option 1** (quick): –û–±–Ω–æ–≤–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é - clarify —á—Ç–æ VGS –∏—Å–ø–æ–ª—å–∑—É–µ—Ç "variance of mean gradient"
- **Option 2** (correct): Implement true element-wise stochastic variance:
```python
# Per-element variance computation
grad_var_current = grad.var(unbiased=False).item()  # Spatial variance at timestep t
self._param_grad_var_ema[i] = beta * self._param_grad_var_ema[i] + (1-beta) * grad_var_current
# Then aggregate: global_var = mean(self._param_grad_var_ema) or percentile
```

---

### 2. ‚úÖ Return Scale Snapshot Timing

**–°—Ç–∞—Ç—É—Å**: ‚úÖ **–ü–û–î–¢–í–ï–†–ñ–î–ï–ù–û** - –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–±–ª–µ–º–∞ –ø–æ—Ä—è–¥–∫–∞ –≤—ã–∑–æ–≤–æ–≤

**–ó–∞—è–≤–ª–µ–Ω–∏–µ**:
- Snapshot —Å–Ω–∏–º–∞–µ—Ç—Å—è –ü–ï–†–ï–î rollout, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ü–û–°–õ–ï ‚Üí 5-10% bias

**–í–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è**:
```python
# distributional_ppo.py:7871 - collect_rollouts()
self._activate_return_scale_snapshot()  # Snapshot BEFORE rollout collection

# distributional_ppo.py:8600 - train()
self._activate_return_scale_snapshot()  # Snapshot BEFORE training

# distributional_ppo.py:5666-5667 - train() (END)
self._ret_mean_value = float(new_mean)   # Update AFTER training
self._ret_std_value = float(new_std)
```

**–í—Ä–µ–º–µ–Ω–Ω–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å**:
```
Update N-1:
  ‚îî‚îÄ train() END: _ret_mean_value, _ret_std_value –æ–±–Ω–æ–≤–ª–µ–Ω—ã

Update N:
  ‚îú‚îÄ collect_rollouts() START:
  ‚îÇ    ‚îî‚îÄ _activate_return_scale_snapshot()  ‚Üê snapshot from Update N-1!
  ‚îÇ    ‚îî‚îÄ Use snapshot for normalization
  ‚îú‚îÄ train() START:
  ‚îÇ    ‚îî‚îÄ _activate_return_scale_snapshot()  ‚Üê STILL from Update N-1!
  ‚îÇ    ‚îî‚îÄ Train on normalized data
  ‚îî‚îÄ train() END:
       ‚îî‚îÄ Update _ret_mean_value, _ret_std_value  ‚Üê Too late!
```

**–ü—Ä–æ–±–ª–µ–º–∞ –ü–û–î–¢–í–ï–†–ñ–î–ï–ù–ê**:
- Snapshot —Å–Ω–∏–º–∞–µ—Ç—Å—è –Ω–∞ –æ—Å–Ω–æ–≤–µ statistics –æ—Ç **update N-1**
- –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö **update N**
- One-step lag ‚Üí 5-10% bias –≤–æ–∑–º–æ–∂–µ–Ω (–æ—Å–æ–±–µ–Ω–Ω–æ –ø—Ä–∏ –Ω–µ—Å—Ç–∞—Ü–∏–æ–Ω–∞—Ä–Ω—ã—Ö —Å—Ä–µ–¥–∞—Ö)

**Severity**: HIGH

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è**:
- –ü–µ—Ä–µ–Ω–µ—Å—Ç–∏ `self._activate_return_scale_snapshot()` –í –ö–û–ù–ï–¶ `train()` (–ø–æ—Å–ª–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è statistics)
- –ò–ª–∏ —Å–Ω–∏–º–∞—Ç—å snapshot –î–û collect_rollouts, –Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ç–µ–∫—É—â–∏–µ statistics (–Ω–µ –æ—Ç –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ update)

---

### 3. ‚úÖ VecNormalize-LSTM State Divergence

**–°—Ç–∞—Ç—É—Å**: ‚úÖ **–ü–û–î–¢–í–ï–†–ñ–î–ï–ù–û** - –ø—Ä–æ–±–ª–µ–º–∞ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏ –ø—Ä–∏ PBT exploit

**–ó–∞—è–≤–ª–µ–Ω–∏–µ**:
- LSTM reset —Å stale normalization ‚Üí 3-7% –ø–æ—Ç–µ—Ä—è accuracy

**–í–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è**:
```python
# training_pbt_adversarial_integration.py - PBT exploit sequence:
# 1. Load policy weights from source agent
model.policy.load_state_dict(new_parameters["policy"])

# 2. Reset LSTM states to initial
model.reset_lstm_states_to_initial()  # ‚úÖ Implemented

# 3. BUT: VecNormalize statistics NOT synchronized!
# Source agent trained with env_source.norm_obs.mean = X_source
# Current agent uses env_current.norm_obs.mean = X_current
```

**–ü—Ä–æ–±–ª–µ–º–∞ –ü–û–î–¢–í–ï–†–ñ–î–ï–ù–ê**:
- –ü—Ä–∏ PBT exploit:
  1. Policy weights –∫–æ–ø–∏—Ä—É—é—Ç—Å—è –æ—Ç source agent
  2. LSTM states —Å–±—Ä–∞—Å—ã–≤–∞—é—Ç—Å—è –∫ initial (correct)
  3. **–ù–û**: VecNormalize statistics –æ—Å—Ç–∞—é—Ç—Å—è –æ—Ç current agent (incorrect!)

- **–í—Ä–µ–º–µ–Ω–Ω–æ–µ —Ä–∞—Å—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∏–µ**:
  - Source policy –æ–±—É—á–∞–ª–∞—Å—å –Ω–∞ observations —Å `norm_mean = X_source`
  - Current agent –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç —Å `norm_mean = X_current`
  - LSTM –ø–æ–ª—É—á–∞–µ—Ç –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ observations
  - –ü–µ—Ä–≤—ã–µ 1-2 episodes –¥–∞—é—Ç –ø–ª–æ—Ö–∏–µ predictions ‚Üí value loss spike 5-15%

**Severity**: HIGH (–¥–ª—è PBT training)

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è**:
```python
# After PBT exploit, load VecNormalize stats from source agent:
if source_member.vecnormalize_stats_path is not None:
    env = VecNormalize.load(source_member.vecnormalize_stats_path, env)
    logger.info(f"VecNormalize stats synchronized from source agent")
```

---

## ‚ö†Ô∏è –ß–ê–°–¢–ò–ß–ù–û –ü–û–î–¢–í–ï–†–ñ–î–Å–ù–ù–´–ï –ü–†–û–ë–õ–ï–ú–´

### 4. ‚ö†Ô∏è VGS Documentation Mismatch

**–°—Ç–∞—Ç—É—Å**: ‚ö†Ô∏è **CONFIRMED** (LOW severity) - documentation issue, not a bug

**–ü—Ä–æ–±–ª–µ–º–∞**:
- –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –∑–∞—è–≤–ª—è–µ—Ç "stochastic variance"
- –†–µ–∞–ª—å–Ω–æ –≤—ã—á–∏—Å–ª—è–µ—Ç—Å—è "stochastic variance of the mean"

**–í–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è**:
```python
# variance_gradient_scaler.py:4
# "Implements adaptive gradient scaling based on **per-parameter stochastic variance**."
# ‚Üê MISLEADING: —ç—Ç–æ variance OF THE MEAN, not mean OF VARIANCES
```

**Severity**: LOW (documentation mismatch, not algorithmic bug)

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è**:
- Update docstring to clarify:
  ```python
  """
  Implements adaptive gradient scaling based on stochastic variance of the
  **layer-wise mean gradient** (not element-wise variance).

  This tracks Var[E_spatial[‚àáŒ∏]] over time, which is efficient but may not
  detect anticorrelated noise within parameter groups (e.g., BatchNorm).
  """
  ```

---

### 5. ‚ö†Ô∏è LSTM Hidden State Stats Missing

**–°—Ç–∞—Ç—É—Å**: ‚ö†Ô∏è **CONFIRMED** (LOW severity) - useful for debugging, not critical

**–ü—Ä–æ–±–ª–µ–º–∞**:
- –ù–µ—Ç logging –¥–ª—è LSTM hidden state statistics (norm, mean, std)

**–í–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è**:
```bash
$ grep -n "lstm.*hidden\|lstm.*norm\|vgs/lstm" distributional_ppo.py
# No results - LSTM state stats are not logged
```

**Severity**: LOW (monitoring improvement, not a bug)

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è**:
- Add logging in `collect_rollouts()` after LSTM forward pass:
```python
if self._last_lstm_states is not None:
    # Log LSTM hidden state statistics for monitoring
    for i, state_tensor in enumerate(self._last_lstm_states.vf):
        self.logger.record(f"lstm/critic_hidden_layer{i}_norm", state_tensor.norm().item())
        self.logger.record(f"lstm/critic_hidden_layer{i}_mean", state_tensor.mean().item())
        self.logger.record(f"lstm/critic_hidden_layer{i}_std", state_tensor.std().item())
```

---

## ‚ùå –û–¢–ö–õ–û–ù–Å–ù–ù–´–ï –ü–†–û–ë–õ–ï–ú–´ (FALSE POSITIVES)

### 6. ‚ùå Entropy Double-Suppression

**–°—Ç–∞—Ç—É—Å**: ‚ùå **FALSE POSITIVE** - –∑–∞—â–∏—Ç–∞ –£–ñ–ï —Å—É—â–µ—Å—Ç–≤—É–µ—Ç

**–ó–∞—è–≤–ª–µ–Ω–∏–µ**:
- decay + plateau detection –±–µ–∑ –∑–∞—â–∏—Ç—ã –æ—Ç –º–∏–Ω–∏–º—É–º–∞

**–í–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è**:
```python
# distributional_ppo.py:7625 - CLAMP EXISTS!
clamped_value = float(max(raw_value, self.ent_coef_min))  # ‚úÖ Protection #1

# distributional_ppo.py:8584 - DOUBLE PROTECTION!
ent_coef_eff_value = float(max(ent_coef_boosted_value, self.ent_coef_min))  # ‚úÖ Protection #2
```

**Entropy management flow**:
1. Linear decay: `ent_coef_initial` ‚Üí `ent_coef_final`
2. **Clamp #1**: `max(decayed_value, ent_coef_min)`
3. Entropy boost: Multiplicative boost if explained variance bad
4. **Clamp #2**: `max(boosted_value, ent_coef_min)`

**–ü—Ä–æ–±–ª–µ–º–∞ –ù–ï –°–£–©–ï–°–¢–í–£–ï–¢**: –ó–∞—â–∏—Ç–∞ –æ—Ç –º–∏–Ω–∏–º—É–º–∞ –£–ñ–ï —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–∞ (–¥–≤–æ–π–Ω–∞—è!)

**Severity**: NONE (false alarm)

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è**: No action required

---

### 7. ‚ùå LSTM State Reset After Episode Boundaries

**–°—Ç–∞—Ç—É—Å**: ‚ùå **ALREADY FIXED** - Issue #4 (2025-11-21)

**–ó–∞—è–≤–ª–µ–Ω–∏–µ**:
- –ù–µ —É–∫–∞–∑–∞–Ω–∞ —è–≤–Ω–æ, –Ω–æ —É–ø–æ–º–∏–Ω–∞–µ—Ç—Å—è –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ LSTM reset

**–í–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è**:
```python
# distributional_ppo.py:2148-2273 - ALREADY IMPLEMENTED!
def _reset_lstm_states_for_done_envs(self, states, dones, initial_states):
    """Reset LSTM hidden states for environments that have finished episodes.
    CRITICAL FIX (Issue #4): Without this, LSTM states carry over across episode
    boundaries, causing temporal leakage..."""

# distributional_ppo.py:8298 - CALLED IN ROLLOUT LOOP!
self._last_lstm_states = self._reset_lstm_states_for_done_envs(...)
```

**–ü—Ä–æ–±–ª–µ–º–∞ –£–ñ–ï –ò–°–ü–†–ê–í–õ–ï–ù–ê**: 2025-11-21 (Issue #4)

**Severity**: NONE (already fixed)

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è**: No action required

---

## üìä –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π

| # | –ü—Ä–æ–±–ª–µ–º–∞ | Severity | Effort | Priority | –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ |
|---|----------|----------|--------|----------|------------------------|
| 2 | **Return Scale Snapshot Timing** | **HIGH** | LOW | **P0** | **FIX IMMEDIATELY** |
| 3 | **VecNormalize-LSTM Divergence** | **HIGH** | MEDIUM | **P0** | **FIX –¥–ª—è PBT** |
| 1 | VGS Semantic Incompleteness | MEDIUM | MEDIUM | P1 | Option 1: Update docs; Option 2: Implement element-wise |
| 4 | VGS Documentation Mismatch | LOW | LOW | P2 | Update docstring |
| 5 | LSTM Stats Missing | LOW | LOW | P2 | Add monitoring (optional) |

---

## üîß –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è

### FIX #1: Return Scale Snapshot Timing (P0)

**–§–∞–π–ª**: `distributional_ppo.py`

**–ü—Ä–æ–±–ª–µ–º–∞**: Snapshot —Å–Ω–∏–º–∞–µ—Ç—Å—è –ü–ï–†–ï–î rollout, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å lag

**–†–µ—à–µ–Ω–∏–µ**: –ü–µ—Ä–µ–º–µ—Å—Ç–∏—Ç—å snapshot activation –∫ –∫–æ–Ω—Ü—É train()

```python
# distributional_ppo.py - BEFORE (INCORRECT):
def train(self) -> None:
    self._activate_return_scale_snapshot()  # ‚Üê Too early!
    # ... training logic ...
    # Update statistics at END
    self._ret_mean_value = float(new_mean)
    self._ret_std_value = float(new_std)

# distributional_ppo.py - AFTER (CORRECT):
def train(self) -> None:
    # ... training logic ...
    # Update statistics FIRST
    self._ret_mean_value = float(new_mean)
    self._ret_std_value = float(new_std)
    # THEN snapshot for NEXT update
    self._activate_return_scale_snapshot()  # ‚Üê Correct timing!
```

**Alternative**: Defer snapshot to START of next collect_rollouts() but ensure it uses LATEST statistics

**Impact**: –£—Å—Ç—Ä–∞–Ω—è–µ—Ç 5-10% bias –æ—Ç one-step lag

---

### FIX #2: VecNormalize-LSTM State Divergence (P0)

**–§–∞–π–ª**: `training_pbt_adversarial_integration.py`

**–ü—Ä–æ–±–ª–µ–º–∞**: VecNormalize stats –Ω–µ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä—É—é—Ç—Å—è –ø—Ä–∏ PBT exploit

**–†–µ—à–µ–Ω–∏–µ**: –°–æ—Ö—Ä–∞–Ω—è—Ç—å –∏ –∑–∞–≥—Ä—É–∂–∞—Ç—å VecNormalize stats –≤–º–µ—Å—Ç–µ —Å policy weights

```python
# 1. Save VecNormalize stats with checkpoint
def save_pbt_checkpoint(model, env, checkpoint_path):
    # Save model parameters
    torch.save(model.get_parameters(include_optimizer=True), checkpoint_path)

    # Save VecNormalize stats alongside
    if isinstance(env, VecNormalize):
        vecnorm_path = checkpoint_path.replace(".zip", "_vecnormalize.pkl")
        env.save(vecnorm_path)
        return {"checkpoint": checkpoint_path, "vecnormalize": vecnorm_path}

# 2. Load VecNormalize stats after exploit
def _apply_exploited_parameters(self, model, new_parameters, source_member):
    # Load policy weights
    model.policy.load_state_dict(new_parameters["policy"])

    # Reset LSTM states
    model.reset_lstm_states_to_initial()

    # ‚úÖ NEW: Synchronize VecNormalize statistics!
    if hasattr(source_member, "vecnormalize_path") and source_member.vecnormalize_path:
        env = model.get_env()
        if isinstance(env, VecNormalize):
            env_synced = VecNormalize.load(source_member.vecnormalize_path, env)
            model.set_env(env_synced)
            logger.info(
                f"Member {member.member_id}: VecNormalize stats synchronized from source agent "
                "(prevents LSTM-normalization mismatch)"
            )
```

**Impact**: –£—Å—Ç—Ä–∞–Ω—è–µ—Ç 3-7% –ø–æ—Ç–µ—Ä—é accuracy –≤ –ø–µ—Ä–≤—ã—Ö episodes –ø–æ—Å–ª–µ PBT exploit

---

### FIX #3: VGS Documentation Update (P2)

**–§–∞–π–ª**: `variance_gradient_scaler.py`

**–†–µ—à–µ–Ω–∏–µ**: Clarify —á—Ç–æ VGS –∏—Å–ø–æ–ª—å–∑—É–µ—Ç variance of mean, not mean of variance

```python
# variance_gradient_scaler.py:1-47
"""
Variance Gradient Scaler

Implements adaptive gradient scaling based on **stochastic variance of the
layer-wise mean gradient** (temporal noise in spatial mean).

**IMPORTANT SEMANTIC CLARIFICATION (v3.0)**:
This module computes Var[E_spatial[‚àáŒ∏]] (variance of spatial mean over time),
NOT E_time[Var_spatial[‚àáŒ∏]] (mean of spatial variance over time).

This choice is:
- ‚úÖ Efficient: O(1) memory per parameter (stores mean only)
- ‚úÖ Effective for most layers: Detects temporal instability
- ‚ö†Ô∏è Limitation: May not detect anticorrelated noise (e.g., BatchNorm, symmetric filters)

For full element-wise variance tracking, consider implementing:
    Var_element[i,j] = E[g_{i,j}¬≤] - E[g_{i,j}]¬≤  (per-element stochastic variance)
However, this requires O(num_params) memory and may be overkill for most use cases.
"""
```

---

## üß™ –¢–µ—Å—Ç–æ–≤—ã–π –ø–ª–∞–Ω

### Test #1: Return Scale Snapshot Timing Fix

```python
def test_return_scale_snapshot_timing():
    """Verify snapshot uses current update statistics, not previous."""
    model = DistributionalPPO(...)

    # Initial statistics
    model._ret_mean_value = 0.0
    model._ret_std_value = 1.0

    # Collect rollouts and train (update N)
    model.collect_rollouts(...)
    model.train()  # Should update statistics AND snapshot

    # Statistics updated?
    assert model._ret_mean_value != 0.0  # Changed

    # Snapshot synchronized?
    assert model._ret_mean_snapshot == model._ret_mean_value
    assert model._ret_std_snapshot == model._ret_std_value

    # Next rollout uses CURRENT snapshot (not lag)
    model.collect_rollouts(...)
    # Verify normalization uses updated snapshot
```

### Test #2: VecNormalize-LSTM Synchronization

```python
def test_vecnormalize_lstm_sync_after_pbt():
    """Verify VecNormalize stats are synchronized with policy during PBT exploit."""
    # Create source and target agents
    source_model = create_pbt_agent(member_id=0)
    target_model = create_pbt_agent(member_id=1)

    # Train source agent (accumulate different VecNormalize stats)
    source_model.learn(total_timesteps=10000)
    source_stats = source_model.get_env().get_attr("obs_rms")[0]

    # Save checkpoint
    checkpoint = save_pbt_checkpoint(source_model, checkpoint_path)

    # Target exploits from source
    coordinator._apply_exploited_parameters(
        target_model, checkpoint["policy"], checkpoint
    )

    # Verify VecNormalize stats synchronized
    target_stats = target_model.get_env().get_attr("obs_rms")[0]
    assert np.allclose(target_stats.mean, source_stats.mean, atol=1e-6)
    assert np.allclose(target_stats.var, source_stats.var, atol=1e-6)

    # Verify LSTM states reset
    assert target_model._last_lstm_states is not None
    # Check all states are close to zero (initial)
```

### Test #3: VGS Variance Computation Semantics

```python
def test_vgs_variance_semantics():
    """Verify VGS computes variance of mean, and document limitation."""
    scaler = VarianceGradientScaler(model.parameters())

    # Create anticorrelated gradient pattern
    # Parameter with +0.5, -0.5 elements (mean ‚âà 0, variance = 0.25)
    for _ in range(10):
        for param in model.parameters():
            param.grad = torch.tensor([0.5, -0.5, 0.5, -0.5])  # Anticorrelated
        scaler.update_statistics()

    # VGS should report LOW variance (variance of mean ‚âà 0)
    var = scaler.get_normalized_variance()
    assert var < 0.01  # Mean ‚âà 0 ‚Üí low "variance of mean"

    # BUT: Element-wise variance is HIGH (0.25)
    # This demonstrates the limitation of current VGS approach
```

---

## üìù –ò—Ç–æ–≥–æ–≤—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏

1. **–ù–ï–ú–ï–î–õ–ï–ù–ù–û –∏—Å–ø—Ä–∞–≤–∏—Ç—å** (P0):
   - Return Scale Snapshot Timing (5-10% bias)
   - VecNormalize-LSTM Divergence (3-7% –ø–æ—Ç–µ—Ä—è –¥–ª—è PBT)

2. **–†–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ release** (P1):
   - VGS semantic improvement (element-wise variance)
   - VGS documentation update

3. **–û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ** (P2):
   - LSTM state stats logging –¥–ª—è debugging

4. **–ë–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π**:
   - Entropy management (—É–∂–µ –∑–∞—â–∏—â–µ–Ω–æ)
   - LSTM episode boundary reset (—É–∂–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–æ)

---

**Prepared by**: Claude Code Analysis
**Date**: 2025-11-23
**Version**: 1.0
