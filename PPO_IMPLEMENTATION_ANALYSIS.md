# –ì–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ PPO

## –†–µ–∑—é–º–µ
–ü–æ—Å–ª–µ —Ç—â–∞—Ç–µ–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ PPO –≤ `distributional_ppo.py` (9717 —Å—Ç—Ä–æ–∫ –∫–æ–¥–∞), —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–π —Å—Ç–∞—Ç—å–µ–π Schulman et al. 2017 –∏ –ª—É—á—à–∏–º–∏ –ø—Ä–∞–∫—Ç–∏–∫–∞–º–∏ –∏–∑ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π, —è **–Ω–µ –æ–±–Ω–∞—Ä—É–∂–∏–ª –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –∫–æ–Ω—Ü–µ–ø—Ç—É–∞–ª—å–Ω—ã—Ö, –ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –∏–ª–∏ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –æ—à–∏–±–æ–∫**.

–†–µ–∞–ª–∏–∑–∞—Ü–∏—è –≤ —Ü–µ–ª–æ–º –∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞ –∏ —Å–ª–µ–¥—É–µ—Ç –ø—Ä–∏–Ω—Ü–∏–ø–∞–º PPO —Å –Ω–µ–∫–æ—Ç–æ—Ä—ã–º–∏ –ø—Ä–æ–¥—É–º–∞–Ω–Ω—ã–º–∏ –¥–æ–ø–æ–ª–Ω–µ–Ω–∏—è–º–∏.

---

## ‚úÖ –ü—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–µ –∏ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã

### 1. **PPO Policy Loss (—Å—Ç—Ä–æ–∫–∏ 7850-7854)**
```python
policy_loss_1 = advantages_selected * ratio
policy_loss_2 = advantages_selected * torch.clamp(ratio, 1 - clip_range, 1 + clip_range)
policy_loss_ppo = -torch.min(policy_loss_1, policy_loss_2).mean()
```
**–°—Ç–∞—Ç—É—Å:** ‚úÖ **–ü–†–ê–í–ò–õ–¨–ù–û**
- –¢–æ—á–Ω–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Ñ–æ—Ä–º—É–ª–µ clipped surrogate objective –∏–∑ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–π —Å—Ç–∞—Ç—å–∏
- –ó–Ω–∞–∫ –∫–æ—Ä—Ä–µ–∫—Ç–µ–Ω –¥–ª—è gradient descent

### 2. **GAE Computation (—Å—Ç—Ä–æ–∫–∏ 184-186)**
```python
delta = rewards[step] + gamma * next_values * next_non_terminal - values[step]
last_gae_lam = delta + gamma * gae_lambda * next_non_terminal * last_gae_lam
advantages[step] = last_gae_lam
```
**–°—Ç–∞—Ç—É—Å:** ‚úÖ **–ü–†–ê–í–ò–õ–¨–ù–û**
- –ö–æ—Ä—Ä–µ–∫—Ç–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è Generalized Advantage Estimation
- –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ done flags —á–µ—Ä–µ–∑ `next_non_terminal`
- –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ time limit bootstrap (—Å—Ç—Ä–æ–∫–∏ 179-182)

### 3. **Returns Calculation (—Å—Ç—Ä–æ–∫–∞ 189)**
```python
rollout_buffer.returns = (advantages + values).astype(np.float32, copy=False)
```
**–°—Ç–∞—Ç—É—Å:** ‚úÖ **–ü–†–ê–í–ò–õ–¨–ù–û**
- –ö–æ—Ä—Ä–µ–∫—Ç–Ω–∞—è —Ñ–æ—Ä–º—É–ª–∞: returns = advantages + values
- –≠–∫–≤–∏–≤–∞–ª–µ–Ω—Ç–Ω–æ TD(Œª) returns

### 4. **Value Function Clipping (—Å—Ç—Ä–æ–∫–∏ 8380-8432)**
```python
value_pred_raw_clipped = torch.clamp(
    value_pred_raw_full,
    min=old_values_raw_aligned - clip_delta,
    max=old_values_raw_aligned + clip_delta,
)
# ...
critic_loss_clipped = self._quantile_huber_loss(
    quantiles_norm_clipped_for_loss, targets_norm_for_loss  # UNCLIPPED target
)
critic_loss = torch.max(critic_loss_unclipped, critic_loss_clipped)
```
**–°—Ç–∞—Ç—É—Å:** ‚úÖ **–ü–†–ê–í–ò–õ–¨–ù–û**
- –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è —Ñ–æ—Ä–º—É–ª–∞ VF clipping: `max(loss_unclipped, loss_clipped)`
- **–ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û**: targets –Ω–µ –∫–ª–∏–ø—è—Ç—Å—è (—Å—Ç—Ä–æ–∫–∞ 8430), —Ç–æ–ª—å–∫–æ predictions
- –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç OpenAI baselines implementation

### 5. **Quantile Huber Loss (—Å—Ç—Ä–æ–∫–∏ 2475-2484)**
```python
delta = predicted_quantiles - targets
huber = torch.where(
    abs_delta <= kappa,
    0.5 * delta.pow(2),
    kappa * (abs_delta - 0.5 * kappa),
)
indicator = (delta.detach() < 0.0).float()
loss = torch.abs(tau - indicator) * huber
```
**–°—Ç–∞—Ç—É—Å:** ‚úÖ **–ü–†–ê–í–ò–õ–¨–ù–û**
- –ö–æ—Ä—Ä–µ–∫—Ç–Ω–∞—è —Ñ–æ—Ä–º—É–ª–∞ quantile regression loss
- –ü—Ä–∞–≤–∏–ª—å–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ `.detach()` –¥–ª—è indicator function (–ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã —á–µ—Ä–µ–∑ –¥–∏—Å–∫—Ä–µ—Ç–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é)

### 6. **Importance Sampling Ratio (—Å—Ç—Ä–æ–∫–∏ 7847-7849)**
```python
log_ratio = log_prob_selected - old_log_prob_selected
log_ratio = torch.clamp(log_ratio, min=-85.0, max=85.0)  # Numerical stability
ratio = torch.exp(log_ratio)
```
**–°—Ç–∞—Ç—É—Å:** ‚úÖ **–ü–†–ê–í–ò–õ–¨–ù–û**
- –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≤ log space –¥–ª—è numerical stability
- Clamping –ø–µ—Ä–µ–¥ exp() –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç overflow
- exp(85) ‚âà 8e36 (finite), exp(89) = inf

### 7. **Overall Loss Combination (—Å—Ç—Ä–æ–∫–∏ 8726-8731)**
```python
loss = (
    policy_loss.to(dtype=torch.float32)
    + ent_coef_eff_value * entropy_loss.to(dtype=torch.float32)
    + vf_coef_effective * critic_loss
    + cvar_term
)
```
**–°—Ç–∞—Ç—É—Å:** ‚úÖ **–ü–†–ê–í–ò–õ–¨–ù–û**
- –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è –∫–æ–º–±–∏–Ω–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ loss
- –ó–Ω–∞–∫–∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã (entropy_loss —É–∂–µ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–π –Ω–∞ —Å—Ç—Ä–æ–∫–µ 7996)

### 8. **Gradient Accumulation with Weights (—Å—Ç—Ä–æ–∫–∏ 7787, 8750-8751)**
```python
weight = sample_weight / bucket_target_weight  # Normalized weights
loss_weighted = loss * loss.new_tensor(weight)
loss_weighted.backward()
```
**–°—Ç–∞—Ç—É—Å:** ‚úÖ **–ü–†–ê–í–ò–õ–¨–ù–û**
- –í–µ—Å–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω—ã (—Å—É–º–º–∞ = 1)
- –ö–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π gradient accumulation

### 9. **KL Divergence Approximation (—Å—Ç—Ä–æ–∫–∞ 8773)**
```python
approx_kl_component = (rollout_data.old_log_prob - log_prob).mean().item()
```
**–°—Ç–∞—Ç—É—Å:** ‚úÖ **–ü–†–ê–í–ò–õ–¨–ù–û**
- –ö–æ—Ä—Ä–µ–∫—Ç–Ω–∞—è –∞–ø–ø—Ä–æ–∫—Å–∏–º–∞—Ü–∏—è KL(old||new) ‚âà old_log_prob - new_log_prob

### 10. **Entropy Loss (—Å—Ç—Ä–æ–∫–∞ 7996)**
```python
entropy_loss = -torch.mean(entropy_selected)
```
**–°—Ç–∞—Ç—É—Å:** ‚úÖ **–ü–†–ê–í–ò–õ–¨–ù–û**
- –ó–Ω–∞–∫ –∫–æ—Ä—Ä–µ–∫—Ç–µ–Ω (–º–∞–∫—Å–∏–º–∏–∑–∏—Ä—É–µ–º —ç–Ω—Ç—Ä–æ–ø–∏—é = –º–∏–Ω–∏–º–∏–∑–∏—Ä—É–µ–º -entropy)

---

## ‚ö†Ô∏è –û—Ç–ª–∏—á–∏—è –æ—Ç –∫–∞–Ω–æ–Ω–∏—á–µ—Å–∫–æ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ OpenAI (–ù–ï –æ—à–∏–±–∫–∏!)

### 1. **Advantage Normalization: Global vs Per-Minibatch**

**–¢–µ–∫—É—â–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è (—Å—Ç—Ä–æ–∫–∏ 6468-6490):**
```python
# Normalize advantages globally (standard PPO practice)
if self.normalize_advantage and rollout_buffer.advantages is not None:
    advantages_flat = rollout_buffer.advantages.reshape(-1).astype(np.float64)
    adv_mean = float(np.mean(advantages_flat))
    adv_std = float(np.std(advantages_flat, ddof=1))
    normalized_advantages = ((rollout_buffer.advantages - adv_mean) / adv_std_clamped).astype(np.float32)
    rollout_buffer.advantages = normalized_advantages
```

**OpenAI baselines (–∏–∑ "37 Implementation Details"):**
- –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç **–Ω–∞ —É—Ä–æ–≤–Ω–µ –∫–∞–∂–¥–æ–≥–æ mini-batch** –≤–æ –≤—Ä–µ–º—è training loop

**–ê–Ω–∞–ª–∏–∑:**
- ‚úÖ –≠—Ç–æ **–Ω–µ –æ—à–∏–±–∫–∞**, –∞ design choice
- ‚úÖ Stable-Baselines3 —Ç–∞–∫–∂–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç global normalization
- ‚úÖ Global normalization –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –±–æ–ª–µ–µ —Å—Ç–∞–±–∏–ª—å–Ω—ã–π learning signal
- ‚ö†Ô∏è Per-minibatch normalization –º–æ–∂–µ—Ç –¥–∞—Ç—å –ª—É—á—à—É—é performance –≤ –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –∑–∞–¥–∞—á–∞—Ö

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è:** –û—Å—Ç–∞–≤–∏—Ç—å –∫–∞–∫ –µ—Å—Ç—å, –Ω–æ –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –æ–ø—Ü–∏—é `normalize_advantage_per_minibatch` –¥–ª—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤.

---

## üîç –°–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ (–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ)

### 1. **Distributional Value Function**
- –ò—Å–ø–æ–ª—å–∑—É–µ—Ç quantile regression –≤–º–µ—Å—Ç–æ –ø—Ä–æ—Å—Ç–æ–π MSE
- –≠—Ç–æ **—É–ª—É—á—à–µ–Ω–∏–µ** –Ω–∞–¥ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–º PPO, –Ω–µ –æ—à–∏–±–∫–∞
- –ü—Ä–∞–≤–∏–ª—å–Ω–æ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ —Å–æ–≥–ª–∞—Å–Ω–æ —Ç–µ–æ—Ä–∏–∏ distributional RL

### 2. **CVaR Regularization**
- –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∫–æ–º–ø–æ–Ω–µ–Ω—Ç –¥–ª—è risk-sensitive learning
- –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∫–æ—Ä—Ä–µ–∫—Ç–µ–Ω (–ø—Ä–æ–≤–µ—Ä–µ–Ω—ã —Å—Ç—Ä–æ–∫–∏ 2486-2594)

### 3. **AWR-style Behavior Cloning (—Å—Ç—Ä–æ–∫–∏ 7888-7913)**
```python
max_weight = 100.0
exp_arg = torch.clamp(advantages_selected / self.cql_beta, max=math.log(max_weight))
weights = torch.exp(exp_arg)
policy_loss_bc = (-log_prob_selected * weights).mean()
```
- ‚úÖ –ö–æ—Ä—Ä–µ–∫—Ç–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è Advantage Weighted Regression
- ‚úÖ –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø–æ—Ä—è–¥–æ–∫ –æ–ø–µ—Ä–∞—Ü–∏–π: clamp‚Üíexp (–Ω–µ exp‚Üíclamp!)
- ‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ advantages

### 4. **Value Clipping –¥–ª—è Distributional Critic**
```python
delta_norm = value_pred_norm_after_vf - value_pred_norm_full
quantiles_norm_clipped = quantiles_fp32 + delta_norm
```
- ‚úÖ –ü—Ä–∏–º–µ–Ω—è–µ—Ç –æ–¥–∏–Ω–∞–∫–æ–≤—É—é delta –∫–æ –≤—Å–µ–º –∫–≤–∞–Ω—Ç–∏–ª—è–º
- ‚úÖ –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ñ–æ—Ä–º—É —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è (–ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø–æ–¥—Ö–æ–¥)
- ‚úÖ –≠–∫–≤–∏–≤–∞–ª–µ–Ω—Ç–Ω–æ –∫–ª–∏–ø–ø–∏–Ω–≥—É location parameter

---

## üéØ –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ "37 Implementation Details"

| # | Detail | –°—Ç–∞—Ç—É—Å | –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π |
|---|--------|--------|-------------|
| 1 | Vectorized Architecture | ‚úÖ | –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —á–µ—Ä–µ–∑ VecEnv |
| 2 | Orthogonal Init | ‚ö†Ô∏è | –ù–µ –ø—Ä–æ–≤–µ—Ä–µ–Ω–æ (–∑–∞–≤–∏—Å–∏—Ç –æ—Ç policy network) |
| 3 | Adam Epsilon | ‚ö†Ô∏è | –ù–µ –ø—Ä–æ–≤–µ—Ä–µ–Ω–æ –≤ –¥–∞–Ω–Ω–æ–º –∞–Ω–∞–ª–∏–∑–µ |
| 4 | LR Annealing | ‚úÖ | –†–µ–∞–ª–∏–∑–æ–≤–∞–Ω (scheduler support) |
| 5 | GAE | ‚úÖ | –ö–æ—Ä—Ä–µ–∫—Ç–Ω–æ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω |
| 6 | Mini-batch Updates | ‚úÖ | –†–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ |
| 7 | Advantage Normalization | ‚ö†Ô∏è | Global –≤–º–µ—Å—Ç–æ per-minibatch (design choice) |
| 8 | Clipped Surrogate | ‚úÖ | –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è —Ñ–æ—Ä–º—É–ª–∞ |
| 9 | VF Loss Clipping | ‚úÖ | –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è —Ñ–æ—Ä–º—É–ª–∞ —Å max() |
| 10 | Overall Loss | ‚úÖ | –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è –∫–æ–º–±–∏–Ω–∞—Ü–∏—è |

---

## üß™ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–π –ø—Ä–æ–≤–µ—Ä–∫–∏

–•–æ—Ç—è –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –æ—à–∏–±–æ–∫ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, —Ä–µ–∫–æ–º–µ–Ω–¥—É—é:

1. **Unit Test –¥–ª—è Ratio Checking (–ø–µ—Ä–≤–∞—è —ç–ø–æ—Ö–∞)**
   - –ù–∞ –ø–µ—Ä–≤–æ–π —ç–ø–æ—Ö–µ/–ø–µ—Ä–≤–æ–º mini-batch ratio –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å ‚âà 1.0
   - –≠—Ç–æ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—Ç–ª–∞–¥–æ—á–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∏–∑ "37 Implementation Details"

2. **–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ KL Divergence**
   - approx_kl > 0.02 –æ–±—ã—á–Ω–æ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ –ø—Ä–æ–±–ª–µ–º—ã
   - –ö–æ–¥ –∏–º–µ–µ—Ç early stopping, –Ω–æ —Å—Ç–æ–∏—Ç –ª–æ–≥–∏—Ä–æ–≤–∞—Ç—å

3. **Explained Variance**
   - –î–æ–ª–∂–Ω–∞ –±—ã—Ç—å > 0 –∏ —Ä–∞—Å—Ç–∏ —Å–æ –≤—Ä–µ–º–µ–Ω–µ–º
   - –ù–∏–∑–∫–∞—è EV –º–æ–∂–µ—Ç —É–∫–∞–∑—ã–≤–∞—Ç—å –Ω–∞ –ø—Ä–æ–±–ª–µ–º—ã —Å value function

4. **–û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è Per-Minibatch Normalization**
   - –î–æ–±–∞–≤–∏—Ç—å —Ñ–ª–∞–≥ –¥–ª—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ —Å —Ä–∞–∑–Ω—ã–º–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è–º–∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏

---

## üìä –í—ã–≤–æ–¥—ã

### ‚úÖ –ß—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω–æ:
1. **–í—Å–µ –æ—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã PPO** –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã
2. **Numerical stability** —Ö–æ—Ä–æ—à–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–∞
3. **Gradient flow** –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π (detach –≤ –Ω—É–∂–Ω—ã—Ö –º–µ—Å—Ç–∞—Ö)
4. **Value function clipping** —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω –ø–æ —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏–∏
5. **GAE —Å done flags** —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ

### ‚ö†Ô∏è –û—Ç–ª–∏—á–∏—è –æ—Ç –æ—Ä–∏–≥–∏–Ω–∞–ª–∞ (–Ω–µ –æ—à–∏–±–∫–∏):
1. **Global advantage normalization** –≤–º–µ—Å—Ç–æ per-minibatch
2. **Distributional critic** (—É–ª—É—á—à–µ–Ω–∏–µ –Ω–∞–¥ vanilla PPO)
3. **–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ features**: CVaR, AWR, PopArt –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è

### üéì –û–±—â–µ–µ –∑–∞–∫–ª—é—á–µ–Ω–∏–µ:
**–†–µ–∞–ª–∏–∑–∞—Ü–∏—è PPO —è–≤–ª—è–µ—Ç—Å—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π, —Ö–æ—Ä–æ—à–æ –ø—Ä–æ–¥—É–º–∞–Ω–Ω–æ–π –∏ –≤–∫–ª—é—á–∞–µ—Ç —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è –∏–∑ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π. –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∏–ª–∏ –∫–æ–Ω—Ü–µ–ø—Ç—É–∞–ª—å–Ω—ã—Ö –æ—à–∏–±–æ–∫ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ.**

---

*–ê–Ω–∞–ª–∏–∑ –≤—ã–ø–æ–ª–Ω–µ–Ω: 2025-11-18*
*–§–∞–π–ª: distributional_ppo.py (9717 lines)*
*–û—Å–Ω–æ–≤–∞–Ω –Ω–∞: Schulman et al. 2017, "37 Implementation Details" (ICLR), –ª—É—á—à–∏–µ –ø—Ä–∞–∫—Ç–∏–∫–∏ 2024*
